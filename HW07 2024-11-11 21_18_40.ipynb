{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2badb765",
   "metadata": {},
   "source": [
    "# 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0561a6",
   "metadata": {},
   "source": [
    "1. simple linear regression use only one independent variable for one target variable, in contrast, multiple linear regression use many independent variable for one target varible. so multiple linear regression's formula has many different X and it can fit many factors at same time. \n",
    "\n",
    "2. continuous variable is a algebra variable that can be any number in a interval, it relates to the slope in the linear form. Indicator variable can be only two status such as 0 or 1, it is the intercept or the difference of target variale between two categories.  \n",
    "\n",
    "3. the model may have different tendency in different type. e.g. y = beta0+beta1x+beta2d+epsilon. d is the indicator variable. Beta0 is the intercept when d=0 and beta0+beta2 is the intercept whrn d=1, so it can show the difference between different goups or types.\n",
    "\n",
    "4. e.g. y = beta0+beta1x+beta2d+beta3(xd)+epsilon, beta3(xd) is the interaction effect here\n",
    "\n",
    "5. we use d1, d2 to represent different non-binary variables, for each d we still have 0 or 1. in this mode we can compare more than 2 varables at one time. \n",
    "\n",
    "the summary: Sure! Here’s a summary of the discussion for your homework:\n",
    "\n",
    "Summary of Discussion on Multiple Linear Regression with Indicator Variables:\n",
    "\n",
    "In Multiple Linear Regression, when a non-binary categorical variable is included, we create indicator variables (also called dummy variables) to represent the different categories of the variable. These indicator variables take values of 0 or 1, indicating the absence or presence of a category.\n",
    "\n",
    "For example, if we have a categorical variable “Color” with three categories (e.g., Red, Green, Blue), we can create two indicator variables:\n",
    "\t•\t￼ (1 if the color is Red, 0 otherwise)\n",
    "\t•\t￼ (1 if the color is Green, 0 otherwise)\n",
    "\n",
    "The category “Blue” is implied when both ￼ and ￼ are 0. The Multiple Linear Regression model would then look like:\n",
    "\n",
    "￼\n",
    "\n",
    "In this model:\n",
    "\t•\t￼ is the intercept when the color is Blue (the baseline category).\n",
    "\t•\t￼ and ￼ represent how much the value of ￼ changes when the color is Red or Green, compared to Blue.\n",
    "\t•\t￼ is a continuous variable in the model, and ￼ is the error term.\n",
    "\n",
    "This method allows us to compare the effect of each category (Red, Green) relative to the baseline category (Blue). The use of binary encodings (0 or 1) for categorical variables enables us to model multiple categories and assess their individual effects on the dependent variable.\n",
    "\n",
    "When we introduce interaction terms (e.g., ￼), we can also explore how the relationship between the continuous variable ￼ and the dependent variable ￼ differs across categories.\n",
    "\n",
    "This summary covers the key concepts from our discussion: indicator variables, encoding categorical variables, and comparing multiple categories in a regression model.\n",
    "\n",
    "https://chatgpt.com/share/67316f05-ddbc-8004-88f8-7c7946932f7a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1429cf10",
   "metadata": {},
   "source": [
    "# 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e749e7",
   "metadata": {},
   "source": [
    "outcome: the effect of the selling, e.g. the profit.\n",
    "\n",
    "predictor: advertisement invest\n",
    "\n",
    "- we need considered the interaction effect, the effect of TV ad may not only depends on itself,  but also depends on the online ad invest. they are not independent. \n",
    "\n",
    "- linear form without interaction effect: y = beta0+beta1x1+beta2x2, x1 is TV ad investment and x2 is online ad investment. linear form with interaction effect, y = beta0+beta1x1+beta2x2+beta3(x1x2)\n",
    "\n",
    "- the model without interaction means that good effect of online ad has totally no effect on TV ad, so these two variables are independent, we don't need to combine them together to predict the result. in contrast, if we add more investment on TV ad, it may let the online ad has better effect, so when we write the fomula, we need to put them together. \n",
    "\n",
    "- binary model: \n",
    "    - without interaction: the investment only depends on each effect of two types of ads, i.e. we only care about is TV ad's investment \"high\" or \"low\", online ad is same. \n",
    "    - with interaction: we need to consider that when one type of ad has \"high\"or \"low\" investment, the other may be effected. e.g. when TV ad and online ad both have high investment, it might have super good effect, and model will show this \"supper\" part"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0177ae9b",
   "metadata": {},
   "source": [
    "# 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74a12e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.228109\n",
      "         Iterations 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>       <td>str8fyre</td>     <th>  No. Observations:  </th>  <td>   800</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>  <td>   788</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>  <td>    11</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Mon, 11 Nov 2024</td> <th>  Pseudo R-squ.:     </th>  <td>0.05156</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>21:06:22</td>     <th>  Log-Likelihood:    </th> <td> -182.49</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th> <td> -192.41</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th>  <td>0.04757</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "                      <td></td>                        <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>                                <td>   -3.2644</td> <td>    0.714</td> <td>   -4.572</td> <td> 0.000</td> <td>   -4.664</td> <td>   -1.865</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Legendary[T.True]</th>                        <td>    4.3478</td> <td>    2.179</td> <td>    1.996</td> <td> 0.046</td> <td>    0.078</td> <td>    8.618</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>I(Q(\"Type 2\") == \"None\")[T.True]</th>         <td>    1.5432</td> <td>    0.853</td> <td>    1.810</td> <td> 0.070</td> <td>   -0.128</td> <td>    3.215</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Generation)[T.2]</th>                       <td>   -0.0574</td> <td>    0.468</td> <td>   -0.123</td> <td> 0.902</td> <td>   -0.975</td> <td>    0.861</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Generation)[T.3]</th>                       <td>   -0.6480</td> <td>    0.466</td> <td>   -1.390</td> <td> 0.164</td> <td>   -1.561</td> <td>    0.265</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Generation)[T.4]</th>                       <td>   -0.8255</td> <td>    0.545</td> <td>   -1.516</td> <td> 0.130</td> <td>   -1.893</td> <td>    0.242</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Generation)[T.5]</th>                       <td>   -0.5375</td> <td>    0.449</td> <td>   -1.198</td> <td> 0.231</td> <td>   -1.417</td> <td>    0.342</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Generation)[T.6]</th>                       <td>    0.3213</td> <td>    0.477</td> <td>    0.673</td> <td> 0.501</td> <td>   -0.614</td> <td>    1.257</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack</th>                                   <td>    0.0172</td> <td>    0.006</td> <td>    3.086</td> <td> 0.002</td> <td>    0.006</td> <td>    0.028</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Legendary[T.True]</th>                 <td>   -0.0365</td> <td>    0.019</td> <td>   -1.884</td> <td> 0.060</td> <td>   -0.074</td> <td>    0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense</th>                                  <td>   -0.0098</td> <td>    0.008</td> <td>   -1.247</td> <td> 0.213</td> <td>   -0.025</td> <td>    0.006</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:I(Q(\"Type 2\") == \"None\")[T.True]</th> <td>   -0.0197</td> <td>    0.012</td> <td>   -1.651</td> <td> 0.099</td> <td>   -0.043</td> <td>    0.004</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}                           &     str8fyre     & \\textbf{  No. Observations:  } &      800    \\\\\n",
       "\\textbf{Model:}                                   &      Logit       & \\textbf{  Df Residuals:      } &      788    \\\\\n",
       "\\textbf{Method:}                                  &       MLE        & \\textbf{  Df Model:          } &       11    \\\\\n",
       "\\textbf{Date:}                                    & Mon, 11 Nov 2024 & \\textbf{  Pseudo R-squ.:     } &  0.05156    \\\\\n",
       "\\textbf{Time:}                                    &     21:06:22     & \\textbf{  Log-Likelihood:    } &   -182.49   \\\\\n",
       "\\textbf{converged:}                               &       True       & \\textbf{  LL-Null:           } &   -192.41   \\\\\n",
       "\\textbf{Covariance Type:}                         &    nonrobust     & \\textbf{  LLR p-value:       } &  0.04757    \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                                                  & \\textbf{coef} & \\textbf{std err} & \\textbf{z} & \\textbf{P$> |$z$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{Intercept}                                &      -3.2644  &        0.714     &    -4.572  &         0.000        &       -4.664    &       -1.865     \\\\\n",
       "\\textbf{Legendary[T.True]}                        &       4.3478  &        2.179     &     1.996  &         0.046        &        0.078    &        8.618     \\\\\n",
       "\\textbf{I(Q(\"Type 2\") == \"None\")[T.True]}         &       1.5432  &        0.853     &     1.810  &         0.070        &       -0.128    &        3.215     \\\\\n",
       "\\textbf{C(Generation)[T.2]}                       &      -0.0574  &        0.468     &    -0.123  &         0.902        &       -0.975    &        0.861     \\\\\n",
       "\\textbf{C(Generation)[T.3]}                       &      -0.6480  &        0.466     &    -1.390  &         0.164        &       -1.561    &        0.265     \\\\\n",
       "\\textbf{C(Generation)[T.4]}                       &      -0.8255  &        0.545     &    -1.516  &         0.130        &       -1.893    &        0.242     \\\\\n",
       "\\textbf{C(Generation)[T.5]}                       &      -0.5375  &        0.449     &    -1.198  &         0.231        &       -1.417    &        0.342     \\\\\n",
       "\\textbf{C(Generation)[T.6]}                       &       0.3213  &        0.477     &     0.673  &         0.501        &       -0.614    &        1.257     \\\\\n",
       "\\textbf{Attack}                                   &       0.0172  &        0.006     &     3.086  &         0.002        &        0.006    &        0.028     \\\\\n",
       "\\textbf{Attack:Legendary[T.True]}                 &      -0.0365  &        0.019     &    -1.884  &         0.060        &       -0.074    &        0.001     \\\\\n",
       "\\textbf{Defense}                                  &      -0.0098  &        0.008     &    -1.247  &         0.213        &       -0.025    &        0.006     \\\\\n",
       "\\textbf{Defense:I(Q(\"Type 2\") == \"None\")[T.True]} &      -0.0197  &        0.012     &    -1.651  &         0.099        &       -0.043    &        0.004     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{Logit Regression Results}\n",
       "\\end{center}"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:               str8fyre   No. Observations:                  800\n",
       "Model:                          Logit   Df Residuals:                      788\n",
       "Method:                           MLE   Df Model:                           11\n",
       "Date:                Mon, 11 Nov 2024   Pseudo R-squ.:                 0.05156\n",
       "Time:                        21:06:22   Log-Likelihood:                -182.49\n",
       "converged:                       True   LL-Null:                       -192.41\n",
       "Covariance Type:            nonrobust   LLR p-value:                   0.04757\n",
       "============================================================================================================\n",
       "                                               coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------------------------------------\n",
       "Intercept                                   -3.2644      0.714     -4.572      0.000      -4.664      -1.865\n",
       "Legendary[T.True]                            4.3478      2.179      1.996      0.046       0.078       8.618\n",
       "I(Q(\"Type 2\") == \"None\")[T.True]             1.5432      0.853      1.810      0.070      -0.128       3.215\n",
       "C(Generation)[T.2]                          -0.0574      0.468     -0.123      0.902      -0.975       0.861\n",
       "C(Generation)[T.3]                          -0.6480      0.466     -1.390      0.164      -1.561       0.265\n",
       "C(Generation)[T.4]                          -0.8255      0.545     -1.516      0.130      -1.893       0.242\n",
       "C(Generation)[T.5]                          -0.5375      0.449     -1.198      0.231      -1.417       0.342\n",
       "C(Generation)[T.6]                           0.3213      0.477      0.673      0.501      -0.614       1.257\n",
       "Attack                                       0.0172      0.006      3.086      0.002       0.006       0.028\n",
       "Attack:Legendary[T.True]                    -0.0365      0.019     -1.884      0.060      -0.074       0.001\n",
       "Defense                                     -0.0098      0.008     -1.247      0.213      -0.025       0.006\n",
       "Defense:I(Q(\"Type 2\") == \"None\")[T.True]    -0.0197      0.012     -1.651      0.099      -0.043       0.004\n",
       "============================================================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here's an example of how you can do this\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/KeithGalli/pandas/master/pokemon_data.csv\"\n",
    "pokeaman = pd.read_csv(url).fillna('None')\n",
    "\n",
    "pokeaman['str8fyre'] = (pokeaman['Type 1']=='Fire').astype(int)\n",
    "linear_model_specification_formula = \\\n",
    "'str8fyre ~ Attack*Legendary + Defense*I(Q(\"Type 2\")==\"None\") + C(Generation)'\n",
    "log_reg_fit = smf.logit(linear_model_specification_formula, data=pokeaman).fit()\n",
    "log_reg_fit.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "547b49b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "interaction_model_specification_formula = (\n",
    "    'str8fyre ~ Attack + Defense + Legendary + C(Generation) + C(\"Type 1\") + '\n",
    "    'Attack:Legendary + Defense:Legendary + Attack:C(Generation) + Defense:C(Generation) + '\n",
    "    'Attack:C(\"Type 1\") + Defense:C(\"Type 1\")'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fcfe287",
   "metadata": {},
   "source": [
    "logic regression is used for predicting rate of a event in binary problem. use log formula and rate to set up model. this case is same. we can use confidence interval, p-value, std err model fitting, coefficient and many different ways to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "556d6887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.232266\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.228109\n",
      "         Iterations 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "Legendary Status=False<br>Attack=%{x}<br>Fire Type (1=Fire)=%{y}<extra></extra>",
         "legendgroup": "False",
         "marker": {
          "color": "#636efa",
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "False",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          49,
          62,
          82,
          100,
          52,
          64,
          84,
          130,
          104,
          48,
          63,
          83,
          103,
          30,
          20,
          45,
          35,
          25,
          90,
          150,
          45,
          60,
          80,
          80,
          56,
          81,
          60,
          90,
          60,
          85,
          55,
          90,
          75,
          100,
          47,
          62,
          92,
          57,
          72,
          102,
          45,
          70,
          41,
          76,
          45,
          70,
          45,
          80,
          50,
          65,
          80,
          70,
          95,
          55,
          65,
          55,
          80,
          45,
          70,
          52,
          82,
          80,
          105,
          70,
          110,
          50,
          65,
          95,
          20,
          35,
          50,
          50,
          80,
          100,
          130,
          75,
          90,
          105,
          40,
          70,
          80,
          95,
          120,
          85,
          100,
          65,
          75,
          75,
          35,
          60,
          65,
          85,
          110,
          45,
          70,
          80,
          105,
          65,
          95,
          35,
          50,
          65,
          65,
          45,
          48,
          73,
          105,
          130,
          30,
          50,
          40,
          95,
          50,
          80,
          120,
          105,
          55,
          65,
          90,
          85,
          130,
          5,
          55,
          95,
          125,
          40,
          65,
          67,
          92,
          45,
          75,
          45,
          110,
          50,
          83,
          95,
          125,
          155,
          100,
          10,
          125,
          155,
          85,
          48,
          55,
          65,
          65,
          130,
          60,
          40,
          60,
          80,
          115,
          105,
          135,
          110,
          64,
          84,
          134,
          100,
          49,
          62,
          82,
          52,
          64,
          84,
          65,
          80,
          105,
          46,
          76,
          30,
          50,
          20,
          35,
          60,
          90,
          90,
          38,
          58,
          40,
          25,
          30,
          20,
          40,
          50,
          75,
          40,
          55,
          75,
          95,
          80,
          20,
          50,
          100,
          75,
          35,
          45,
          55,
          70,
          30,
          75,
          65,
          45,
          85,
          65,
          65,
          85,
          75,
          60,
          72,
          33,
          80,
          65,
          90,
          70,
          75,
          85,
          125,
          80,
          120,
          95,
          130,
          150,
          10,
          125,
          185,
          95,
          80,
          130,
          40,
          50,
          50,
          100,
          55,
          65,
          105,
          55,
          40,
          80,
          60,
          90,
          90,
          95,
          60,
          120,
          80,
          95,
          20,
          35,
          95,
          30,
          63,
          75,
          80,
          10,
          64,
          84,
          134,
          164,
          100,
          45,
          65,
          85,
          110,
          60,
          85,
          120,
          160,
          70,
          85,
          110,
          150,
          55,
          90,
          30,
          70,
          45,
          35,
          70,
          35,
          50,
          30,
          50,
          70,
          40,
          70,
          100,
          55,
          85,
          30,
          50,
          25,
          35,
          65,
          85,
          30,
          60,
          40,
          130,
          60,
          80,
          160,
          45,
          90,
          90,
          51,
          71,
          91,
          60,
          120,
          20,
          45,
          45,
          65,
          75,
          85,
          85,
          105,
          70,
          90,
          110,
          140,
          40,
          60,
          100,
          45,
          75,
          75,
          50,
          40,
          73,
          47,
          60,
          43,
          73,
          90,
          120,
          140,
          70,
          90,
          60,
          100,
          120,
          85,
          25,
          45,
          60,
          100,
          70,
          100,
          85,
          115,
          40,
          70,
          110,
          115,
          100,
          55,
          95,
          48,
          78,
          80,
          120,
          40,
          70,
          41,
          81,
          95,
          125,
          15,
          60,
          70,
          90,
          75,
          115,
          165,
          40,
          70,
          68,
          50,
          130,
          150,
          23,
          50,
          80,
          120,
          40,
          60,
          80,
          64,
          104,
          84,
          90,
          30,
          75,
          95,
          135,
          145,
          55,
          75,
          135,
          145,
          68,
          89,
          109,
          58,
          78,
          104,
          51,
          66,
          86,
          55,
          75,
          120,
          45,
          85,
          25,
          85,
          65,
          85,
          120,
          30,
          70,
          125,
          165,
          42,
          52,
          29,
          59,
          79,
          69,
          94,
          30,
          80,
          45,
          65,
          105,
          35,
          60,
          48,
          83,
          100,
          50,
          80,
          66,
          76,
          136,
          60,
          125,
          55,
          82,
          30,
          63,
          93,
          24,
          89,
          80,
          25,
          5,
          65,
          92,
          70,
          90,
          130,
          170,
          85,
          70,
          110,
          145,
          72,
          112,
          50,
          90,
          61,
          106,
          100,
          49,
          69,
          20,
          62,
          92,
          132,
          120,
          70,
          85,
          140,
          100,
          123,
          95,
          50,
          76,
          110,
          60,
          95,
          130,
          80,
          125,
          165,
          55,
          100,
          80,
          50,
          65,
          65,
          65,
          65,
          65,
          70,
          80,
          100,
          45,
          60,
          75,
          63,
          93,
          123,
          55,
          75,
          100,
          55,
          85,
          60,
          80,
          110,
          50,
          88,
          53,
          98,
          53,
          98,
          53,
          98,
          25,
          55,
          55,
          77,
          115,
          60,
          100,
          75,
          105,
          135,
          45,
          57,
          85,
          135,
          60,
          60,
          80,
          105,
          140,
          50,
          65,
          95,
          100,
          125,
          53,
          63,
          103,
          45,
          55,
          100,
          27,
          67,
          35,
          60,
          92,
          72,
          82,
          117,
          90,
          140,
          30,
          86,
          65,
          95,
          75,
          90,
          58,
          30,
          50,
          78,
          108,
          112,
          140,
          50,
          95,
          65,
          105,
          50,
          95,
          30,
          45,
          55,
          30,
          40,
          65,
          44,
          87,
          50,
          65,
          95,
          60,
          100,
          75,
          75,
          135,
          55,
          85,
          40,
          60,
          75,
          47,
          77,
          50,
          94,
          55,
          80,
          100,
          55,
          85,
          115,
          55,
          75,
          30,
          40,
          55,
          87,
          117,
          147,
          70,
          110,
          50,
          40,
          70,
          66,
          85,
          125,
          120,
          74,
          124,
          85,
          125,
          110,
          83,
          123,
          55,
          65,
          97,
          109,
          65,
          85,
          105,
          85,
          60,
          72,
          72,
          77,
          128,
          120,
          61,
          78,
          107,
          45,
          59,
          69,
          56,
          63,
          95,
          36,
          56,
          50,
          73,
          81,
          35,
          22,
          52,
          50,
          68,
          38,
          45,
          65,
          65,
          100,
          82,
          124,
          80,
          48,
          48,
          48,
          80,
          110,
          150,
          50,
          52,
          72,
          48,
          80,
          54,
          92,
          52,
          105,
          60,
          75,
          53,
          73,
          38,
          55,
          89,
          121,
          59,
          77,
          65,
          92,
          58,
          50,
          50,
          75,
          100,
          80,
          70,
          110,
          66,
          66,
          66,
          66,
          90,
          85,
          95,
          100,
          69,
          117,
          30,
          70
         ],
         "xaxis": "x",
         "y": [
          0,
          0,
          0,
          0,
          1,
          1,
          1,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "Legendary Status=True<br>Attack=%{x}<br>Fire Type (1=Fire)=%{y}<extra></extra>",
         "legendgroup": "True",
         "marker": {
          "color": "#EF553B",
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "True",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          85,
          90,
          100,
          110,
          190,
          150,
          85,
          115,
          75,
          90,
          130,
          100,
          50,
          75,
          80,
          100,
          90,
          130,
          100,
          150,
          150,
          180,
          150,
          180,
          100,
          150,
          180,
          70,
          95,
          75,
          105,
          125,
          120,
          120,
          90,
          160,
          100,
          120,
          90,
          100,
          103,
          120,
          100,
          90,
          129,
          90,
          115,
          100,
          115,
          105,
          120,
          150,
          125,
          145,
          130,
          170,
          120,
          131,
          131,
          100,
          100,
          160,
          110,
          160,
          110
         ],
         "xaxis": "x",
         "y": [
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1
         ],
         "yaxis": "y"
        },
        {
         "mode": "lines",
         "name": "Best Fit Line",
         "type": "scatter",
         "x": [
          49,
          62,
          82,
          100,
          52,
          64,
          84,
          130,
          104,
          48,
          63,
          83,
          103,
          30,
          20,
          45,
          35,
          25,
          90,
          150,
          45,
          60,
          80,
          80,
          56,
          81,
          60,
          90,
          60,
          85,
          55,
          90,
          75,
          100,
          47,
          62,
          92,
          57,
          72,
          102,
          45,
          70,
          41,
          76,
          45,
          70,
          45,
          80,
          50,
          65,
          80,
          70,
          95,
          55,
          65,
          55,
          80,
          45,
          70,
          52,
          82,
          80,
          105,
          70,
          110,
          50,
          65,
          95,
          20,
          35,
          50,
          50,
          80,
          100,
          130,
          75,
          90,
          105,
          40,
          70,
          80,
          95,
          120,
          85,
          100,
          65,
          75,
          75,
          35,
          60,
          65,
          85,
          110,
          45,
          70,
          80,
          105,
          65,
          95,
          35,
          50,
          65,
          65,
          45,
          48,
          73,
          105,
          130,
          30,
          50,
          40,
          95,
          50,
          80,
          120,
          105,
          55,
          65,
          90,
          85,
          130,
          5,
          55,
          95,
          125,
          40,
          65,
          67,
          92,
          45,
          75,
          45,
          110,
          50,
          83,
          95,
          125,
          155,
          100,
          10,
          125,
          155,
          85,
          48,
          55,
          65,
          65,
          130,
          60,
          40,
          60,
          80,
          115,
          105,
          135,
          110,
          85,
          90,
          100,
          64,
          84,
          134,
          110,
          190,
          150,
          100,
          49,
          62,
          82,
          52,
          64,
          84,
          65,
          80,
          105,
          46,
          76,
          30,
          50,
          20,
          35,
          60,
          90,
          90,
          38,
          58,
          40,
          25,
          30,
          20,
          40,
          50,
          75,
          40,
          55,
          75,
          95,
          80,
          20,
          50,
          100,
          75,
          35,
          45,
          55,
          70,
          30,
          75,
          65,
          45,
          85,
          65,
          65,
          85,
          75,
          60,
          72,
          33,
          80,
          65,
          90,
          70,
          75,
          85,
          125,
          80,
          120,
          95,
          130,
          150,
          10,
          125,
          185,
          95,
          80,
          130,
          40,
          50,
          50,
          100,
          55,
          65,
          105,
          55,
          40,
          80,
          60,
          90,
          90,
          95,
          60,
          120,
          80,
          95,
          20,
          35,
          95,
          30,
          63,
          75,
          80,
          10,
          85,
          115,
          75,
          64,
          84,
          134,
          164,
          90,
          130,
          100,
          45,
          65,
          85,
          110,
          60,
          85,
          120,
          160,
          70,
          85,
          110,
          150,
          55,
          90,
          30,
          70,
          45,
          35,
          70,
          35,
          50,
          30,
          50,
          70,
          40,
          70,
          100,
          55,
          85,
          30,
          50,
          25,
          35,
          65,
          85,
          30,
          60,
          40,
          130,
          60,
          80,
          160,
          45,
          90,
          90,
          51,
          71,
          91,
          60,
          120,
          20,
          45,
          45,
          65,
          75,
          85,
          85,
          105,
          70,
          90,
          110,
          140,
          40,
          60,
          100,
          45,
          75,
          75,
          50,
          40,
          73,
          47,
          60,
          43,
          73,
          90,
          120,
          140,
          70,
          90,
          60,
          100,
          120,
          85,
          25,
          45,
          60,
          100,
          70,
          100,
          85,
          115,
          40,
          70,
          110,
          115,
          100,
          55,
          95,
          48,
          78,
          80,
          120,
          40,
          70,
          41,
          81,
          95,
          125,
          15,
          60,
          70,
          90,
          75,
          115,
          165,
          40,
          70,
          68,
          50,
          130,
          150,
          23,
          50,
          80,
          120,
          40,
          60,
          80,
          64,
          104,
          84,
          90,
          30,
          75,
          95,
          135,
          145,
          55,
          75,
          135,
          145,
          100,
          50,
          75,
          80,
          100,
          90,
          130,
          100,
          150,
          150,
          180,
          150,
          180,
          100,
          150,
          180,
          70,
          95,
          68,
          89,
          109,
          58,
          78,
          104,
          51,
          66,
          86,
          55,
          75,
          120,
          45,
          85,
          25,
          85,
          65,
          85,
          120,
          30,
          70,
          125,
          165,
          42,
          52,
          29,
          59,
          79,
          69,
          94,
          30,
          80,
          45,
          65,
          105,
          35,
          60,
          48,
          83,
          100,
          50,
          80,
          66,
          76,
          136,
          60,
          125,
          55,
          82,
          30,
          63,
          93,
          24,
          89,
          80,
          25,
          5,
          65,
          92,
          70,
          90,
          130,
          170,
          85,
          70,
          110,
          145,
          72,
          112,
          50,
          90,
          61,
          106,
          100,
          49,
          69,
          20,
          62,
          92,
          132,
          120,
          70,
          85,
          140,
          100,
          123,
          95,
          50,
          76,
          110,
          60,
          95,
          130,
          80,
          125,
          165,
          55,
          100,
          80,
          50,
          65,
          65,
          65,
          65,
          65,
          75,
          105,
          125,
          120,
          120,
          90,
          160,
          100,
          120,
          70,
          80,
          100,
          90,
          100,
          103,
          120,
          100,
          45,
          60,
          75,
          63,
          93,
          123,
          55,
          75,
          100,
          55,
          85,
          60,
          80,
          110,
          50,
          88,
          53,
          98,
          53,
          98,
          53,
          98,
          25,
          55,
          55,
          77,
          115,
          60,
          100,
          75,
          105,
          135,
          45,
          57,
          85,
          135,
          60,
          60,
          80,
          105,
          140,
          50,
          65,
          95,
          100,
          125,
          53,
          63,
          103,
          45,
          55,
          100,
          27,
          67,
          35,
          60,
          92,
          72,
          82,
          117,
          90,
          140,
          30,
          86,
          65,
          95,
          75,
          90,
          58,
          30,
          50,
          78,
          108,
          112,
          140,
          50,
          95,
          65,
          105,
          50,
          95,
          30,
          45,
          55,
          30,
          40,
          65,
          44,
          87,
          50,
          65,
          95,
          60,
          100,
          75,
          75,
          135,
          55,
          85,
          40,
          60,
          75,
          47,
          77,
          50,
          94,
          55,
          80,
          100,
          55,
          85,
          115,
          55,
          75,
          30,
          40,
          55,
          87,
          117,
          147,
          70,
          110,
          50,
          40,
          70,
          66,
          85,
          125,
          120,
          74,
          124,
          85,
          125,
          110,
          83,
          123,
          55,
          65,
          97,
          109,
          65,
          85,
          105,
          85,
          60,
          90,
          129,
          90,
          115,
          100,
          115,
          105,
          120,
          150,
          125,
          145,
          130,
          170,
          120,
          72,
          72,
          77,
          128,
          120,
          61,
          78,
          107,
          45,
          59,
          69,
          56,
          63,
          95,
          36,
          56,
          50,
          73,
          81,
          35,
          22,
          52,
          50,
          68,
          38,
          45,
          65,
          65,
          100,
          82,
          124,
          80,
          48,
          48,
          48,
          80,
          110,
          150,
          50,
          52,
          72,
          48,
          80,
          54,
          92,
          52,
          105,
          60,
          75,
          53,
          73,
          38,
          55,
          89,
          121,
          59,
          77,
          65,
          92,
          58,
          50,
          50,
          75,
          100,
          80,
          70,
          110,
          66,
          66,
          66,
          66,
          90,
          85,
          95,
          100,
          69,
          117,
          30,
          70,
          131,
          131,
          100,
          100,
          160,
          110,
          160,
          110
         ],
         "y": [
          0.0778792035498896,
          0.07457528161559435,
          0.0712321660124653,
          0.05033595114580418,
          0.08738396566787178,
          0.08166045866295452,
          0.07802650338600178,
          0.08384359796351015,
          0.09765741311026806,
          0.061795225440030674,
          0.05967877561128276,
          0.056962347200456474,
          0.054362415567981694,
          0.07597234251759417,
          0.051331631285316835,
          0.07340832653030371,
          0.08603521380068797,
          0.05833678402022263,
          0.1377140625115454,
          0.25037942720279543,
          0.08411000925839532,
          0.08129551520611489,
          0.07767642654273574,
          0.07254772238781808,
          0.10168402706094565,
          0.09617294700024251,
          0.11348661492439359,
          0.09942142914344392,
          0.09428537112416968,
          0.089135495510495,
          0.09408037938681553,
          0.11345105704728022,
          0.06395199548364343,
          0.0603485368499751,
          0.07307253887463278,
          0.070598911994945,
          0.07557637733237298,
          0.09619753046340021,
          0.09055714284895756,
          0.09679518289098149,
          0.07544318617375528,
          0.07124152082002845,
          0.08039749255389189,
          0.0742248672541757,
          0.10984152949049947,
          0.10393948391281894,
          0.0899768013923462,
          0.08313520028839025,
          0.07257150974514233,
          0.07011356061512246,
          0.06773278067640122,
          0.09096539197676239,
          0.08597963952337909,
          0.08222400922730853,
          0.08037658773895254,
          0.11473685339596958,
          0.10860386236351044,
          0.0899768013923462,
          0.08504016401852565,
          0.08167106257215709,
          0.07627551663193786,
          0.13198456006983642,
          0.12505987367297425,
          0.10393948391281894,
          0.10162592259416689,
          0.08896924766648504,
          0.07508411485939397,
          0.07009051803188066,
          0.08899790272342491,
          0.08603521380068797,
          0.0831621478493682,
          0.06323655774511776,
          0.10860386236351044,
          0.10390656675595059,
          0.12638039633412237,
          0.12509855657960758,
          0.1210938414889567,
          0.11720015371484736,
          0.0850676717669137,
          0.07946714893076463,
          0.055012741530021954,
          0.05311543656662521,
          0.057603490129230446,
          0.1074123439088672,
          0.10390656675595059,
          0.07508411485939397,
          0.04509689989871021,
          0.016516663793560208,
          0.049554624664540046,
          0.04672191249967923,
          0.08600742259571308,
          0.12241631331865278,
          0.11592658865244627,
          0.06854048822893367,
          0.06469635468082467,
          0.10860386236351044,
          0.10276064243480137,
          0.046174147439080396,
          0.021025046086448812,
          0.08603521380068797,
          0.0831621478493682,
          0.08037658773895254,
          0.061073956619538274,
          0.015364227477180328,
          0.08130607592172717,
          0.07680501299003055,
          0.08405556419335103,
          0.07941544863518794,
          0.06180752272944342,
          0.05900027875437931,
          0.04564789353260616,
          0.08035046369610141,
          0.04154031865455491,
          0.04781993514216057,
          0.16009586539166004,
          0.09743966123289735,
          0.0583173691044825,
          0.04953797875902126,
          0.0467061713713484,
          0.06248716069049193,
          0.07418115783701619,
          0.08606301314013812,
          0.03316322625800539,
          0.08597963952337909,
          0.09193423310276908,
          0.052532018077142605,
          0.04953797875902126,
          0.08221333995253108,
          0.10164528747721638,
          0.06854048822893367,
          0.06395199548364343,
          0.05969067852190885,
          0.10162592259416689,
          0.0951397706623225,
          0.10233138787366125,
          0.11670385239159489,
          0.09193423310276908,
          0.09825688532124123,
          0.07420543792264037,
          0.04566329507976036,
          0.12131212403768077,
          0.11362173984754793,
          0.07679498803967633,
          0.07805702340395608,
          0.08222400922730853,
          0.08037658773895254,
          0.08037658773895254,
          0.16274572543276616,
          0.06620932037062709,
          0.03437370256470546,
          0.03050726320317858,
          0.06321562161891886,
          0.07676993091210067,
          0.11720015371484736,
          0.12502120101857886,
          0.12371286917682836,
          0.0644756400362856,
          0.08380185621482869,
          0.08765093804729066,
          0.0972657862648554,
          0.09300771405973435,
          0.10854228152602902,
          0.09799613824563486,
          0.20042524083143445,
          0.1927246396283149,
          0.06928869433835616,
          0.05999441297073303,
          0.05660801590032262,
          0.054023330526859724,
          0.08395437777121009,
          0.0784361967915962,
          0.0749340159695256,
          0.0730935513036451,
          0.06965597441527557,
          0.07043857823879059,
          0.08861696307069804,
          0.08280391669129904,
          0.07810988735235797,
          0.07462109652171854,
          0.06970180224513926,
          0.06284163537131762,
          0.0955994340763169,
          0.0893732716144001,
          0.07805899519582597,
          0.07669680355917763,
          0.07326611230803595,
          0.10680669925990925,
          0.07584043114320155,
          0.09563000041700538,
          0.04276838309956517,
          0.04078828534119071,
          0.079883751315965,
          0.07545496047672984,
          0.0763472130260245,
          0.07377155547625308,
          0.06138019722981453,
          0.05859114990970278,
          0.05659669070380603,
          0.05281551612985451,
          0.04922411080661741,
          0.05401249282563697,
          0.0704617265780467,
          0.07212277757730154,
          0.07048488194774859,
          0.05999042618449305,
          0.08740870535655358,
          0.07810988735235797,
          0.09243928311957274,
          0.09453545914662781,
          0.07547962349725082,
          0.06885875372257205,
          0.07719875657099733,
          0.038436421906769776,
          0.12247243233165908,
          0.06577541228233276,
          0.0729309239731691,
          0.09817172562431845,
          0.05494074891375231,
          0.08545563995714592,
          0.05097279902098366,
          0.03372379836875294,
          0.07127608527953204,
          0.04640936940390263,
          0.013347874618109816,
          0.014005555021090486,
          0.1044367557109859,
          0.1164737551818854,
          0.08837181987852621,
          0.09342220886710254,
          0.06802515633060227,
          0.0034416822427090683,
          0.1229517271551396,
          0.1397014006457563,
          0.11524309017581898,
          0.1044367557109859,
          0.12973708258230648,
          0.0763472130260245,
          0.0278760735929937,
          0.0854832695424571,
          0.08738051290180211,
          0.04864850719571135,
          0.1079582661485159,
          0.09879289057318502,
          0.08452136059135346,
          0.050394743700227874,
          0.02993846024885029,
          0.10915509990711743,
          0.1165101359303687,
          0.0680699897214043,
          0.06728960793147293,
          0.0729309239731691,
          0.06351084718275148,
          0.060671516869671226,
          0.10511289032069776,
          0.06506249186489453,
          0.07722394195657362,
          0.06728960793147293,
          0.09563000041700538,
          0.10285446193673707,
          0.11729079314979207,
          0.04920757017607366,
          0.08174738176913593,
          0.08711772364010219,
          0.10639144403528727,
          0.04465373370364313,
          0.08741434483438965,
          0.08354763190070692,
          0.08540592710457727,
          0.06959186228284227,
          0.043097401406522545,
          0.11737352706152542,
          0.06651753417855881,
          0.04980202128083499,
          0.05466165261800426,
          0.052160923253902806,
          0.060646382539537635,
          0.05530426715099306,
          0.05593532023481717,
          0.0728772182045986,
          0.09982806859796924,
          0.05402607799566056,
          0.04862809577708435,
          0.049186936319353204,
          0.05923277019653086,
          0.05595398924407307,
          0.05155284001004668,
          0.038357518719629315,
          0.04629927684520145,
          0.04980202128083499,
          0.03334305149366718,
          0.05402607799566056,
          0.03334305149366718,
          0.03216766068382396,
          0.044821223812123646,
          0.042750327936803535,
          0.0407710304952886,
          0.0379916402330469,
          0.0620915046400144,
          0.06651209305823733,
          0.059985484670844455,
          0.05593532023481717,
          0.044821223812123646,
          0.02089330091063519,
          0.045353702260394,
          0.04429470689733634,
          0.0412574707563407,
          0.052160923253902806,
          0.04357337660561843,
          0.04058350091278922,
          0.032946718368158115,
          0.07122350449976594,
          0.04174946214772013,
          0.03981457642495954,
          0.07624139335996377,
          0.022731897301242893,
          0.07290110510677855,
          0.07290110510677855,
          0.06311588599737623,
          0.06025303370176423,
          0.057512065205320186,
          0.0635476930882544,
          0.08350845240680727,
          0.03455985689993766,
          0.011824763386403706,
          0.0432593182570273,
          0.0412574707563407,
          0.04029008451826815,
          0.022181169084130335,
          0.0393444502608525,
          0.02819141152833082,
          0.026564060832405822,
          0.018962388949915896,
          0.01350585883091894,
          0.009371655970519558,
          0.035382665073528725,
          0.03373246251083589,
          0.04694004968323021,
          0.04642113608480417,
          0.0497852966252838,
          0.03752942735624799,
          0.04922000969674103,
          0.0379916402330469,
          0.052167914145888665,
          0.038441021956481684,
          0.05157012622069959,
          0.03772135816290273,
          0.03511828646173908,
          0.10213776432790236,
          0.10907785750847969,
          0.09134095358963741,
          0.06653404338953883,
          0.07290110510677855,
          0.05530426715099306,
          0.05790790344468143,
          0.048042940455889194,
          0.017851795638247472,
          0.03937117957616087,
          0.03255493761032675,
          0.04174946214772013,
          0.0816612433406606,
          0.05402607799566056,
          0.050356731964426964,
          0.07374142024459276,
          0.07892162925722301,
          0.032946718368158115,
          0.030662635528179626,
          0.037072621135608344,
          0.07892162925722301,
          0.06651209305823733,
          0.03665859437693521,
          0.04426478899568344,
          0.0460937578507306,
          0.04293870525951345,
          0.049203470372963916,
          0.059252470690302844,
          0.035382665073528725,
          0.024719679299839894,
          0.02613077330423566,
          0.03162428672150025,
          0.07206962065789134,
          0.05093437465648951,
          0.043273950444295405,
          0.03185886894527125,
          0.0407710304952886,
          0.05155284001004668,
          0.07045598717490818,
          0.07371727989659725,
          0.1126654661120948,
          0.02140534768057254,
          0.017218750745578756,
          0.03309338999512992,
          0.03216766068382396,
          0.093414787186595,
          0.11642836904789243,
          0.03194836684956462,
          0.042750327936803535,
          0.03981457642495954,
          0.06350563527684654,
          0.0379916402330469,
          0.036224327811331086,
          0.03453627658467804,
          0.03066473685721459,
          0.03707514494329256,
          0.029227905317839167,
          0.021914504170556598,
          0.031417180994741285,
          0.0497852966252838,
          0.035782816597505804,
          0.07539954719575848,
          0.042204174960970106,
          0.029588989956961347,
          0.02820109732184806,
          0.037503899600969094,
          0.031752221073141323,
          0.009931022366219901,
          0.023204979386183402,
          0.015201984774136004,
          0.038298397817006065,
          0.03166176260116322,
          0.04961275338996774,
          0.0597400909847401,
          0.04845935857486217,
          0.08607662605178719,
          0.04306464429095987,
          0.046197342751424626,
          0.08607662605178719,
          0.10514654606438616,
          0.04208523430242795,
          0.14533239872815013,
          0.2769451830253243,
          0.012367967776815625,
          0.045701659927731765,
          0.03807800772371682,
          0.03622038293041545,
          0.03453250894819365,
          0.044922392187793266,
          0.05073456957441087,
          0.05265015345571131,
          0.03640835245950497,
          0.03512883217908725,
          0.0334900570473303,
          0.05280194428600104,
          0.05038176082699803,
          0.06425536348001416,
          0.04079149961829379,
          0.04921141388707768,
          0.031727047366488154,
          0.055817517317384754,
          0.05609003841387234,
          0.057394968038699035,
          0.056710606954298824,
          0.03667707779265358,
          0.03843945827495548,
          0.10212091954816699,
          0.12159528929199748,
          0.012784887995347464,
          0.006949035169807851,
          0.03142449965398287,
          0.02532901259936714,
          0.02413596739889386,
          0.024725477598006813,
          0.06280901015359398,
          0.033193953676417226,
          0.025505825274807593,
          0.02657759496206482,
          0.055313138292238086,
          0.06652263540708826,
          0.03374952224646083,
          0.03179006491683912,
          0.037727521534646244,
          0.042945683059368196,
          0.05389403980978426,
          0.047086985394700034,
          0.05806774222571661,
          0.04933722533435347,
          0.031482624274230084,
          0.05539812112674581,
          0.03666459071775562,
          0.08697649400260805,
          0.044608102055541804,
          0.04491026106892576,
          0.029604018388542173,
          0.04566498834155267,
          0.04897716503804867,
          0.016377823885666592,
          0.023222756960889327,
          0.028205750753172134,
          0.029961386271306047,
          0.04178455166777735,
          0.04808305409043205,
          0.027012511079992772,
          0.050976773225118927,
          0.04863595115305891,
          0.0509425826393926,
          0.06132385956117622,
          0.06502449546831697,
          0.0546704266452595,
          0.05724603097934296,
          0.06679963889791404,
          0.032707531475074804,
          0.029717914595919742,
          0.021151322980203294,
          0.025625544324852458,
          0.049224646949874384,
          0.05859178179035882,
          0.04954979549814157,
          0.03406668755890499,
          0.03247579418513524,
          0.026268535038102413,
          0.04326049334331681,
          0.043245865500608216,
          0.04530903884722877,
          0.06884135892559709,
          0.01874330070134123,
          0.029940846205373756,
          0.034932926076295924,
          0.02327571681648246,
          0.06931399944733126,
          0.05013550106827903,
          0.019675159753926454,
          0.030594236511407868,
          0.024418881836953277,
          0.017860972398891287,
          0.021918143654674368,
          0.06278404604955384,
          0.0402966500237168,
          0.07288869066698661,
          0.0762533518591179,
          0.010095017961986443,
          0.020144008435091294,
          0.0402966500237168,
          0.02551461259987152,
          0.01981468024737757,
          0.01981468024737757,
          0.01981468024737757,
          0.01981468024737757,
          0.01981468024737757,
          0.017796998533152007,
          0.03652333129834512,
          0.07518162591661023,
          0.03523991802522636,
          0.04678445296078474,
          0.030124704418237183,
          0.06475990792115037,
          0.02776991602534843,
          0.04678445296078474,
          0.017432201848506624,
          0.03495676740607067,
          0.03332574192856722,
          0.03785165173966701,
          0.03696104492703549,
          0.05447099912476642,
          0.03523991802522636,
          0.04698367029339886,
          0.04193500334548041,
          0.03769695184331435,
          0.03387206526782026,
          0.059538670965454975,
          0.0732001217276273,
          0.08969728820029714,
          0.05426308887321426,
          0.05552801508368204,
          0.05237265898929727,
          0.058994918258762456,
          0.05500772214557212,
          0.05750626454536555,
          0.054882951726436,
          0.05486461930012356,
          0.05724635376439371,
          0.0740436082909864,
          0.05083364719247551,
          0.06944670572019988,
          0.05083364719247551,
          0.06944670572019988,
          0.05083364719247551,
          0.06944670572019988,
          0.03816114614823134,
          0.030800488827525906,
          0.050595483136409006,
          0.05526913782187314,
          0.06677797993449283,
          0.06884174231733056,
          0.07105298795982602,
          0.039052957596796056,
          0.041906609367730295,
          0.041892419273922295,
          0.049663360945811974,
          0.048281023621246304,
          0.08200897875213901,
          0.10949394511959605,
          0.03222598130479557,
          0.018109460729118888,
          0.06306777061389324,
          0.05550947984346238,
          0.07233227988041738,
          0.054901289922508166,
          0.053007613142099,
          0.05682065638967227,
          0.05237265898929727,
          0.08013990504082703,
          0.03725593140048476,
          0.0315410355983049,
          0.05814928004438054,
          0.03962450095375709,
          0.025191686024607626,
          0.04951679355298021,
          0.03155399514884795,
          0.03552400142748583,
          0.04000545076471995,
          0.03769695184331435,
          0.06305941623745125,
          0.07576241024372142,
          0.07404845544036115,
          0.06832706769121595,
          0.08108269838663608,
          0.12340392690761114,
          0.01709420382621266,
          0.05722346560040836,
          0.03469092176947971,
          0.0279801421024181,
          0.04827127910996454,
          0.030422997958392574,
          0.034283816256443694,
          0.02283493546599119,
          0.012168586059409228,
          0.03131086212863757,
          0.029136770655887326,
          0.10366094046783862,
          0.10829384772484588,
          0.04028321338592958,
          0.051526873294001255,
          0.06529606202547776,
          0.07836083745508547,
          0.054901289922508166,
          0.06992778803032845,
          0.0377097767522209,
          0.033883634728039756,
          0.026683994973324097,
          0.04345126523111923,
          0.042434716642482564,
          0.03999187755554344,
          0.04447901432197133,
          0.06119821778633414,
          0.04772231971338908,
          0.04606419377013045,
          0.04940382672052243,
          0.05363188560730239,
          0.06452393953207805,
          0.05552801508368204,
          0.06835857724801338,
          0.05949118620358618,
          0.05426308887321426,
          0.05424495161456294,
          0.042434716642482564,
          0.04046941391973442,
          0.041920804057800916,
          0.0460735143065629,
          0.05683202370270691,
          0.026621659551338554,
          0.025362321871514236,
          0.03814817381637547,
          0.03594283569127129,
          0.03426743557984852,
          0.058180262807805,
          0.05424495161456294,
          0.06677797993449283,
          0.047163406213147054,
          0.044989390465017404,
          0.03511945119394209,
          0.03682238034386355,
          0.028670629265531968,
          0.06379409443850588,
          0.07835062666587164,
          0.08382532205173547,
          0.06915043713909627,
          0.06304688660813383,
          0.06308866107159053,
          0.025745960484903047,
          0.06915043713909627,
          0.03560884711717472,
          0.07155373470428858,
          0.09806691612338947,
          0.06160141917986105,
          0.06307194818922375,
          0.07401453152455637,
          0.05424495161456294,
          0.056801715615992565,
          0.05115855582163603,
          0.06993698443517078,
          0.07834552172780411,
          0.03552884612914192,
          0.02604903658609433,
          0.06587642139679076,
          0.03978347012413135,
          0.056839603098621926,
          0.05424495161456294,
          0.05176224216611455,
          0.06680001173460968,
          0.04343657581058706,
          0.02762162933257986,
          0.07547245332503623,
          0.06184825984939649,
          0.0845352918580344,
          0.06212768737942697,
          0.0845352918580344,
          0.07549218734978944,
          0.05930682103566837,
          0.06354258624674683,
          0.07211117091616746,
          0.09039974916686178,
          0.07633498245880546,
          0.10442053400914988,
          0.06810384149155116,
          0.03510268576692521,
          0.03510268576692521,
          0.04477731599495619,
          0.06753949983811484,
          0.05746795976364986,
          0.08977295952423799,
          0.07239446661758443,
          0.06961178020806474,
          0.10490268743118485,
          0.09642350929819948,
          0.08936378259430038,
          0.11829951633828506,
          0.10911626985542751,
          0.12698824039408307,
          0.09752511063270021,
          0.07208186796494717,
          0.10652620650887502,
          0.11700098019347288,
          0.1034844779375939,
          0.09390428519781249,
          0.06167953807871132,
          0.09925539285795647,
          0.08720231541707267,
          0.08836812192022439,
          0.09839313457050548,
          0.09558324175429161,
          0.09017222379089153,
          0.1175203707538289,
          0.142764376789865,
          0.1177551064749565,
          0.15011261481932578,
          0.11827002378459288,
          0.08998686414687745,
          0.0666860295910702,
          0.0666860295910702,
          0.0691554614568157,
          0.0488298545876085,
          0.26885821230112283,
          0.0239588080952123,
          0.08680934203865573,
          0.09241171186070009,
          0.07648861308777903,
          0.08371200388512806,
          0.0975064455044356,
          0.09322099641247047,
          0.07895550397911348,
          0.07489164046585463,
          0.09493152621089164,
          0.07491613428072831,
          0.08545205258924359,
          0.0752597058965299,
          0.10653966472984146,
          0.09991443964073983,
          0.10439279751792371,
          0.08500572616877262,
          0.10722101616097134,
          0.09769939900509872,
          0.09387421319403011,
          0.11076907106036098,
          0.09663923348783285,
          0.0239588080952123,
          0.1183068904715914,
          0.12271074221910759,
          0.12890434104441822,
          0.07821858745033676,
          0.12404838002455758,
          0.13281046196200283,
          0.08876746574991984,
          0.08876746574991984,
          0.08876746574991984,
          0.08876746574991984,
          0.057231490968297,
          0.05400292376678172,
          0.06064070515174188,
          0.06423916446764547,
          0.07492103392707711,
          0.03275269054695647,
          0.09496190142204387,
          0.08111485698960261,
          0.1428968021684384,
          0.1428968021684384,
          0.07197938000882674,
          0.048107291582241245,
          0.1602483853729048,
          0.1775983219666359,
          0.2853948555553885,
          0.08174022101420018
         ]
        }
       ],
       "layout": {
        "legend": {
         "title": {
          "text": "Legendary Status"
         },
         "tracegroupgap": 0
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Additive Model: Fire Type Prediction by Attack and Legendary Status"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Attack"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Fire Type (1=Fire)"
         }
        }
       }
      },
      "text/html": [
       "<div>                            <div id=\"f6b6f7a2-7169-4c8b-9b6c-3252d257b023\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"f6b6f7a2-7169-4c8b-9b6c-3252d257b023\")) {                    Plotly.newPlot(                        \"f6b6f7a2-7169-4c8b-9b6c-3252d257b023\",                        [{\"hovertemplate\":\"Legendary Status=False\\u003cbr\\u003eAttack=%{x}\\u003cbr\\u003eFire Type (1=Fire)=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"False\",\"marker\":{\"color\":\"#636efa\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"False\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[49,62,82,100,52,64,84,130,104,48,63,83,103,30,20,45,35,25,90,150,45,60,80,80,56,81,60,90,60,85,55,90,75,100,47,62,92,57,72,102,45,70,41,76,45,70,45,80,50,65,80,70,95,55,65,55,80,45,70,52,82,80,105,70,110,50,65,95,20,35,50,50,80,100,130,75,90,105,40,70,80,95,120,85,100,65,75,75,35,60,65,85,110,45,70,80,105,65,95,35,50,65,65,45,48,73,105,130,30,50,40,95,50,80,120,105,55,65,90,85,130,5,55,95,125,40,65,67,92,45,75,45,110,50,83,95,125,155,100,10,125,155,85,48,55,65,65,130,60,40,60,80,115,105,135,110,64,84,134,100,49,62,82,52,64,84,65,80,105,46,76,30,50,20,35,60,90,90,38,58,40,25,30,20,40,50,75,40,55,75,95,80,20,50,100,75,35,45,55,70,30,75,65,45,85,65,65,85,75,60,72,33,80,65,90,70,75,85,125,80,120,95,130,150,10,125,185,95,80,130,40,50,50,100,55,65,105,55,40,80,60,90,90,95,60,120,80,95,20,35,95,30,63,75,80,10,64,84,134,164,100,45,65,85,110,60,85,120,160,70,85,110,150,55,90,30,70,45,35,70,35,50,30,50,70,40,70,100,55,85,30,50,25,35,65,85,30,60,40,130,60,80,160,45,90,90,51,71,91,60,120,20,45,45,65,75,85,85,105,70,90,110,140,40,60,100,45,75,75,50,40,73,47,60,43,73,90,120,140,70,90,60,100,120,85,25,45,60,100,70,100,85,115,40,70,110,115,100,55,95,48,78,80,120,40,70,41,81,95,125,15,60,70,90,75,115,165,40,70,68,50,130,150,23,50,80,120,40,60,80,64,104,84,90,30,75,95,135,145,55,75,135,145,68,89,109,58,78,104,51,66,86,55,75,120,45,85,25,85,65,85,120,30,70,125,165,42,52,29,59,79,69,94,30,80,45,65,105,35,60,48,83,100,50,80,66,76,136,60,125,55,82,30,63,93,24,89,80,25,5,65,92,70,90,130,170,85,70,110,145,72,112,50,90,61,106,100,49,69,20,62,92,132,120,70,85,140,100,123,95,50,76,110,60,95,130,80,125,165,55,100,80,50,65,65,65,65,65,70,80,100,45,60,75,63,93,123,55,75,100,55,85,60,80,110,50,88,53,98,53,98,53,98,25,55,55,77,115,60,100,75,105,135,45,57,85,135,60,60,80,105,140,50,65,95,100,125,53,63,103,45,55,100,27,67,35,60,92,72,82,117,90,140,30,86,65,95,75,90,58,30,50,78,108,112,140,50,95,65,105,50,95,30,45,55,30,40,65,44,87,50,65,95,60,100,75,75,135,55,85,40,60,75,47,77,50,94,55,80,100,55,85,115,55,75,30,40,55,87,117,147,70,110,50,40,70,66,85,125,120,74,124,85,125,110,83,123,55,65,97,109,65,85,105,85,60,72,72,77,128,120,61,78,107,45,59,69,56,63,95,36,56,50,73,81,35,22,52,50,68,38,45,65,65,100,82,124,80,48,48,48,80,110,150,50,52,72,48,80,54,92,52,105,60,75,53,73,38,55,89,121,59,77,65,92,58,50,50,75,100,80,70,110,66,66,66,66,90,85,95,100,69,117,30,70],\"xaxis\":\"x\",\"y\":[0,0,0,0,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,1,1,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"Legendary Status=True\\u003cbr\\u003eAttack=%{x}\\u003cbr\\u003eFire Type (1=Fire)=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"True\",\"marker\":{\"color\":\"#EF553B\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"True\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[85,90,100,110,190,150,85,115,75,90,130,100,50,75,80,100,90,130,100,150,150,180,150,180,100,150,180,70,95,75,105,125,120,120,90,160,100,120,90,100,103,120,100,90,129,90,115,100,115,105,120,150,125,145,130,170,120,131,131,100,100,160,110,160,110],\"xaxis\":\"x\",\"y\":[0,0,1,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"mode\":\"lines\",\"name\":\"Best Fit Line\",\"x\":[49,62,82,100,52,64,84,130,104,48,63,83,103,30,20,45,35,25,90,150,45,60,80,80,56,81,60,90,60,85,55,90,75,100,47,62,92,57,72,102,45,70,41,76,45,70,45,80,50,65,80,70,95,55,65,55,80,45,70,52,82,80,105,70,110,50,65,95,20,35,50,50,80,100,130,75,90,105,40,70,80,95,120,85,100,65,75,75,35,60,65,85,110,45,70,80,105,65,95,35,50,65,65,45,48,73,105,130,30,50,40,95,50,80,120,105,55,65,90,85,130,5,55,95,125,40,65,67,92,45,75,45,110,50,83,95,125,155,100,10,125,155,85,48,55,65,65,130,60,40,60,80,115,105,135,110,85,90,100,64,84,134,110,190,150,100,49,62,82,52,64,84,65,80,105,46,76,30,50,20,35,60,90,90,38,58,40,25,30,20,40,50,75,40,55,75,95,80,20,50,100,75,35,45,55,70,30,75,65,45,85,65,65,85,75,60,72,33,80,65,90,70,75,85,125,80,120,95,130,150,10,125,185,95,80,130,40,50,50,100,55,65,105,55,40,80,60,90,90,95,60,120,80,95,20,35,95,30,63,75,80,10,85,115,75,64,84,134,164,90,130,100,45,65,85,110,60,85,120,160,70,85,110,150,55,90,30,70,45,35,70,35,50,30,50,70,40,70,100,55,85,30,50,25,35,65,85,30,60,40,130,60,80,160,45,90,90,51,71,91,60,120,20,45,45,65,75,85,85,105,70,90,110,140,40,60,100,45,75,75,50,40,73,47,60,43,73,90,120,140,70,90,60,100,120,85,25,45,60,100,70,100,85,115,40,70,110,115,100,55,95,48,78,80,120,40,70,41,81,95,125,15,60,70,90,75,115,165,40,70,68,50,130,150,23,50,80,120,40,60,80,64,104,84,90,30,75,95,135,145,55,75,135,145,100,50,75,80,100,90,130,100,150,150,180,150,180,100,150,180,70,95,68,89,109,58,78,104,51,66,86,55,75,120,45,85,25,85,65,85,120,30,70,125,165,42,52,29,59,79,69,94,30,80,45,65,105,35,60,48,83,100,50,80,66,76,136,60,125,55,82,30,63,93,24,89,80,25,5,65,92,70,90,130,170,85,70,110,145,72,112,50,90,61,106,100,49,69,20,62,92,132,120,70,85,140,100,123,95,50,76,110,60,95,130,80,125,165,55,100,80,50,65,65,65,65,65,75,105,125,120,120,90,160,100,120,70,80,100,90,100,103,120,100,45,60,75,63,93,123,55,75,100,55,85,60,80,110,50,88,53,98,53,98,53,98,25,55,55,77,115,60,100,75,105,135,45,57,85,135,60,60,80,105,140,50,65,95,100,125,53,63,103,45,55,100,27,67,35,60,92,72,82,117,90,140,30,86,65,95,75,90,58,30,50,78,108,112,140,50,95,65,105,50,95,30,45,55,30,40,65,44,87,50,65,95,60,100,75,75,135,55,85,40,60,75,47,77,50,94,55,80,100,55,85,115,55,75,30,40,55,87,117,147,70,110,50,40,70,66,85,125,120,74,124,85,125,110,83,123,55,65,97,109,65,85,105,85,60,90,129,90,115,100,115,105,120,150,125,145,130,170,120,72,72,77,128,120,61,78,107,45,59,69,56,63,95,36,56,50,73,81,35,22,52,50,68,38,45,65,65,100,82,124,80,48,48,48,80,110,150,50,52,72,48,80,54,92,52,105,60,75,53,73,38,55,89,121,59,77,65,92,58,50,50,75,100,80,70,110,66,66,66,66,90,85,95,100,69,117,30,70,131,131,100,100,160,110,160,110],\"y\":[0.0778792035498896,0.07457528161559435,0.0712321660124653,0.05033595114580418,0.08738396566787178,0.08166045866295452,0.07802650338600178,0.08384359796351015,0.09765741311026806,0.061795225440030674,0.05967877561128276,0.056962347200456474,0.054362415567981694,0.07597234251759417,0.051331631285316835,0.07340832653030371,0.08603521380068797,0.05833678402022263,0.1377140625115454,0.25037942720279543,0.08411000925839532,0.08129551520611489,0.07767642654273574,0.07254772238781808,0.10168402706094565,0.09617294700024251,0.11348661492439359,0.09942142914344392,0.09428537112416968,0.089135495510495,0.09408037938681553,0.11345105704728022,0.06395199548364343,0.0603485368499751,0.07307253887463278,0.070598911994945,0.07557637733237298,0.09619753046340021,0.09055714284895756,0.09679518289098149,0.07544318617375528,0.07124152082002845,0.08039749255389189,0.0742248672541757,0.10984152949049947,0.10393948391281894,0.0899768013923462,0.08313520028839025,0.07257150974514233,0.07011356061512246,0.06773278067640122,0.09096539197676239,0.08597963952337909,0.08222400922730853,0.08037658773895254,0.11473685339596958,0.10860386236351044,0.0899768013923462,0.08504016401852565,0.08167106257215709,0.07627551663193786,0.13198456006983642,0.12505987367297425,0.10393948391281894,0.10162592259416689,0.08896924766648504,0.07508411485939397,0.07009051803188066,0.08899790272342491,0.08603521380068797,0.0831621478493682,0.06323655774511776,0.10860386236351044,0.10390656675595059,0.12638039633412237,0.12509855657960758,0.1210938414889567,0.11720015371484736,0.0850676717669137,0.07946714893076463,0.055012741530021954,0.05311543656662521,0.057603490129230446,0.1074123439088672,0.10390656675595059,0.07508411485939397,0.04509689989871021,0.016516663793560208,0.049554624664540046,0.04672191249967923,0.08600742259571308,0.12241631331865278,0.11592658865244627,0.06854048822893367,0.06469635468082467,0.10860386236351044,0.10276064243480137,0.046174147439080396,0.021025046086448812,0.08603521380068797,0.0831621478493682,0.08037658773895254,0.061073956619538274,0.015364227477180328,0.08130607592172717,0.07680501299003055,0.08405556419335103,0.07941544863518794,0.06180752272944342,0.05900027875437931,0.04564789353260616,0.08035046369610141,0.04154031865455491,0.04781993514216057,0.16009586539166004,0.09743966123289735,0.0583173691044825,0.04953797875902126,0.0467061713713484,0.06248716069049193,0.07418115783701619,0.08606301314013812,0.03316322625800539,0.08597963952337909,0.09193423310276908,0.052532018077142605,0.04953797875902126,0.08221333995253108,0.10164528747721638,0.06854048822893367,0.06395199548364343,0.05969067852190885,0.10162592259416689,0.0951397706623225,0.10233138787366125,0.11670385239159489,0.09193423310276908,0.09825688532124123,0.07420543792264037,0.04566329507976036,0.12131212403768077,0.11362173984754793,0.07679498803967633,0.07805702340395608,0.08222400922730853,0.08037658773895254,0.08037658773895254,0.16274572543276616,0.06620932037062709,0.03437370256470546,0.03050726320317858,0.06321562161891886,0.07676993091210067,0.11720015371484736,0.12502120101857886,0.12371286917682836,0.0644756400362856,0.08380185621482869,0.08765093804729066,0.0972657862648554,0.09300771405973435,0.10854228152602902,0.09799613824563486,0.20042524083143445,0.1927246396283149,0.06928869433835616,0.05999441297073303,0.05660801590032262,0.054023330526859724,0.08395437777121009,0.0784361967915962,0.0749340159695256,0.0730935513036451,0.06965597441527557,0.07043857823879059,0.08861696307069804,0.08280391669129904,0.07810988735235797,0.07462109652171854,0.06970180224513926,0.06284163537131762,0.0955994340763169,0.0893732716144001,0.07805899519582597,0.07669680355917763,0.07326611230803595,0.10680669925990925,0.07584043114320155,0.09563000041700538,0.04276838309956517,0.04078828534119071,0.079883751315965,0.07545496047672984,0.0763472130260245,0.07377155547625308,0.06138019722981453,0.05859114990970278,0.05659669070380603,0.05281551612985451,0.04922411080661741,0.05401249282563697,0.0704617265780467,0.07212277757730154,0.07048488194774859,0.05999042618449305,0.08740870535655358,0.07810988735235797,0.09243928311957274,0.09453545914662781,0.07547962349725082,0.06885875372257205,0.07719875657099733,0.038436421906769776,0.12247243233165908,0.06577541228233276,0.0729309239731691,0.09817172562431845,0.05494074891375231,0.08545563995714592,0.05097279902098366,0.03372379836875294,0.07127608527953204,0.04640936940390263,0.013347874618109816,0.014005555021090486,0.1044367557109859,0.1164737551818854,0.08837181987852621,0.09342220886710254,0.06802515633060227,0.0034416822427090683,0.1229517271551396,0.1397014006457563,0.11524309017581898,0.1044367557109859,0.12973708258230648,0.0763472130260245,0.0278760735929937,0.0854832695424571,0.08738051290180211,0.04864850719571135,0.1079582661485159,0.09879289057318502,0.08452136059135346,0.050394743700227874,0.02993846024885029,0.10915509990711743,0.1165101359303687,0.0680699897214043,0.06728960793147293,0.0729309239731691,0.06351084718275148,0.060671516869671226,0.10511289032069776,0.06506249186489453,0.07722394195657362,0.06728960793147293,0.09563000041700538,0.10285446193673707,0.11729079314979207,0.04920757017607366,0.08174738176913593,0.08711772364010219,0.10639144403528727,0.04465373370364313,0.08741434483438965,0.08354763190070692,0.08540592710457727,0.06959186228284227,0.043097401406522545,0.11737352706152542,0.06651753417855881,0.04980202128083499,0.05466165261800426,0.052160923253902806,0.060646382539537635,0.05530426715099306,0.05593532023481717,0.0728772182045986,0.09982806859796924,0.05402607799566056,0.04862809577708435,0.049186936319353204,0.05923277019653086,0.05595398924407307,0.05155284001004668,0.038357518719629315,0.04629927684520145,0.04980202128083499,0.03334305149366718,0.05402607799566056,0.03334305149366718,0.03216766068382396,0.044821223812123646,0.042750327936803535,0.0407710304952886,0.0379916402330469,0.0620915046400144,0.06651209305823733,0.059985484670844455,0.05593532023481717,0.044821223812123646,0.02089330091063519,0.045353702260394,0.04429470689733634,0.0412574707563407,0.052160923253902806,0.04357337660561843,0.04058350091278922,0.032946718368158115,0.07122350449976594,0.04174946214772013,0.03981457642495954,0.07624139335996377,0.022731897301242893,0.07290110510677855,0.07290110510677855,0.06311588599737623,0.06025303370176423,0.057512065205320186,0.0635476930882544,0.08350845240680727,0.03455985689993766,0.011824763386403706,0.0432593182570273,0.0412574707563407,0.04029008451826815,0.022181169084130335,0.0393444502608525,0.02819141152833082,0.026564060832405822,0.018962388949915896,0.01350585883091894,0.009371655970519558,0.035382665073528725,0.03373246251083589,0.04694004968323021,0.04642113608480417,0.0497852966252838,0.03752942735624799,0.04922000969674103,0.0379916402330469,0.052167914145888665,0.038441021956481684,0.05157012622069959,0.03772135816290273,0.03511828646173908,0.10213776432790236,0.10907785750847969,0.09134095358963741,0.06653404338953883,0.07290110510677855,0.05530426715099306,0.05790790344468143,0.048042940455889194,0.017851795638247472,0.03937117957616087,0.03255493761032675,0.04174946214772013,0.0816612433406606,0.05402607799566056,0.050356731964426964,0.07374142024459276,0.07892162925722301,0.032946718368158115,0.030662635528179626,0.037072621135608344,0.07892162925722301,0.06651209305823733,0.03665859437693521,0.04426478899568344,0.0460937578507306,0.04293870525951345,0.049203470372963916,0.059252470690302844,0.035382665073528725,0.024719679299839894,0.02613077330423566,0.03162428672150025,0.07206962065789134,0.05093437465648951,0.043273950444295405,0.03185886894527125,0.0407710304952886,0.05155284001004668,0.07045598717490818,0.07371727989659725,0.1126654661120948,0.02140534768057254,0.017218750745578756,0.03309338999512992,0.03216766068382396,0.093414787186595,0.11642836904789243,0.03194836684956462,0.042750327936803535,0.03981457642495954,0.06350563527684654,0.0379916402330469,0.036224327811331086,0.03453627658467804,0.03066473685721459,0.03707514494329256,0.029227905317839167,0.021914504170556598,0.031417180994741285,0.0497852966252838,0.035782816597505804,0.07539954719575848,0.042204174960970106,0.029588989956961347,0.02820109732184806,0.037503899600969094,0.031752221073141323,0.009931022366219901,0.023204979386183402,0.015201984774136004,0.038298397817006065,0.03166176260116322,0.04961275338996774,0.0597400909847401,0.04845935857486217,0.08607662605178719,0.04306464429095987,0.046197342751424626,0.08607662605178719,0.10514654606438616,0.04208523430242795,0.14533239872815013,0.2769451830253243,0.012367967776815625,0.045701659927731765,0.03807800772371682,0.03622038293041545,0.03453250894819365,0.044922392187793266,0.05073456957441087,0.05265015345571131,0.03640835245950497,0.03512883217908725,0.0334900570473303,0.05280194428600104,0.05038176082699803,0.06425536348001416,0.04079149961829379,0.04921141388707768,0.031727047366488154,0.055817517317384754,0.05609003841387234,0.057394968038699035,0.056710606954298824,0.03667707779265358,0.03843945827495548,0.10212091954816699,0.12159528929199748,0.012784887995347464,0.006949035169807851,0.03142449965398287,0.02532901259936714,0.02413596739889386,0.024725477598006813,0.06280901015359398,0.033193953676417226,0.025505825274807593,0.02657759496206482,0.055313138292238086,0.06652263540708826,0.03374952224646083,0.03179006491683912,0.037727521534646244,0.042945683059368196,0.05389403980978426,0.047086985394700034,0.05806774222571661,0.04933722533435347,0.031482624274230084,0.05539812112674581,0.03666459071775562,0.08697649400260805,0.044608102055541804,0.04491026106892576,0.029604018388542173,0.04566498834155267,0.04897716503804867,0.016377823885666592,0.023222756960889327,0.028205750753172134,0.029961386271306047,0.04178455166777735,0.04808305409043205,0.027012511079992772,0.050976773225118927,0.04863595115305891,0.0509425826393926,0.06132385956117622,0.06502449546831697,0.0546704266452595,0.05724603097934296,0.06679963889791404,0.032707531475074804,0.029717914595919742,0.021151322980203294,0.025625544324852458,0.049224646949874384,0.05859178179035882,0.04954979549814157,0.03406668755890499,0.03247579418513524,0.026268535038102413,0.04326049334331681,0.043245865500608216,0.04530903884722877,0.06884135892559709,0.01874330070134123,0.029940846205373756,0.034932926076295924,0.02327571681648246,0.06931399944733126,0.05013550106827903,0.019675159753926454,0.030594236511407868,0.024418881836953277,0.017860972398891287,0.021918143654674368,0.06278404604955384,0.0402966500237168,0.07288869066698661,0.0762533518591179,0.010095017961986443,0.020144008435091294,0.0402966500237168,0.02551461259987152,0.01981468024737757,0.01981468024737757,0.01981468024737757,0.01981468024737757,0.01981468024737757,0.017796998533152007,0.03652333129834512,0.07518162591661023,0.03523991802522636,0.04678445296078474,0.030124704418237183,0.06475990792115037,0.02776991602534843,0.04678445296078474,0.017432201848506624,0.03495676740607067,0.03332574192856722,0.03785165173966701,0.03696104492703549,0.05447099912476642,0.03523991802522636,0.04698367029339886,0.04193500334548041,0.03769695184331435,0.03387206526782026,0.059538670965454975,0.0732001217276273,0.08969728820029714,0.05426308887321426,0.05552801508368204,0.05237265898929727,0.058994918258762456,0.05500772214557212,0.05750626454536555,0.054882951726436,0.05486461930012356,0.05724635376439371,0.0740436082909864,0.05083364719247551,0.06944670572019988,0.05083364719247551,0.06944670572019988,0.05083364719247551,0.06944670572019988,0.03816114614823134,0.030800488827525906,0.050595483136409006,0.05526913782187314,0.06677797993449283,0.06884174231733056,0.07105298795982602,0.039052957596796056,0.041906609367730295,0.041892419273922295,0.049663360945811974,0.048281023621246304,0.08200897875213901,0.10949394511959605,0.03222598130479557,0.018109460729118888,0.06306777061389324,0.05550947984346238,0.07233227988041738,0.054901289922508166,0.053007613142099,0.05682065638967227,0.05237265898929727,0.08013990504082703,0.03725593140048476,0.0315410355983049,0.05814928004438054,0.03962450095375709,0.025191686024607626,0.04951679355298021,0.03155399514884795,0.03552400142748583,0.04000545076471995,0.03769695184331435,0.06305941623745125,0.07576241024372142,0.07404845544036115,0.06832706769121595,0.08108269838663608,0.12340392690761114,0.01709420382621266,0.05722346560040836,0.03469092176947971,0.0279801421024181,0.04827127910996454,0.030422997958392574,0.034283816256443694,0.02283493546599119,0.012168586059409228,0.03131086212863757,0.029136770655887326,0.10366094046783862,0.10829384772484588,0.04028321338592958,0.051526873294001255,0.06529606202547776,0.07836083745508547,0.054901289922508166,0.06992778803032845,0.0377097767522209,0.033883634728039756,0.026683994973324097,0.04345126523111923,0.042434716642482564,0.03999187755554344,0.04447901432197133,0.06119821778633414,0.04772231971338908,0.04606419377013045,0.04940382672052243,0.05363188560730239,0.06452393953207805,0.05552801508368204,0.06835857724801338,0.05949118620358618,0.05426308887321426,0.05424495161456294,0.042434716642482564,0.04046941391973442,0.041920804057800916,0.0460735143065629,0.05683202370270691,0.026621659551338554,0.025362321871514236,0.03814817381637547,0.03594283569127129,0.03426743557984852,0.058180262807805,0.05424495161456294,0.06677797993449283,0.047163406213147054,0.044989390465017404,0.03511945119394209,0.03682238034386355,0.028670629265531968,0.06379409443850588,0.07835062666587164,0.08382532205173547,0.06915043713909627,0.06304688660813383,0.06308866107159053,0.025745960484903047,0.06915043713909627,0.03560884711717472,0.07155373470428858,0.09806691612338947,0.06160141917986105,0.06307194818922375,0.07401453152455637,0.05424495161456294,0.056801715615992565,0.05115855582163603,0.06993698443517078,0.07834552172780411,0.03552884612914192,0.02604903658609433,0.06587642139679076,0.03978347012413135,0.056839603098621926,0.05424495161456294,0.05176224216611455,0.06680001173460968,0.04343657581058706,0.02762162933257986,0.07547245332503623,0.06184825984939649,0.0845352918580344,0.06212768737942697,0.0845352918580344,0.07549218734978944,0.05930682103566837,0.06354258624674683,0.07211117091616746,0.09039974916686178,0.07633498245880546,0.10442053400914988,0.06810384149155116,0.03510268576692521,0.03510268576692521,0.04477731599495619,0.06753949983811484,0.05746795976364986,0.08977295952423799,0.07239446661758443,0.06961178020806474,0.10490268743118485,0.09642350929819948,0.08936378259430038,0.11829951633828506,0.10911626985542751,0.12698824039408307,0.09752511063270021,0.07208186796494717,0.10652620650887502,0.11700098019347288,0.1034844779375939,0.09390428519781249,0.06167953807871132,0.09925539285795647,0.08720231541707267,0.08836812192022439,0.09839313457050548,0.09558324175429161,0.09017222379089153,0.1175203707538289,0.142764376789865,0.1177551064749565,0.15011261481932578,0.11827002378459288,0.08998686414687745,0.0666860295910702,0.0666860295910702,0.0691554614568157,0.0488298545876085,0.26885821230112283,0.0239588080952123,0.08680934203865573,0.09241171186070009,0.07648861308777903,0.08371200388512806,0.0975064455044356,0.09322099641247047,0.07895550397911348,0.07489164046585463,0.09493152621089164,0.07491613428072831,0.08545205258924359,0.0752597058965299,0.10653966472984146,0.09991443964073983,0.10439279751792371,0.08500572616877262,0.10722101616097134,0.09769939900509872,0.09387421319403011,0.11076907106036098,0.09663923348783285,0.0239588080952123,0.1183068904715914,0.12271074221910759,0.12890434104441822,0.07821858745033676,0.12404838002455758,0.13281046196200283,0.08876746574991984,0.08876746574991984,0.08876746574991984,0.08876746574991984,0.057231490968297,0.05400292376678172,0.06064070515174188,0.06423916446764547,0.07492103392707711,0.03275269054695647,0.09496190142204387,0.08111485698960261,0.1428968021684384,0.1428968021684384,0.07197938000882674,0.048107291582241245,0.1602483853729048,0.1775983219666359,0.2853948555553885,0.08174022101420018],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Attack\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Fire Type (1=Fire)\"}},\"legend\":{\"title\":{\"text\":\"Legendary Status\"},\"tracegroupgap\":0},\"title\":{\"text\":\"Additive Model: Fire Type Prediction by Attack and Legendary Status\"}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('f6b6f7a2-7169-4c8b-9b6c-3252d257b023');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "Legendary Status=False<br>Attack=%{x}<br>Fire Type (1=Fire)=%{y}<extra></extra>",
         "legendgroup": "False",
         "marker": {
          "color": "#636efa",
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "False",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          49,
          62,
          82,
          100,
          52,
          64,
          84,
          130,
          104,
          48,
          63,
          83,
          103,
          30,
          20,
          45,
          35,
          25,
          90,
          150,
          45,
          60,
          80,
          80,
          56,
          81,
          60,
          90,
          60,
          85,
          55,
          90,
          75,
          100,
          47,
          62,
          92,
          57,
          72,
          102,
          45,
          70,
          41,
          76,
          45,
          70,
          45,
          80,
          50,
          65,
          80,
          70,
          95,
          55,
          65,
          55,
          80,
          45,
          70,
          52,
          82,
          80,
          105,
          70,
          110,
          50,
          65,
          95,
          20,
          35,
          50,
          50,
          80,
          100,
          130,
          75,
          90,
          105,
          40,
          70,
          80,
          95,
          120,
          85,
          100,
          65,
          75,
          75,
          35,
          60,
          65,
          85,
          110,
          45,
          70,
          80,
          105,
          65,
          95,
          35,
          50,
          65,
          65,
          45,
          48,
          73,
          105,
          130,
          30,
          50,
          40,
          95,
          50,
          80,
          120,
          105,
          55,
          65,
          90,
          85,
          130,
          5,
          55,
          95,
          125,
          40,
          65,
          67,
          92,
          45,
          75,
          45,
          110,
          50,
          83,
          95,
          125,
          155,
          100,
          10,
          125,
          155,
          85,
          48,
          55,
          65,
          65,
          130,
          60,
          40,
          60,
          80,
          115,
          105,
          135,
          110,
          64,
          84,
          134,
          100,
          49,
          62,
          82,
          52,
          64,
          84,
          65,
          80,
          105,
          46,
          76,
          30,
          50,
          20,
          35,
          60,
          90,
          90,
          38,
          58,
          40,
          25,
          30,
          20,
          40,
          50,
          75,
          40,
          55,
          75,
          95,
          80,
          20,
          50,
          100,
          75,
          35,
          45,
          55,
          70,
          30,
          75,
          65,
          45,
          85,
          65,
          65,
          85,
          75,
          60,
          72,
          33,
          80,
          65,
          90,
          70,
          75,
          85,
          125,
          80,
          120,
          95,
          130,
          150,
          10,
          125,
          185,
          95,
          80,
          130,
          40,
          50,
          50,
          100,
          55,
          65,
          105,
          55,
          40,
          80,
          60,
          90,
          90,
          95,
          60,
          120,
          80,
          95,
          20,
          35,
          95,
          30,
          63,
          75,
          80,
          10,
          64,
          84,
          134,
          164,
          100,
          45,
          65,
          85,
          110,
          60,
          85,
          120,
          160,
          70,
          85,
          110,
          150,
          55,
          90,
          30,
          70,
          45,
          35,
          70,
          35,
          50,
          30,
          50,
          70,
          40,
          70,
          100,
          55,
          85,
          30,
          50,
          25,
          35,
          65,
          85,
          30,
          60,
          40,
          130,
          60,
          80,
          160,
          45,
          90,
          90,
          51,
          71,
          91,
          60,
          120,
          20,
          45,
          45,
          65,
          75,
          85,
          85,
          105,
          70,
          90,
          110,
          140,
          40,
          60,
          100,
          45,
          75,
          75,
          50,
          40,
          73,
          47,
          60,
          43,
          73,
          90,
          120,
          140,
          70,
          90,
          60,
          100,
          120,
          85,
          25,
          45,
          60,
          100,
          70,
          100,
          85,
          115,
          40,
          70,
          110,
          115,
          100,
          55,
          95,
          48,
          78,
          80,
          120,
          40,
          70,
          41,
          81,
          95,
          125,
          15,
          60,
          70,
          90,
          75,
          115,
          165,
          40,
          70,
          68,
          50,
          130,
          150,
          23,
          50,
          80,
          120,
          40,
          60,
          80,
          64,
          104,
          84,
          90,
          30,
          75,
          95,
          135,
          145,
          55,
          75,
          135,
          145,
          68,
          89,
          109,
          58,
          78,
          104,
          51,
          66,
          86,
          55,
          75,
          120,
          45,
          85,
          25,
          85,
          65,
          85,
          120,
          30,
          70,
          125,
          165,
          42,
          52,
          29,
          59,
          79,
          69,
          94,
          30,
          80,
          45,
          65,
          105,
          35,
          60,
          48,
          83,
          100,
          50,
          80,
          66,
          76,
          136,
          60,
          125,
          55,
          82,
          30,
          63,
          93,
          24,
          89,
          80,
          25,
          5,
          65,
          92,
          70,
          90,
          130,
          170,
          85,
          70,
          110,
          145,
          72,
          112,
          50,
          90,
          61,
          106,
          100,
          49,
          69,
          20,
          62,
          92,
          132,
          120,
          70,
          85,
          140,
          100,
          123,
          95,
          50,
          76,
          110,
          60,
          95,
          130,
          80,
          125,
          165,
          55,
          100,
          80,
          50,
          65,
          65,
          65,
          65,
          65,
          70,
          80,
          100,
          45,
          60,
          75,
          63,
          93,
          123,
          55,
          75,
          100,
          55,
          85,
          60,
          80,
          110,
          50,
          88,
          53,
          98,
          53,
          98,
          53,
          98,
          25,
          55,
          55,
          77,
          115,
          60,
          100,
          75,
          105,
          135,
          45,
          57,
          85,
          135,
          60,
          60,
          80,
          105,
          140,
          50,
          65,
          95,
          100,
          125,
          53,
          63,
          103,
          45,
          55,
          100,
          27,
          67,
          35,
          60,
          92,
          72,
          82,
          117,
          90,
          140,
          30,
          86,
          65,
          95,
          75,
          90,
          58,
          30,
          50,
          78,
          108,
          112,
          140,
          50,
          95,
          65,
          105,
          50,
          95,
          30,
          45,
          55,
          30,
          40,
          65,
          44,
          87,
          50,
          65,
          95,
          60,
          100,
          75,
          75,
          135,
          55,
          85,
          40,
          60,
          75,
          47,
          77,
          50,
          94,
          55,
          80,
          100,
          55,
          85,
          115,
          55,
          75,
          30,
          40,
          55,
          87,
          117,
          147,
          70,
          110,
          50,
          40,
          70,
          66,
          85,
          125,
          120,
          74,
          124,
          85,
          125,
          110,
          83,
          123,
          55,
          65,
          97,
          109,
          65,
          85,
          105,
          85,
          60,
          72,
          72,
          77,
          128,
          120,
          61,
          78,
          107,
          45,
          59,
          69,
          56,
          63,
          95,
          36,
          56,
          50,
          73,
          81,
          35,
          22,
          52,
          50,
          68,
          38,
          45,
          65,
          65,
          100,
          82,
          124,
          80,
          48,
          48,
          48,
          80,
          110,
          150,
          50,
          52,
          72,
          48,
          80,
          54,
          92,
          52,
          105,
          60,
          75,
          53,
          73,
          38,
          55,
          89,
          121,
          59,
          77,
          65,
          92,
          58,
          50,
          50,
          75,
          100,
          80,
          70,
          110,
          66,
          66,
          66,
          66,
          90,
          85,
          95,
          100,
          69,
          117,
          30,
          70
         ],
         "xaxis": "x",
         "y": [
          0,
          0,
          0,
          0,
          1,
          1,
          1,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "Legendary Status=True<br>Attack=%{x}<br>Fire Type (1=Fire)=%{y}<extra></extra>",
         "legendgroup": "True",
         "marker": {
          "color": "#EF553B",
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "True",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          85,
          90,
          100,
          110,
          190,
          150,
          85,
          115,
          75,
          90,
          130,
          100,
          50,
          75,
          80,
          100,
          90,
          130,
          100,
          150,
          150,
          180,
          150,
          180,
          100,
          150,
          180,
          70,
          95,
          75,
          105,
          125,
          120,
          120,
          90,
          160,
          100,
          120,
          90,
          100,
          103,
          120,
          100,
          90,
          129,
          90,
          115,
          100,
          115,
          105,
          120,
          150,
          125,
          145,
          130,
          170,
          120,
          131,
          131,
          100,
          100,
          160,
          110,
          160,
          110
         ],
         "xaxis": "x",
         "y": [
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1
         ],
         "yaxis": "y"
        },
        {
         "mode": "lines",
         "name": "Best Fit Line",
         "type": "scatter",
         "x": [
          49,
          62,
          82,
          100,
          52,
          64,
          84,
          130,
          104,
          48,
          63,
          83,
          103,
          30,
          20,
          45,
          35,
          25,
          90,
          150,
          45,
          60,
          80,
          80,
          56,
          81,
          60,
          90,
          60,
          85,
          55,
          90,
          75,
          100,
          47,
          62,
          92,
          57,
          72,
          102,
          45,
          70,
          41,
          76,
          45,
          70,
          45,
          80,
          50,
          65,
          80,
          70,
          95,
          55,
          65,
          55,
          80,
          45,
          70,
          52,
          82,
          80,
          105,
          70,
          110,
          50,
          65,
          95,
          20,
          35,
          50,
          50,
          80,
          100,
          130,
          75,
          90,
          105,
          40,
          70,
          80,
          95,
          120,
          85,
          100,
          65,
          75,
          75,
          35,
          60,
          65,
          85,
          110,
          45,
          70,
          80,
          105,
          65,
          95,
          35,
          50,
          65,
          65,
          45,
          48,
          73,
          105,
          130,
          30,
          50,
          40,
          95,
          50,
          80,
          120,
          105,
          55,
          65,
          90,
          85,
          130,
          5,
          55,
          95,
          125,
          40,
          65,
          67,
          92,
          45,
          75,
          45,
          110,
          50,
          83,
          95,
          125,
          155,
          100,
          10,
          125,
          155,
          85,
          48,
          55,
          65,
          65,
          130,
          60,
          40,
          60,
          80,
          115,
          105,
          135,
          110,
          85,
          90,
          100,
          64,
          84,
          134,
          110,
          190,
          150,
          100,
          49,
          62,
          82,
          52,
          64,
          84,
          65,
          80,
          105,
          46,
          76,
          30,
          50,
          20,
          35,
          60,
          90,
          90,
          38,
          58,
          40,
          25,
          30,
          20,
          40,
          50,
          75,
          40,
          55,
          75,
          95,
          80,
          20,
          50,
          100,
          75,
          35,
          45,
          55,
          70,
          30,
          75,
          65,
          45,
          85,
          65,
          65,
          85,
          75,
          60,
          72,
          33,
          80,
          65,
          90,
          70,
          75,
          85,
          125,
          80,
          120,
          95,
          130,
          150,
          10,
          125,
          185,
          95,
          80,
          130,
          40,
          50,
          50,
          100,
          55,
          65,
          105,
          55,
          40,
          80,
          60,
          90,
          90,
          95,
          60,
          120,
          80,
          95,
          20,
          35,
          95,
          30,
          63,
          75,
          80,
          10,
          85,
          115,
          75,
          64,
          84,
          134,
          164,
          90,
          130,
          100,
          45,
          65,
          85,
          110,
          60,
          85,
          120,
          160,
          70,
          85,
          110,
          150,
          55,
          90,
          30,
          70,
          45,
          35,
          70,
          35,
          50,
          30,
          50,
          70,
          40,
          70,
          100,
          55,
          85,
          30,
          50,
          25,
          35,
          65,
          85,
          30,
          60,
          40,
          130,
          60,
          80,
          160,
          45,
          90,
          90,
          51,
          71,
          91,
          60,
          120,
          20,
          45,
          45,
          65,
          75,
          85,
          85,
          105,
          70,
          90,
          110,
          140,
          40,
          60,
          100,
          45,
          75,
          75,
          50,
          40,
          73,
          47,
          60,
          43,
          73,
          90,
          120,
          140,
          70,
          90,
          60,
          100,
          120,
          85,
          25,
          45,
          60,
          100,
          70,
          100,
          85,
          115,
          40,
          70,
          110,
          115,
          100,
          55,
          95,
          48,
          78,
          80,
          120,
          40,
          70,
          41,
          81,
          95,
          125,
          15,
          60,
          70,
          90,
          75,
          115,
          165,
          40,
          70,
          68,
          50,
          130,
          150,
          23,
          50,
          80,
          120,
          40,
          60,
          80,
          64,
          104,
          84,
          90,
          30,
          75,
          95,
          135,
          145,
          55,
          75,
          135,
          145,
          100,
          50,
          75,
          80,
          100,
          90,
          130,
          100,
          150,
          150,
          180,
          150,
          180,
          100,
          150,
          180,
          70,
          95,
          68,
          89,
          109,
          58,
          78,
          104,
          51,
          66,
          86,
          55,
          75,
          120,
          45,
          85,
          25,
          85,
          65,
          85,
          120,
          30,
          70,
          125,
          165,
          42,
          52,
          29,
          59,
          79,
          69,
          94,
          30,
          80,
          45,
          65,
          105,
          35,
          60,
          48,
          83,
          100,
          50,
          80,
          66,
          76,
          136,
          60,
          125,
          55,
          82,
          30,
          63,
          93,
          24,
          89,
          80,
          25,
          5,
          65,
          92,
          70,
          90,
          130,
          170,
          85,
          70,
          110,
          145,
          72,
          112,
          50,
          90,
          61,
          106,
          100,
          49,
          69,
          20,
          62,
          92,
          132,
          120,
          70,
          85,
          140,
          100,
          123,
          95,
          50,
          76,
          110,
          60,
          95,
          130,
          80,
          125,
          165,
          55,
          100,
          80,
          50,
          65,
          65,
          65,
          65,
          65,
          75,
          105,
          125,
          120,
          120,
          90,
          160,
          100,
          120,
          70,
          80,
          100,
          90,
          100,
          103,
          120,
          100,
          45,
          60,
          75,
          63,
          93,
          123,
          55,
          75,
          100,
          55,
          85,
          60,
          80,
          110,
          50,
          88,
          53,
          98,
          53,
          98,
          53,
          98,
          25,
          55,
          55,
          77,
          115,
          60,
          100,
          75,
          105,
          135,
          45,
          57,
          85,
          135,
          60,
          60,
          80,
          105,
          140,
          50,
          65,
          95,
          100,
          125,
          53,
          63,
          103,
          45,
          55,
          100,
          27,
          67,
          35,
          60,
          92,
          72,
          82,
          117,
          90,
          140,
          30,
          86,
          65,
          95,
          75,
          90,
          58,
          30,
          50,
          78,
          108,
          112,
          140,
          50,
          95,
          65,
          105,
          50,
          95,
          30,
          45,
          55,
          30,
          40,
          65,
          44,
          87,
          50,
          65,
          95,
          60,
          100,
          75,
          75,
          135,
          55,
          85,
          40,
          60,
          75,
          47,
          77,
          50,
          94,
          55,
          80,
          100,
          55,
          85,
          115,
          55,
          75,
          30,
          40,
          55,
          87,
          117,
          147,
          70,
          110,
          50,
          40,
          70,
          66,
          85,
          125,
          120,
          74,
          124,
          85,
          125,
          110,
          83,
          123,
          55,
          65,
          97,
          109,
          65,
          85,
          105,
          85,
          60,
          90,
          129,
          90,
          115,
          100,
          115,
          105,
          120,
          150,
          125,
          145,
          130,
          170,
          120,
          72,
          72,
          77,
          128,
          120,
          61,
          78,
          107,
          45,
          59,
          69,
          56,
          63,
          95,
          36,
          56,
          50,
          73,
          81,
          35,
          22,
          52,
          50,
          68,
          38,
          45,
          65,
          65,
          100,
          82,
          124,
          80,
          48,
          48,
          48,
          80,
          110,
          150,
          50,
          52,
          72,
          48,
          80,
          54,
          92,
          52,
          105,
          60,
          75,
          53,
          73,
          38,
          55,
          89,
          121,
          59,
          77,
          65,
          92,
          58,
          50,
          50,
          75,
          100,
          80,
          70,
          110,
          66,
          66,
          66,
          66,
          90,
          85,
          95,
          100,
          69,
          117,
          30,
          70,
          131,
          131,
          100,
          100,
          160,
          110,
          160,
          110
         ],
         "y": [
          0.051870473179323924,
          0.0562301500356128,
          0.06452240255249116,
          0.059595518237456443,
          0.10921569516568261,
          0.08819827987208945,
          0.06974961890010782,
          0.10659853335945497,
          0.0955738197415481,
          0.056409578379128064,
          0.04730675941190599,
          0.037323707104900995,
          0.029382384994346398,
          0.09621588215197491,
          0.04732861630052293,
          0.048144460101753495,
          0.049311848481598815,
          0.03464484042083051,
          0.10776618489458129,
          0.25268481505313284,
          0.05285852656686191,
          0.05863268860969511,
          0.06725399523897199,
          0.06423324229943546,
          0.14259352530836222,
          0.10878377093351249,
          0.07377504356372007,
          0.08629716011335885,
          0.12014572151371682,
          0.09109299585472368,
          0.12360583408838692,
          0.14170934673517535,
          0.050001712140962835,
          0.0371936643882512,
          0.079414602415652,
          0.0668625145656812,
          0.07297755953456761,
          0.12737124532234215,
          0.10256447730390458,
          0.09347894888303555,
          0.08576130672392186,
          0.06441444581781801,
          0.09984785249106373,
          0.0671101688592015,
          0.06362116646247476,
          0.07543212351598053,
          0.055376006342985125,
          0.07040611935612995,
          0.04984954062002233,
          0.05531420315121147,
          0.061339245746797985,
          0.06885122769923148,
          0.08154856188258308,
          0.056645094911320436,
          0.06068553761796319,
          0.18008427745087108,
          0.13882452937898687,
          0.12103758317971619,
          0.09179169359219992,
          0.09565830651640869,
          0.06801440523154229,
          0.20066457544995583,
          0.15558434691725545,
          0.13598913424780898,
          0.10008678911549554,
          0.11460897958951419,
          0.0740977040593947,
          0.07115660592753953,
          0.13931664363419663,
          0.11851547849269839,
          0.10045760192271261,
          0.05826407444667093,
          0.13882452937898687,
          0.11179494800966176,
          0.13550692335128253,
          0.0893229026433946,
          0.09866541464238301,
          0.10886826823713752,
          0.05105605550203429,
          0.06280546575762443,
          0.05337296090864082,
          0.05919980630954484,
          0.07695521816313397,
          0.13159265307892584,
          0.11179494800966176,
          0.057941484029988256,
          0.04479781373813964,
          0.023013135558839274,
          0.033812323439761975,
          0.04032774643072385,
          0.06355078022564949,
          0.09545836118703475,
          0.1124698280315702,
          0.070881301393685,
          0.05465984776330975,
          0.13882452937898687,
          0.10579815539747389,
          0.027681622255203958,
          0.03213123234417902,
          0.049311848481598815,
          0.05472096218640811,
          0.06068553761796319,
          0.05039278493188995,
          0.016850802428263294,
          0.09739901977961364,
          0.07338794187974378,
          0.07061109198995269,
          0.05281741278997153,
          0.06398770345571217,
          0.05067173351024046,
          0.03340333670311388,
          0.07793940905861907,
          0.024876939927353025,
          0.026678565875096944,
          0.22663756083361328,
          0.09513264341056984,
          0.047777286596981765,
          0.03194518138728216,
          0.02364722309978597,
          0.06061818891481711,
          0.09845536234010355,
          0.14392841169203668,
          0.015165862986623167,
          0.07917454376802902,
          0.07381620741652305,
          0.04302662550586858,
          0.03194518138728216,
          0.08758997202922922,
          0.11283092844997043,
          0.070881301393685,
          0.05658196017483262,
          0.04181547655412865,
          0.10301770186218005,
          0.060038350620542025,
          0.12128445332950208,
          0.14499121903418713,
          0.07381620741652305,
          0.14361533538361398,
          0.05674483185497528,
          0.04016656063467179,
          0.13045491821679245,
          0.15744461333352608,
          0.06958638075876422,
          0.08988394827350045,
          0.09500390749234944,
          0.08488619399000018,
          0.08488619399000018,
          0.22053837064925386,
          0.05959066076716101,
          0.027601890286133315,
          0.03033391468713402,
          0.0585674761030412,
          0.0891307731119873,
          0.10886826823713752,
          0.14376078453869331,
          0.14762906039734883,
          0.1762480486113337,
          0.18377507257021367,
          0.15016593961189845,
          0.12434062742262586,
          0.0998042825353299,
          0.1301159600616955,
          0.1038206171270304,
          0.02738792833708632,
          0.08807068110353662,
          0.04934008985593955,
          0.054305143516034035,
          0.04405752529991949,
          0.034735152628004216,
          0.10375491995698168,
          0.0836896439137949,
          0.0665652361037985,
          0.07220781295931927,
          0.0590572209496348,
          0.05068843735642205,
          0.11990190912558267,
          0.08591756380827868,
          0.04301612442630243,
          0.04946183385936434,
          0.03648167097200262,
          0.03867266230176373,
          0.06381175329451398,
          0.07825453179637254,
          0.07144642956990946,
          0.04549207996828337,
          0.0522885251428027,
          0.17722800612111592,
          0.10188258298493393,
          0.049517440881453365,
          0.03373656957333932,
          0.030127706342904837,
          0.0518263922435799,
          0.06159173440657877,
          0.09334344742670495,
          0.07877819506699905,
          0.0473442730549452,
          0.06152344587912112,
          0.038742054167873796,
          0.030162266874830024,
          0.03729239677873011,
          0.03050851580786981,
          0.06259011189630395,
          0.042500707520645775,
          0.04558099233480385,
          0.04449779268209772,
          0.09959707647517824,
          0.10435424353971108,
          0.10755864887591944,
          0.06603321227879817,
          0.04776934795096591,
          0.06299377342201352,
          0.08053191571178347,
          0.019615711948172986,
          0.09307706661702816,
          0.056144985839731695,
          0.07440409473051283,
          0.1233924516125418,
          0.05092629935150408,
          0.06987028990722849,
          0.034856910684017556,
          0.04089680521632072,
          0.06631944979760646,
          0.04444755901520337,
          0.021227570558388174,
          0.031072954936106383,
          0.13210286342411884,
          0.1262590503676548,
          0.08093531543754523,
          0.11153512547623604,
          0.10664042108448803,
          0.004438869255329869,
          0.12842060033046365,
          0.2177056732483556,
          0.09683028133133766,
          0.13210286342411884,
          0.14643023787877926,
          0.09334344742670495,
          0.025469570331042437,
          0.054297532891429356,
          0.08369959606725073,
          0.0386287387861989,
          0.15486982234014343,
          0.10048898035517813,
          0.05620766204516892,
          0.034751845803471786,
          0.03467262709386443,
          0.06994715257313312,
          0.09367667819301771,
          0.06518873917299493,
          0.06745468079433349,
          0.07440409473051283,
          0.03685285965154846,
          0.044630959990167446,
          0.12137315660606464,
          0.07806226544116798,
          0.09871222008718898,
          0.04954958731575383,
          0.049517440881453365,
          0.14303856361723302,
          0.17017399792510124,
          0.029124697744320697,
          0.12984782288237898,
          0.21634331076312505,
          0.10323364298605688,
          0.09320219881423597,
          0.06205740337916958,
          0.0711442201839988,
          0.10862096570786064,
          0.120924302670877,
          0.12014242722382995,
          0.08548080343795633,
          0.06979350536499049,
          0.06719527168237419,
          0.07025706008066092,
          0.055714081766946304,
          0.05936231855131566,
          0.07440696118679894,
          0.04546607224086786,
          0.07295634159423983,
          0.12409208033476844,
          0.06632202710627258,
          0.04138247965686184,
          0.05163865776629049,
          0.08158975924621727,
          0.07878121569481625,
          0.05254755116153139,
          0.04456897062068404,
          0.04882603228147658,
          0.06719527168237419,
          0.03252275744947776,
          0.03904419957454759,
          0.03252275744947776,
          0.023132111090121806,
          0.02429810631443843,
          0.02802115850210112,
          0.032295789221156286,
          0.04072562592520388,
          0.04290738023347443,
          0.05803609796215426,
          0.03683296538084088,
          0.04546607224086786,
          0.02429810631443843,
          0.01732270057523507,
          0.02344543503991117,
          0.025180988202122095,
          0.031171428851162086,
          0.043378425009559865,
          0.023835984277098446,
          0.029516142958260383,
          0.030631557150789336,
          0.07806397207408473,
          0.04263643911708119,
          0.033604191512519685,
          0.07063661204546288,
          0.017538370246117083,
          0.05673771943648918,
          0.05673771943648918,
          0.1021741228051385,
          0.08160429614930415,
          0.06487637694922106,
          0.09747585340751368,
          0.11085337609104713,
          0.018656892896365684,
          0.0037451665858079888,
          0.05088859270537859,
          0.04018155323160608,
          0.033459304886803196,
          0.024513501498687536,
          0.035908879273376104,
          0.034205249579592735,
          0.024242118983801117,
          0.023078746879052942,
          0.021969947603565125,
          0.0011591384343730238,
          0.02259586999929652,
          0.02606506647028684,
          0.045963857792239264,
          0.05851142380781372,
          0.054468796017418664,
          0.03092707962018982,
          0.0634200290355794,
          0.04072562592520388,
          0.06060915294438334,
          0.03966291276789847,
          0.034702814094700815,
          0.03930002801886176,
          0.027448539613654985,
          0.07142781697480925,
          0.09560801986152842,
          0.09984037966605065,
          0.0996008090635004,
          0.10397458733812405,
          0.036388817664069845,
          0.05288606464831723,
          0.05534294582943419,
          0.0064008436701467036,
          0.048626732047206044,
          0.028847051210592417,
          0.04263643911708119,
          0.12107895391451112,
          0.03904419957454759,
          0.04816966716564266,
          0.10988071879223255,
          0.07381262944432673,
          0.021534653431827613,
          0.02668139043132475,
          0.04281039508421264,
          0.10267598384636932,
          0.08127160626886232,
          0.026386656986600414,
          0.042345245486266406,
          0.028977675897108212,
          0.03583812532785983,
          0.0513691914593447,
          0.06358351477286069,
          0.02259586999929652,
          0.023105413942746404,
          0.018589258729220997,
          0.029977198219791147,
          0.058728360862490045,
          0.06000268340893539,
          0.06283415317161434,
          0.02478224521265604,
          0.037862422732203825,
          0.05254755116153139,
          0.10756264419514595,
          0.08984851427351866,
          0.1476779930415968,
          0.012862449622658773,
          0.006646707887018933,
          0.02759347245147438,
          0.027163592259996882,
          0.12892651737748456,
          0.17259540279753194,
          0.03254863293575175,
          0.04798238456234363,
          0.033604191512519685,
          0.06460708703208608,
          0.023708115637899506,
          0.027343300525769717,
          0.031517877075793295,
          0.022289847936940575,
          0.024473400258871847,
          0.017489150549401353,
          0.02540400608561074,
          0.029929130933218397,
          0.054468796017418664,
          0.024312388431344444,
          0.08446570915722838,
          0.06276826444891771,
          0.022849254856176813,
          0.026356305056444197,
          0.05340075188802477,
          0.05214187357658875,
          0.002847300760632544,
          0.1256567528397001,
          0.019855367944312568,
          0.11973054103875547,
          0.06438126305875612,
          0.11009799994691538,
          0.04482915253397503,
          0.06847884132719896,
          0.02722641913633798,
          0.006353121961930724,
          0.00980540597040601,
          0.03399611538410161,
          0.01755507791808677,
          0.07729859567680272,
          0.08357115770999683,
          0.11023877434033122,
          0.016335355862495123,
          0.07490144147590404,
          0.03662037545533557,
          0.028479885063891302,
          0.03723014056484569,
          0.054638739880564766,
          0.036856845620579295,
          0.047245657424595695,
          0.03780819377493721,
          0.031606536704974084,
          0.029884806592491233,
          0.03102815086589091,
          0.03574462922231229,
          0.06182396510923579,
          0.04946558143677571,
          0.03835487418385169,
          0.03460908904225074,
          0.06950674808109636,
          0.08051053307178913,
          0.07342489205869758,
          0.0562200178403158,
          0.019465490981318678,
          0.028517431257430056,
          0.17034459604717822,
          0.1842979084540904,
          0.010664573816654496,
          0.007764296495317743,
          0.03299312117658866,
          0.01957389733852702,
          0.02258984664396102,
          0.02102903055494826,
          0.048847412938126326,
          0.01819369262894121,
          0.023643593530778113,
          0.0210077879507896,
          0.07835141289993867,
          0.08554633208200581,
          0.036440111898527985,
          0.02700714136535058,
          0.041466772735801745,
          0.03439711074578529,
          0.058421291950165176,
          0.027478807480882012,
          0.04108558619778404,
          0.062177473081203014,
          0.023587332013051184,
          0.06408939184640872,
          0.035951145726858195,
          0.0789440408029604,
          0.05503200587527252,
          0.04610444982134587,
          0.02907426074594365,
          0.03013917243185417,
          0.040958606294561245,
          0.010727882421295094,
          0.024031816659632874,
          0.01835421388063504,
          0.016244589250764472,
          0.06859356581818327,
          0.03175800319584795,
          0.027281706794517968,
          0.03450448056008485,
          0.03972771804012366,
          0.05764813131069022,
          0.09075126259296194,
          0.09368344562572292,
          0.07399910583112917,
          0.05258947181454367,
          0.0781464727999439,
          0.026220919943375467,
          0.016150461303045937,
          0.016025917915505943,
          0.02588495051532916,
          0.031165671154128587,
          0.05162932101815939,
          0.04940416874873126,
          0.03358251765084933,
          0.026415362023362355,
          0.014222698229928358,
          0.02880447147685569,
          0.03735451221211008,
          0.05426491206236836,
          0.0647392535271875,
          0.017632934100108456,
          0.01996541503169846,
          0.04894973085061288,
          0.010749222061457503,
          0.08205137126383823,
          0.052391752737565696,
          0.015268439475726469,
          0.025781985815895712,
          0.011006609977183555,
          0.008447067363049367,
          0.02437193162917433,
          0.06620788583536469,
          0.03764673565803267,
          0.07013140200522901,
          0.10033338312363434,
          0.010224564620875886,
          0.008022829889737326,
          0.03211075469957042,
          0.018172588963521173,
          0.01751066391430199,
          0.01751066391430199,
          0.01751066391430199,
          0.01751066391430199,
          0.01751066391430199,
          0.029709056949540354,
          0.03464609663780117,
          0.06416104836762267,
          0.03768207827282846,
          0.04550306509192179,
          0.07425939826331802,
          0.010590055831799953,
          0.05448045812691987,
          0.04550306509192179,
          0.007471370738964538,
          0.028293278844360087,
          0.022228983982389268,
          0.06948241548654446,
          0.04380995713890562,
          0.07805062105451488,
          0.01695733326506623,
          0.08555263209489919,
          0.04266640566292494,
          0.030949082421137048,
          0.022374438754279125,
          0.07539542231277906,
          0.06023534582921659,
          0.08858483593706268,
          0.06636748677006353,
          0.060444735196553204,
          0.045088644794102274,
          0.07822592886599872,
          0.055311620808814695,
          0.07188482765708855,
          0.05702557314753779,
          0.046129940622520224,
          0.07632039057980616,
          0.09749776412478069,
          0.05914605885953171,
          0.08034605877313673,
          0.05914605885953171,
          0.08034605877313673,
          0.05914605885953171,
          0.08034605877313673,
          0.04075430664761874,
          0.021351444083389362,
          0.033890337908171864,
          0.04349056473846526,
          0.06812461498501025,
          0.10209065728097505,
          0.0829183285601656,
          0.02983126062467596,
          0.027711856274733056,
          0.02228470913006103,
          0.03068389091150264,
          0.03340667029573343,
          0.12115707573963036,
          0.11145494705636572,
          0.02255913646973078,
          0.017774896035985692,
          0.07514308478273002,
          0.04892971608484065,
          0.0652530934305157,
          0.07030501386897685,
          0.03813409651249302,
          0.051670662613393585,
          0.045088644794102274,
          0.0887682187331669,
          0.02708762795475931,
          0.026431738838101335,
          0.05616039041829851,
          0.02633271894258931,
          0.02120265025911013,
          0.04918384397871582,
          0.01928562528308396,
          0.029638757702073446,
          0.04169987468413533,
          0.030949082421137048,
          0.06916060751295382,
          0.0516167875415878,
          0.05531959826298822,
          0.07033554101995253,
          0.11472295396232049,
          0.18530287684400812,
          0.013123373257285437,
          0.05943171720608386,
          0.028667645541335083,
          0.03224236215530978,
          0.03902436505170379,
          0.032637614020019905,
          0.026757729062606202,
          0.014008571101143603,
          0.003393126468208829,
          0.029975572005004607,
          0.037063255258363084,
          0.08923518158685084,
          0.11513102327923368,
          0.037991489644898666,
          0.045209496922386704,
          0.08910124918192056,
          0.09717922812604735,
          0.07030501386897685,
          0.08313174924231855,
          0.03840344159831305,
          0.02782281651104357,
          0.015979381468708816,
          0.05092405067545789,
          0.04526594364774321,
          0.03362803282910667,
          0.028226145383228658,
          0.05073777448129945,
          0.05328678532667379,
          0.044664038893314946,
          0.04153593583735767,
          0.03681421113486708,
          0.058699137566765855,
          0.04288567079082781,
          0.09106206026794907,
          0.07456001965053133,
          0.03553832133642964,
          0.045992516056266225,
          0.026403446767343283,
          0.030439023752511195,
          0.03441419124204736,
          0.02967277450428924,
          0.04431644217770628,
          0.021056157529543908,
          0.02994808637732963,
          0.03285922228167706,
          0.02432980341715731,
          0.019098330951022795,
          0.07612300459696003,
          0.05378874936336485,
          0.06611326023856558,
          0.05025086957339825,
          0.03967239947032148,
          0.021284511153198925,
          0.023988959266774443,
          0.023120165454297847,
          0.07325084657996049,
          0.08961234390317137,
          0.08361174530537412,
          0.09631355500990993,
          0.06100978212932532,
          0.09222887311803973,
          0.016587026943392067,
          0.09631355500990993,
          0.02942897767509831,
          0.09306161356796988,
          0.13172100438714357,
          0.054295170738270525,
          0.04634607072738599,
          0.07860543791141623,
          0.045992516056266225,
          0.06654303411502362,
          0.04005154100060237,
          0.053669281532446815,
          0.08095693836892384,
          0.02669836260774568,
          0.023668547726057573,
          0.0728711505930225,
          0.04593031481220533,
          0.03997990123009259,
          0.045992516056266225,
          0.05285958522779604,
          0.05291879947855883,
          0.03192472610864699,
          0.0786125142498881,
          0.055677129347539545,
          0.13004097569105674,
          0.09984352538337163,
          0.09934192951182977,
          0.08597858919469241,
          0.10241985714347938,
          0.05977925809051198,
          0.02842600208930475,
          0.059881117523841475,
          0.04149058295047566,
          0.05467035646780879,
          0.023634594494125388,
          0.06555408824848251,
          0.030709586648980144,
          0.030709586648980144,
          0.037749111102485244,
          0.07647759623928241,
          0.06430560483493235,
          0.09340692893955756,
          0.05381212634292487,
          0.0905029839177724,
          0.14075851754505078,
          0.10906415882271936,
          0.07816197933632188,
          0.1651666461613087,
          0.1353417309763271,
          0.12214042563382645,
          0.1296159875389641,
          0.060664380902028164,
          0.0752801918653581,
          0.09693831776671215,
          0.09518876860564394,
          0.1212576354229487,
          0.057638180659136826,
          0.07291004584438446,
          0.06562995174459638,
          0.07693468383271918,
          0.13015671888691965,
          0.11756328394078819,
          0.0917303113708651,
          0.15419123397937715,
          0.18019650439127677,
          0.13897340816504675,
          0.17037418326893772,
          0.14196091135479094,
          0.10239011520964553,
          0.056221457885781326,
          0.056221457885781326,
          0.07213700700422954,
          0.07367950378560785,
          0.2970390234311171,
          0.027627808770443196,
          0.0928355584740105,
          0.09189551555804581,
          0.07410357166133799,
          0.07130172742016307,
          0.07323520969443968,
          0.09705537428781863,
          0.06238196953029654,
          0.0933874285454061,
          0.07558201994576999,
          0.07298443542345509,
          0.08936639797474964,
          0.06031174896680442,
          0.06812972472357946,
          0.07508863402394508,
          0.10214318409900554,
          0.11529119258978314,
          0.08145612467820758,
          0.08864198707978886,
          0.09938289761535657,
          0.10885519210795275,
          0.07524728229549742,
          0.027627808770443196,
          0.17142350819764252,
          0.15733703551586514,
          0.14788932305191665,
          0.07829244254234746,
          0.09847167161351529,
          0.14142552530121827,
          0.07590153998552239,
          0.07590153998552239,
          0.07590153998552239,
          0.07590153998552239,
          0.06919134489550352,
          0.06386650972292104,
          0.07492460277152312,
          0.08109153639465008,
          0.06145296198087098,
          0.007955224735505402,
          0.05881672746634976,
          0.07384102674627135,
          0.08413699881148344,
          0.11305384186698365,
          0.15225963654282523,
          0.11896345815998786,
          0.05910171857274614,
          0.21248404179745775,
          0.09315673656754642,
          0.130076410148015
         ]
        }
       ],
       "layout": {
        "legend": {
         "title": {
          "text": "Legendary Status"
         },
         "tracegroupgap": 0
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Synergistic Model: Fire Type Prediction with Interaction Terms"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Attack"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Fire Type (1=Fire)"
         }
        }
       }
      },
      "text/html": [
       "<div>                            <div id=\"f6061a5f-4cc7-47c8-a385-2c354d172567\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"f6061a5f-4cc7-47c8-a385-2c354d172567\")) {                    Plotly.newPlot(                        \"f6061a5f-4cc7-47c8-a385-2c354d172567\",                        [{\"hovertemplate\":\"Legendary Status=False\\u003cbr\\u003eAttack=%{x}\\u003cbr\\u003eFire Type (1=Fire)=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"False\",\"marker\":{\"color\":\"#636efa\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"False\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[49,62,82,100,52,64,84,130,104,48,63,83,103,30,20,45,35,25,90,150,45,60,80,80,56,81,60,90,60,85,55,90,75,100,47,62,92,57,72,102,45,70,41,76,45,70,45,80,50,65,80,70,95,55,65,55,80,45,70,52,82,80,105,70,110,50,65,95,20,35,50,50,80,100,130,75,90,105,40,70,80,95,120,85,100,65,75,75,35,60,65,85,110,45,70,80,105,65,95,35,50,65,65,45,48,73,105,130,30,50,40,95,50,80,120,105,55,65,90,85,130,5,55,95,125,40,65,67,92,45,75,45,110,50,83,95,125,155,100,10,125,155,85,48,55,65,65,130,60,40,60,80,115,105,135,110,64,84,134,100,49,62,82,52,64,84,65,80,105,46,76,30,50,20,35,60,90,90,38,58,40,25,30,20,40,50,75,40,55,75,95,80,20,50,100,75,35,45,55,70,30,75,65,45,85,65,65,85,75,60,72,33,80,65,90,70,75,85,125,80,120,95,130,150,10,125,185,95,80,130,40,50,50,100,55,65,105,55,40,80,60,90,90,95,60,120,80,95,20,35,95,30,63,75,80,10,64,84,134,164,100,45,65,85,110,60,85,120,160,70,85,110,150,55,90,30,70,45,35,70,35,50,30,50,70,40,70,100,55,85,30,50,25,35,65,85,30,60,40,130,60,80,160,45,90,90,51,71,91,60,120,20,45,45,65,75,85,85,105,70,90,110,140,40,60,100,45,75,75,50,40,73,47,60,43,73,90,120,140,70,90,60,100,120,85,25,45,60,100,70,100,85,115,40,70,110,115,100,55,95,48,78,80,120,40,70,41,81,95,125,15,60,70,90,75,115,165,40,70,68,50,130,150,23,50,80,120,40,60,80,64,104,84,90,30,75,95,135,145,55,75,135,145,68,89,109,58,78,104,51,66,86,55,75,120,45,85,25,85,65,85,120,30,70,125,165,42,52,29,59,79,69,94,30,80,45,65,105,35,60,48,83,100,50,80,66,76,136,60,125,55,82,30,63,93,24,89,80,25,5,65,92,70,90,130,170,85,70,110,145,72,112,50,90,61,106,100,49,69,20,62,92,132,120,70,85,140,100,123,95,50,76,110,60,95,130,80,125,165,55,100,80,50,65,65,65,65,65,70,80,100,45,60,75,63,93,123,55,75,100,55,85,60,80,110,50,88,53,98,53,98,53,98,25,55,55,77,115,60,100,75,105,135,45,57,85,135,60,60,80,105,140,50,65,95,100,125,53,63,103,45,55,100,27,67,35,60,92,72,82,117,90,140,30,86,65,95,75,90,58,30,50,78,108,112,140,50,95,65,105,50,95,30,45,55,30,40,65,44,87,50,65,95,60,100,75,75,135,55,85,40,60,75,47,77,50,94,55,80,100,55,85,115,55,75,30,40,55,87,117,147,70,110,50,40,70,66,85,125,120,74,124,85,125,110,83,123,55,65,97,109,65,85,105,85,60,72,72,77,128,120,61,78,107,45,59,69,56,63,95,36,56,50,73,81,35,22,52,50,68,38,45,65,65,100,82,124,80,48,48,48,80,110,150,50,52,72,48,80,54,92,52,105,60,75,53,73,38,55,89,121,59,77,65,92,58,50,50,75,100,80,70,110,66,66,66,66,90,85,95,100,69,117,30,70],\"xaxis\":\"x\",\"y\":[0,0,0,0,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,1,1,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"Legendary Status=True\\u003cbr\\u003eAttack=%{x}\\u003cbr\\u003eFire Type (1=Fire)=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"True\",\"marker\":{\"color\":\"#EF553B\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"True\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[85,90,100,110,190,150,85,115,75,90,130,100,50,75,80,100,90,130,100,150,150,180,150,180,100,150,180,70,95,75,105,125,120,120,90,160,100,120,90,100,103,120,100,90,129,90,115,100,115,105,120,150,125,145,130,170,120,131,131,100,100,160,110,160,110],\"xaxis\":\"x\",\"y\":[0,0,1,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"mode\":\"lines\",\"name\":\"Best Fit Line\",\"x\":[49,62,82,100,52,64,84,130,104,48,63,83,103,30,20,45,35,25,90,150,45,60,80,80,56,81,60,90,60,85,55,90,75,100,47,62,92,57,72,102,45,70,41,76,45,70,45,80,50,65,80,70,95,55,65,55,80,45,70,52,82,80,105,70,110,50,65,95,20,35,50,50,80,100,130,75,90,105,40,70,80,95,120,85,100,65,75,75,35,60,65,85,110,45,70,80,105,65,95,35,50,65,65,45,48,73,105,130,30,50,40,95,50,80,120,105,55,65,90,85,130,5,55,95,125,40,65,67,92,45,75,45,110,50,83,95,125,155,100,10,125,155,85,48,55,65,65,130,60,40,60,80,115,105,135,110,85,90,100,64,84,134,110,190,150,100,49,62,82,52,64,84,65,80,105,46,76,30,50,20,35,60,90,90,38,58,40,25,30,20,40,50,75,40,55,75,95,80,20,50,100,75,35,45,55,70,30,75,65,45,85,65,65,85,75,60,72,33,80,65,90,70,75,85,125,80,120,95,130,150,10,125,185,95,80,130,40,50,50,100,55,65,105,55,40,80,60,90,90,95,60,120,80,95,20,35,95,30,63,75,80,10,85,115,75,64,84,134,164,90,130,100,45,65,85,110,60,85,120,160,70,85,110,150,55,90,30,70,45,35,70,35,50,30,50,70,40,70,100,55,85,30,50,25,35,65,85,30,60,40,130,60,80,160,45,90,90,51,71,91,60,120,20,45,45,65,75,85,85,105,70,90,110,140,40,60,100,45,75,75,50,40,73,47,60,43,73,90,120,140,70,90,60,100,120,85,25,45,60,100,70,100,85,115,40,70,110,115,100,55,95,48,78,80,120,40,70,41,81,95,125,15,60,70,90,75,115,165,40,70,68,50,130,150,23,50,80,120,40,60,80,64,104,84,90,30,75,95,135,145,55,75,135,145,100,50,75,80,100,90,130,100,150,150,180,150,180,100,150,180,70,95,68,89,109,58,78,104,51,66,86,55,75,120,45,85,25,85,65,85,120,30,70,125,165,42,52,29,59,79,69,94,30,80,45,65,105,35,60,48,83,100,50,80,66,76,136,60,125,55,82,30,63,93,24,89,80,25,5,65,92,70,90,130,170,85,70,110,145,72,112,50,90,61,106,100,49,69,20,62,92,132,120,70,85,140,100,123,95,50,76,110,60,95,130,80,125,165,55,100,80,50,65,65,65,65,65,75,105,125,120,120,90,160,100,120,70,80,100,90,100,103,120,100,45,60,75,63,93,123,55,75,100,55,85,60,80,110,50,88,53,98,53,98,53,98,25,55,55,77,115,60,100,75,105,135,45,57,85,135,60,60,80,105,140,50,65,95,100,125,53,63,103,45,55,100,27,67,35,60,92,72,82,117,90,140,30,86,65,95,75,90,58,30,50,78,108,112,140,50,95,65,105,50,95,30,45,55,30,40,65,44,87,50,65,95,60,100,75,75,135,55,85,40,60,75,47,77,50,94,55,80,100,55,85,115,55,75,30,40,55,87,117,147,70,110,50,40,70,66,85,125,120,74,124,85,125,110,83,123,55,65,97,109,65,85,105,85,60,90,129,90,115,100,115,105,120,150,125,145,130,170,120,72,72,77,128,120,61,78,107,45,59,69,56,63,95,36,56,50,73,81,35,22,52,50,68,38,45,65,65,100,82,124,80,48,48,48,80,110,150,50,52,72,48,80,54,92,52,105,60,75,53,73,38,55,89,121,59,77,65,92,58,50,50,75,100,80,70,110,66,66,66,66,90,85,95,100,69,117,30,70,131,131,100,100,160,110,160,110],\"y\":[0.051870473179323924,0.0562301500356128,0.06452240255249116,0.059595518237456443,0.10921569516568261,0.08819827987208945,0.06974961890010782,0.10659853335945497,0.0955738197415481,0.056409578379128064,0.04730675941190599,0.037323707104900995,0.029382384994346398,0.09621588215197491,0.04732861630052293,0.048144460101753495,0.049311848481598815,0.03464484042083051,0.10776618489458129,0.25268481505313284,0.05285852656686191,0.05863268860969511,0.06725399523897199,0.06423324229943546,0.14259352530836222,0.10878377093351249,0.07377504356372007,0.08629716011335885,0.12014572151371682,0.09109299585472368,0.12360583408838692,0.14170934673517535,0.050001712140962835,0.0371936643882512,0.079414602415652,0.0668625145656812,0.07297755953456761,0.12737124532234215,0.10256447730390458,0.09347894888303555,0.08576130672392186,0.06441444581781801,0.09984785249106373,0.0671101688592015,0.06362116646247476,0.07543212351598053,0.055376006342985125,0.07040611935612995,0.04984954062002233,0.05531420315121147,0.061339245746797985,0.06885122769923148,0.08154856188258308,0.056645094911320436,0.06068553761796319,0.18008427745087108,0.13882452937898687,0.12103758317971619,0.09179169359219992,0.09565830651640869,0.06801440523154229,0.20066457544995583,0.15558434691725545,0.13598913424780898,0.10008678911549554,0.11460897958951419,0.0740977040593947,0.07115660592753953,0.13931664363419663,0.11851547849269839,0.10045760192271261,0.05826407444667093,0.13882452937898687,0.11179494800966176,0.13550692335128253,0.0893229026433946,0.09866541464238301,0.10886826823713752,0.05105605550203429,0.06280546575762443,0.05337296090864082,0.05919980630954484,0.07695521816313397,0.13159265307892584,0.11179494800966176,0.057941484029988256,0.04479781373813964,0.023013135558839274,0.033812323439761975,0.04032774643072385,0.06355078022564949,0.09545836118703475,0.1124698280315702,0.070881301393685,0.05465984776330975,0.13882452937898687,0.10579815539747389,0.027681622255203958,0.03213123234417902,0.049311848481598815,0.05472096218640811,0.06068553761796319,0.05039278493188995,0.016850802428263294,0.09739901977961364,0.07338794187974378,0.07061109198995269,0.05281741278997153,0.06398770345571217,0.05067173351024046,0.03340333670311388,0.07793940905861907,0.024876939927353025,0.026678565875096944,0.22663756083361328,0.09513264341056984,0.047777286596981765,0.03194518138728216,0.02364722309978597,0.06061818891481711,0.09845536234010355,0.14392841169203668,0.015165862986623167,0.07917454376802902,0.07381620741652305,0.04302662550586858,0.03194518138728216,0.08758997202922922,0.11283092844997043,0.070881301393685,0.05658196017483262,0.04181547655412865,0.10301770186218005,0.060038350620542025,0.12128445332950208,0.14499121903418713,0.07381620741652305,0.14361533538361398,0.05674483185497528,0.04016656063467179,0.13045491821679245,0.15744461333352608,0.06958638075876422,0.08988394827350045,0.09500390749234944,0.08488619399000018,0.08488619399000018,0.22053837064925386,0.05959066076716101,0.027601890286133315,0.03033391468713402,0.0585674761030412,0.0891307731119873,0.10886826823713752,0.14376078453869331,0.14762906039734883,0.1762480486113337,0.18377507257021367,0.15016593961189845,0.12434062742262586,0.0998042825353299,0.1301159600616955,0.1038206171270304,0.02738792833708632,0.08807068110353662,0.04934008985593955,0.054305143516034035,0.04405752529991949,0.034735152628004216,0.10375491995698168,0.0836896439137949,0.0665652361037985,0.07220781295931927,0.0590572209496348,0.05068843735642205,0.11990190912558267,0.08591756380827868,0.04301612442630243,0.04946183385936434,0.03648167097200262,0.03867266230176373,0.06381175329451398,0.07825453179637254,0.07144642956990946,0.04549207996828337,0.0522885251428027,0.17722800612111592,0.10188258298493393,0.049517440881453365,0.03373656957333932,0.030127706342904837,0.0518263922435799,0.06159173440657877,0.09334344742670495,0.07877819506699905,0.0473442730549452,0.06152344587912112,0.038742054167873796,0.030162266874830024,0.03729239677873011,0.03050851580786981,0.06259011189630395,0.042500707520645775,0.04558099233480385,0.04449779268209772,0.09959707647517824,0.10435424353971108,0.10755864887591944,0.06603321227879817,0.04776934795096591,0.06299377342201352,0.08053191571178347,0.019615711948172986,0.09307706661702816,0.056144985839731695,0.07440409473051283,0.1233924516125418,0.05092629935150408,0.06987028990722849,0.034856910684017556,0.04089680521632072,0.06631944979760646,0.04444755901520337,0.021227570558388174,0.031072954936106383,0.13210286342411884,0.1262590503676548,0.08093531543754523,0.11153512547623604,0.10664042108448803,0.004438869255329869,0.12842060033046365,0.2177056732483556,0.09683028133133766,0.13210286342411884,0.14643023787877926,0.09334344742670495,0.025469570331042437,0.054297532891429356,0.08369959606725073,0.0386287387861989,0.15486982234014343,0.10048898035517813,0.05620766204516892,0.034751845803471786,0.03467262709386443,0.06994715257313312,0.09367667819301771,0.06518873917299493,0.06745468079433349,0.07440409473051283,0.03685285965154846,0.044630959990167446,0.12137315660606464,0.07806226544116798,0.09871222008718898,0.04954958731575383,0.049517440881453365,0.14303856361723302,0.17017399792510124,0.029124697744320697,0.12984782288237898,0.21634331076312505,0.10323364298605688,0.09320219881423597,0.06205740337916958,0.0711442201839988,0.10862096570786064,0.120924302670877,0.12014242722382995,0.08548080343795633,0.06979350536499049,0.06719527168237419,0.07025706008066092,0.055714081766946304,0.05936231855131566,0.07440696118679894,0.04546607224086786,0.07295634159423983,0.12409208033476844,0.06632202710627258,0.04138247965686184,0.05163865776629049,0.08158975924621727,0.07878121569481625,0.05254755116153139,0.04456897062068404,0.04882603228147658,0.06719527168237419,0.03252275744947776,0.03904419957454759,0.03252275744947776,0.023132111090121806,0.02429810631443843,0.02802115850210112,0.032295789221156286,0.04072562592520388,0.04290738023347443,0.05803609796215426,0.03683296538084088,0.04546607224086786,0.02429810631443843,0.01732270057523507,0.02344543503991117,0.025180988202122095,0.031171428851162086,0.043378425009559865,0.023835984277098446,0.029516142958260383,0.030631557150789336,0.07806397207408473,0.04263643911708119,0.033604191512519685,0.07063661204546288,0.017538370246117083,0.05673771943648918,0.05673771943648918,0.1021741228051385,0.08160429614930415,0.06487637694922106,0.09747585340751368,0.11085337609104713,0.018656892896365684,0.0037451665858079888,0.05088859270537859,0.04018155323160608,0.033459304886803196,0.024513501498687536,0.035908879273376104,0.034205249579592735,0.024242118983801117,0.023078746879052942,0.021969947603565125,0.0011591384343730238,0.02259586999929652,0.02606506647028684,0.045963857792239264,0.05851142380781372,0.054468796017418664,0.03092707962018982,0.0634200290355794,0.04072562592520388,0.06060915294438334,0.03966291276789847,0.034702814094700815,0.03930002801886176,0.027448539613654985,0.07142781697480925,0.09560801986152842,0.09984037966605065,0.0996008090635004,0.10397458733812405,0.036388817664069845,0.05288606464831723,0.05534294582943419,0.0064008436701467036,0.048626732047206044,0.028847051210592417,0.04263643911708119,0.12107895391451112,0.03904419957454759,0.04816966716564266,0.10988071879223255,0.07381262944432673,0.021534653431827613,0.02668139043132475,0.04281039508421264,0.10267598384636932,0.08127160626886232,0.026386656986600414,0.042345245486266406,0.028977675897108212,0.03583812532785983,0.0513691914593447,0.06358351477286069,0.02259586999929652,0.023105413942746404,0.018589258729220997,0.029977198219791147,0.058728360862490045,0.06000268340893539,0.06283415317161434,0.02478224521265604,0.037862422732203825,0.05254755116153139,0.10756264419514595,0.08984851427351866,0.1476779930415968,0.012862449622658773,0.006646707887018933,0.02759347245147438,0.027163592259996882,0.12892651737748456,0.17259540279753194,0.03254863293575175,0.04798238456234363,0.033604191512519685,0.06460708703208608,0.023708115637899506,0.027343300525769717,0.031517877075793295,0.022289847936940575,0.024473400258871847,0.017489150549401353,0.02540400608561074,0.029929130933218397,0.054468796017418664,0.024312388431344444,0.08446570915722838,0.06276826444891771,0.022849254856176813,0.026356305056444197,0.05340075188802477,0.05214187357658875,0.002847300760632544,0.1256567528397001,0.019855367944312568,0.11973054103875547,0.06438126305875612,0.11009799994691538,0.04482915253397503,0.06847884132719896,0.02722641913633798,0.006353121961930724,0.00980540597040601,0.03399611538410161,0.01755507791808677,0.07729859567680272,0.08357115770999683,0.11023877434033122,0.016335355862495123,0.07490144147590404,0.03662037545533557,0.028479885063891302,0.03723014056484569,0.054638739880564766,0.036856845620579295,0.047245657424595695,0.03780819377493721,0.031606536704974084,0.029884806592491233,0.03102815086589091,0.03574462922231229,0.06182396510923579,0.04946558143677571,0.03835487418385169,0.03460908904225074,0.06950674808109636,0.08051053307178913,0.07342489205869758,0.0562200178403158,0.019465490981318678,0.028517431257430056,0.17034459604717822,0.1842979084540904,0.010664573816654496,0.007764296495317743,0.03299312117658866,0.01957389733852702,0.02258984664396102,0.02102903055494826,0.048847412938126326,0.01819369262894121,0.023643593530778113,0.0210077879507896,0.07835141289993867,0.08554633208200581,0.036440111898527985,0.02700714136535058,0.041466772735801745,0.03439711074578529,0.058421291950165176,0.027478807480882012,0.04108558619778404,0.062177473081203014,0.023587332013051184,0.06408939184640872,0.035951145726858195,0.0789440408029604,0.05503200587527252,0.04610444982134587,0.02907426074594365,0.03013917243185417,0.040958606294561245,0.010727882421295094,0.024031816659632874,0.01835421388063504,0.016244589250764472,0.06859356581818327,0.03175800319584795,0.027281706794517968,0.03450448056008485,0.03972771804012366,0.05764813131069022,0.09075126259296194,0.09368344562572292,0.07399910583112917,0.05258947181454367,0.0781464727999439,0.026220919943375467,0.016150461303045937,0.016025917915505943,0.02588495051532916,0.031165671154128587,0.05162932101815939,0.04940416874873126,0.03358251765084933,0.026415362023362355,0.014222698229928358,0.02880447147685569,0.03735451221211008,0.05426491206236836,0.0647392535271875,0.017632934100108456,0.01996541503169846,0.04894973085061288,0.010749222061457503,0.08205137126383823,0.052391752737565696,0.015268439475726469,0.025781985815895712,0.011006609977183555,0.008447067363049367,0.02437193162917433,0.06620788583536469,0.03764673565803267,0.07013140200522901,0.10033338312363434,0.010224564620875886,0.008022829889737326,0.03211075469957042,0.018172588963521173,0.01751066391430199,0.01751066391430199,0.01751066391430199,0.01751066391430199,0.01751066391430199,0.029709056949540354,0.03464609663780117,0.06416104836762267,0.03768207827282846,0.04550306509192179,0.07425939826331802,0.010590055831799953,0.05448045812691987,0.04550306509192179,0.007471370738964538,0.028293278844360087,0.022228983982389268,0.06948241548654446,0.04380995713890562,0.07805062105451488,0.01695733326506623,0.08555263209489919,0.04266640566292494,0.030949082421137048,0.022374438754279125,0.07539542231277906,0.06023534582921659,0.08858483593706268,0.06636748677006353,0.060444735196553204,0.045088644794102274,0.07822592886599872,0.055311620808814695,0.07188482765708855,0.05702557314753779,0.046129940622520224,0.07632039057980616,0.09749776412478069,0.05914605885953171,0.08034605877313673,0.05914605885953171,0.08034605877313673,0.05914605885953171,0.08034605877313673,0.04075430664761874,0.021351444083389362,0.033890337908171864,0.04349056473846526,0.06812461498501025,0.10209065728097505,0.0829183285601656,0.02983126062467596,0.027711856274733056,0.02228470913006103,0.03068389091150264,0.03340667029573343,0.12115707573963036,0.11145494705636572,0.02255913646973078,0.017774896035985692,0.07514308478273002,0.04892971608484065,0.0652530934305157,0.07030501386897685,0.03813409651249302,0.051670662613393585,0.045088644794102274,0.0887682187331669,0.02708762795475931,0.026431738838101335,0.05616039041829851,0.02633271894258931,0.02120265025911013,0.04918384397871582,0.01928562528308396,0.029638757702073446,0.04169987468413533,0.030949082421137048,0.06916060751295382,0.0516167875415878,0.05531959826298822,0.07033554101995253,0.11472295396232049,0.18530287684400812,0.013123373257285437,0.05943171720608386,0.028667645541335083,0.03224236215530978,0.03902436505170379,0.032637614020019905,0.026757729062606202,0.014008571101143603,0.003393126468208829,0.029975572005004607,0.037063255258363084,0.08923518158685084,0.11513102327923368,0.037991489644898666,0.045209496922386704,0.08910124918192056,0.09717922812604735,0.07030501386897685,0.08313174924231855,0.03840344159831305,0.02782281651104357,0.015979381468708816,0.05092405067545789,0.04526594364774321,0.03362803282910667,0.028226145383228658,0.05073777448129945,0.05328678532667379,0.044664038893314946,0.04153593583735767,0.03681421113486708,0.058699137566765855,0.04288567079082781,0.09106206026794907,0.07456001965053133,0.03553832133642964,0.045992516056266225,0.026403446767343283,0.030439023752511195,0.03441419124204736,0.02967277450428924,0.04431644217770628,0.021056157529543908,0.02994808637732963,0.03285922228167706,0.02432980341715731,0.019098330951022795,0.07612300459696003,0.05378874936336485,0.06611326023856558,0.05025086957339825,0.03967239947032148,0.021284511153198925,0.023988959266774443,0.023120165454297847,0.07325084657996049,0.08961234390317137,0.08361174530537412,0.09631355500990993,0.06100978212932532,0.09222887311803973,0.016587026943392067,0.09631355500990993,0.02942897767509831,0.09306161356796988,0.13172100438714357,0.054295170738270525,0.04634607072738599,0.07860543791141623,0.045992516056266225,0.06654303411502362,0.04005154100060237,0.053669281532446815,0.08095693836892384,0.02669836260774568,0.023668547726057573,0.0728711505930225,0.04593031481220533,0.03997990123009259,0.045992516056266225,0.05285958522779604,0.05291879947855883,0.03192472610864699,0.0786125142498881,0.055677129347539545,0.13004097569105674,0.09984352538337163,0.09934192951182977,0.08597858919469241,0.10241985714347938,0.05977925809051198,0.02842600208930475,0.059881117523841475,0.04149058295047566,0.05467035646780879,0.023634594494125388,0.06555408824848251,0.030709586648980144,0.030709586648980144,0.037749111102485244,0.07647759623928241,0.06430560483493235,0.09340692893955756,0.05381212634292487,0.0905029839177724,0.14075851754505078,0.10906415882271936,0.07816197933632188,0.1651666461613087,0.1353417309763271,0.12214042563382645,0.1296159875389641,0.060664380902028164,0.0752801918653581,0.09693831776671215,0.09518876860564394,0.1212576354229487,0.057638180659136826,0.07291004584438446,0.06562995174459638,0.07693468383271918,0.13015671888691965,0.11756328394078819,0.0917303113708651,0.15419123397937715,0.18019650439127677,0.13897340816504675,0.17037418326893772,0.14196091135479094,0.10239011520964553,0.056221457885781326,0.056221457885781326,0.07213700700422954,0.07367950378560785,0.2970390234311171,0.027627808770443196,0.0928355584740105,0.09189551555804581,0.07410357166133799,0.07130172742016307,0.07323520969443968,0.09705537428781863,0.06238196953029654,0.0933874285454061,0.07558201994576999,0.07298443542345509,0.08936639797474964,0.06031174896680442,0.06812972472357946,0.07508863402394508,0.10214318409900554,0.11529119258978314,0.08145612467820758,0.08864198707978886,0.09938289761535657,0.10885519210795275,0.07524728229549742,0.027627808770443196,0.17142350819764252,0.15733703551586514,0.14788932305191665,0.07829244254234746,0.09847167161351529,0.14142552530121827,0.07590153998552239,0.07590153998552239,0.07590153998552239,0.07590153998552239,0.06919134489550352,0.06386650972292104,0.07492460277152312,0.08109153639465008,0.06145296198087098,0.007955224735505402,0.05881672746634976,0.07384102674627135,0.08413699881148344,0.11305384186698365,0.15225963654282523,0.11896345815998786,0.05910171857274614,0.21248404179745775,0.09315673656754642,0.130076410148015],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Attack\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Fire Type (1=Fire)\"}},\"legend\":{\"title\":{\"text\":\"Legendary Status\"},\"tracegroupgap\":0},\"title\":{\"text\":\"Synergistic Model: Fire Type Prediction with Interaction Terms\"}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('f6061a5f-4cc7-47c8-a385-2c354d172567');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Additive Model Summary:\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:               str8fyre   No. Observations:                  800\n",
      "Model:                          Logit   Df Residuals:                      791\n",
      "Method:                           MLE   Df Model:                            8\n",
      "Date:                Mon, 11 Nov 2024   Pseudo R-squ.:                 0.03427\n",
      "Time:                        21:06:23   Log-Likelihood:                -185.81\n",
      "converged:                       True   LL-Null:                       -192.41\n",
      "Covariance Type:            nonrobust   LLR p-value:                    0.1055\n",
      "======================================================================================\n",
      "                         coef    std err          z      P>|z|      [0.025      0.975]\n",
      "--------------------------------------------------------------------------------------\n",
      "Intercept             -2.3503      0.516     -4.552      0.000      -3.362      -1.338\n",
      "Legendary[T.True]      0.1073      0.551      0.195      0.846      -0.973       1.188\n",
      "C(Generation)[T.2]    -0.0438      0.465     -0.094      0.925      -0.956       0.868\n",
      "C(Generation)[T.3]    -0.6347      0.465     -1.365      0.172      -1.546       0.276\n",
      "C(Generation)[T.4]    -0.7699      0.540     -1.427      0.154      -1.828       0.288\n",
      "C(Generation)[T.5]    -0.5195      0.446     -1.165      0.244      -1.393       0.354\n",
      "C(Generation)[T.6]     0.2439      0.472      0.516      0.606      -0.682       1.170\n",
      "Attack                 0.0123      0.005      2.389      0.017       0.002       0.022\n",
      "Defense               -0.0148      0.006     -2.339      0.019      -0.027      -0.002\n",
      "======================================================================================\n",
      "\n",
      "Synergistic Model Summary:\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:               str8fyre   No. Observations:                  800\n",
      "Model:                          Logit   Df Residuals:                      788\n",
      "Method:                           MLE   Df Model:                           11\n",
      "Date:                Mon, 11 Nov 2024   Pseudo R-squ.:                 0.05156\n",
      "Time:                        21:06:23   Log-Likelihood:                -182.49\n",
      "converged:                       True   LL-Null:                       -192.41\n",
      "Covariance Type:            nonrobust   LLR p-value:                   0.04757\n",
      "============================================================================================================\n",
      "                                               coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "Intercept                                   -3.2644      0.714     -4.572      0.000      -4.664      -1.865\n",
      "Legendary[T.True]                            4.3478      2.179      1.996      0.046       0.078       8.618\n",
      "I(Q(\"Type 2\") == \"None\")[T.True]             1.5432      0.853      1.810      0.070      -0.128       3.215\n",
      "C(Generation)[T.2]                          -0.0574      0.468     -0.123      0.902      -0.975       0.861\n",
      "C(Generation)[T.3]                          -0.6480      0.466     -1.390      0.164      -1.561       0.265\n",
      "C(Generation)[T.4]                          -0.8255      0.545     -1.516      0.130      -1.893       0.242\n",
      "C(Generation)[T.5]                          -0.5375      0.449     -1.198      0.231      -1.417       0.342\n",
      "C(Generation)[T.6]                           0.3213      0.477      0.673      0.501      -0.614       1.257\n",
      "Attack                                       0.0172      0.006      3.086      0.002       0.006       0.028\n",
      "Attack:Legendary[T.True]                    -0.0365      0.019     -1.884      0.060      -0.074       0.001\n",
      "Defense                                     -0.0098      0.008     -1.247      0.213      -0.025       0.006\n",
      "Defense:I(Q(\"Type 2\") == \"None\")[T.True]    -0.0197      0.012     -1.651      0.099      -0.043       0.004\n",
      "============================================================================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "\n",
    "# Load data\n",
    "url = \"https://raw.githubusercontent.com/KeithGalli/pandas/master/pokemon_data.csv\"\n",
    "pokeaman = pd.read_csv(url).fillna('None')\n",
    "\n",
    "# Create the binary variable for Fire type\n",
    "pokeaman['str8fyre'] = (pokeaman['Type 1']=='Fire').astype(int)\n",
    "\n",
    "# Additive model (no interaction terms)\n",
    "additive_formula = 'str8fyre ~ Attack + Legendary + Defense + C(Generation)'\n",
    "additive_model = smf.logit(additive_formula, data=pokeaman).fit()\n",
    "\n",
    "# Get predicted probabilities for the additive model\n",
    "pokeaman['additive_pred'] = additive_model.predict(pokeaman)\n",
    "\n",
    "# Create a plotly scatter plot for the additive model\n",
    "additive_fig = px.scatter(pokeaman, x='Attack', y='str8fyre', color='Legendary',\n",
    "                          labels={'Attack': 'Attack', 'str8fyre': 'Fire Type (1=Fire)', 'Legendary': 'Legendary Status'},\n",
    "                          title=\"Additive Model: Fire Type Prediction by Attack and Legendary Status\")\n",
    "additive_fig.add_scatter(x=pokeaman['Attack'], y=pokeaman['additive_pred'], mode='lines', name=\"Best Fit Line\")\n",
    "\n",
    "# Synergistic model (with interaction terms)\n",
    "synergistic_formula = 'str8fyre ~ Attack*Legendary + Defense*I(Q(\"Type 2\")==\"None\") + C(Generation)'\n",
    "synergistic_model = smf.logit(synergistic_formula, data=pokeaman).fit()\n",
    "\n",
    "# Get predicted probabilities for the synergistic model\n",
    "pokeaman['synergistic_pred'] = synergistic_model.predict(pokeaman)\n",
    "\n",
    "# Create a plotly scatter plot for the synergistic model\n",
    "synergistic_fig = px.scatter(pokeaman, x='Attack', y='str8fyre', color='Legendary',\n",
    "                             labels={'Attack': 'Attack', 'str8fyre': 'Fire Type (1=Fire)', 'Legendary': 'Legendary Status'},\n",
    "                             title=\"Synergistic Model: Fire Type Prediction with Interaction Terms\")\n",
    "synergistic_fig.add_scatter(x=pokeaman['Attack'], y=pokeaman['synergistic_pred'], mode='lines', name=\"Best Fit Line\")\n",
    "\n",
    "# Show both figures\n",
    "additive_fig.show()\n",
    "synergistic_fig.show()\n",
    "\n",
    "# Display model summaries\n",
    "print(\"Additive Model Summary:\")\n",
    "print(additive_model.summary())\n",
    "print(\"\\nSynergistic Model Summary:\")\n",
    "print(synergistic_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c298dc",
   "metadata": {},
   "source": [
    "Here’s a summary of our discussion for your homework:\n",
    "\n",
    "Summary of Discussion:\n",
    "\n",
    "The conversation focused on an issue encountered while fitting a logistic regression model using the statsmodels library in Python. The model formula included multiple variables, such as Attack, Legendary, Defense, Type 1, and Generation, to predict a binary outcome variable, str8fyre. The error arose during the model fitting process, where the formula was not parsed correctly.\n",
    "\n",
    "The issue seemed to stem from the use of column names with spaces or special characters (e.g., Type 1), which caused issues with formula parsing. Several troubleshooting steps were discussed, including:\n",
    "\t•\tChecking the column names in the DataFrame to ensure they match those used in the formula.\n",
    "\t•\tAttempting to use the C() function to specify categorical variables correctly.\n",
    "\t•\tTrying to rename columns with spaces or special characters to simpler names.\n",
    "\n",
    "Despite these efforts, the error persisted, and the user decided to seek further assistance in a new discussion.\n",
    "\n",
    "This should be useful for your homework as an overview of the problem and the steps taken during the troubleshooting process.\n",
    "\n",
    "Summary of Discussion for Homework:\n",
    "\n",
    "In this analysis, we explored two different specifications of a logistic regression model to predict whether a Pokémon is of the Fire type (str8fyre), based on its attributes, including Attack, Legendary status, Defense, and Generation. We compared an additive model and a synergistic model, both of which were visualized using Plotly.\n",
    "\t1.\tAdditive Model: This model considered the individual effects of the predictors (Attack, Legendary, Defense, and Generation) on the probability of being a Fire-type Pokémon, without any interaction terms. The best fit line showed how each variable independently contributed to the model’s predictions.\n",
    "\t2.\tSynergistic Model: In contrast, this model incorporated interaction terms, specifically between Attack and Legendary as well as Defense and Type 2. These interaction terms suggest that the effect of one variable (e.g., Attack) on the probability of being Fire-type may depend on the level of another variable (e.g., whether the Pokémon is Legendary). The addition of these terms was reflected in a different best fit line.\n",
    "\n",
    "Key Findings:\n",
    "\n",
    "\t•\tAdditive vs. Synergistic Models: By comparing the two models’ best fit lines, we observed whether the interaction terms were necessary. If the lines were similar, it suggested that interactions between variables did not add significant value to the model. If the lines differed, it indicated that the interaction terms improved the model’s fit to the data.\n",
    "\t•\tVisualizing the Model: Plotly was used to create scatter plots with the best fit lines for both models. This provided a clear visual comparison of how the models predicted the Fire-type probability based on Attack and Legendary status.\n",
    "\n",
    "Conclusion:\n",
    "\n",
    "The necessity of interaction terms depends on how well they capture the complexity of the data. In cases where the best fit lines of both models are similar, the additive model may be sufficient. However, if the interaction terms provide a better fit, they should be retained for more accurate predictions.\n",
    "\n",
    "https://chatgpt.com/share/67317a1c-4da8-8004-8acd-f8d59e8396e6\n",
    "https://chatgpt.com/share/67317a25-d98c-8004-b12b-1a0d900cf7f2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac956502",
   "metadata": {},
   "source": [
    "# 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e95eac0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#</th>\n",
       "      <th>Name</th>\n",
       "      <th>Type 1</th>\n",
       "      <th>Type 2</th>\n",
       "      <th>HP</th>\n",
       "      <th>Attack</th>\n",
       "      <th>Defense</th>\n",
       "      <th>Sp. Atk</th>\n",
       "      <th>Sp. Def</th>\n",
       "      <th>Speed</th>\n",
       "      <th>Generation</th>\n",
       "      <th>Legendary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Bulbasaur</td>\n",
       "      <td>Grass</td>\n",
       "      <td>Poison</td>\n",
       "      <td>45</td>\n",
       "      <td>49</td>\n",
       "      <td>49</td>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Ivysaur</td>\n",
       "      <td>Grass</td>\n",
       "      <td>Poison</td>\n",
       "      <td>60</td>\n",
       "      <td>62</td>\n",
       "      <td>63</td>\n",
       "      <td>80</td>\n",
       "      <td>80</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Venusaur</td>\n",
       "      <td>Grass</td>\n",
       "      <td>Poison</td>\n",
       "      <td>80</td>\n",
       "      <td>82</td>\n",
       "      <td>83</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>VenusaurMega Venusaur</td>\n",
       "      <td>Grass</td>\n",
       "      <td>Poison</td>\n",
       "      <td>80</td>\n",
       "      <td>100</td>\n",
       "      <td>123</td>\n",
       "      <td>122</td>\n",
       "      <td>120</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Charmander</td>\n",
       "      <td>Fire</td>\n",
       "      <td>NaN</td>\n",
       "      <td>39</td>\n",
       "      <td>52</td>\n",
       "      <td>43</td>\n",
       "      <td>60</td>\n",
       "      <td>50</td>\n",
       "      <td>65</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>719</td>\n",
       "      <td>Diancie</td>\n",
       "      <td>Rock</td>\n",
       "      <td>Fairy</td>\n",
       "      <td>50</td>\n",
       "      <td>100</td>\n",
       "      <td>150</td>\n",
       "      <td>100</td>\n",
       "      <td>150</td>\n",
       "      <td>50</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>796</th>\n",
       "      <td>719</td>\n",
       "      <td>DiancieMega Diancie</td>\n",
       "      <td>Rock</td>\n",
       "      <td>Fairy</td>\n",
       "      <td>50</td>\n",
       "      <td>160</td>\n",
       "      <td>110</td>\n",
       "      <td>160</td>\n",
       "      <td>110</td>\n",
       "      <td>110</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797</th>\n",
       "      <td>720</td>\n",
       "      <td>HoopaHoopa Confined</td>\n",
       "      <td>Psychic</td>\n",
       "      <td>Ghost</td>\n",
       "      <td>80</td>\n",
       "      <td>110</td>\n",
       "      <td>60</td>\n",
       "      <td>150</td>\n",
       "      <td>130</td>\n",
       "      <td>70</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798</th>\n",
       "      <td>720</td>\n",
       "      <td>HoopaHoopa Unbound</td>\n",
       "      <td>Psychic</td>\n",
       "      <td>Dark</td>\n",
       "      <td>80</td>\n",
       "      <td>160</td>\n",
       "      <td>60</td>\n",
       "      <td>170</td>\n",
       "      <td>130</td>\n",
       "      <td>80</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799</th>\n",
       "      <td>721</td>\n",
       "      <td>Volcanion</td>\n",
       "      <td>Fire</td>\n",
       "      <td>Water</td>\n",
       "      <td>80</td>\n",
       "      <td>110</td>\n",
       "      <td>120</td>\n",
       "      <td>130</td>\n",
       "      <td>90</td>\n",
       "      <td>70</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>800 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       #                   Name   Type 1  Type 2  HP  Attack  Defense  \\\n",
       "0      1              Bulbasaur    Grass  Poison  45      49       49   \n",
       "1      2                Ivysaur    Grass  Poison  60      62       63   \n",
       "2      3               Venusaur    Grass  Poison  80      82       83   \n",
       "3      3  VenusaurMega Venusaur    Grass  Poison  80     100      123   \n",
       "4      4             Charmander     Fire     NaN  39      52       43   \n",
       "..   ...                    ...      ...     ...  ..     ...      ...   \n",
       "795  719                Diancie     Rock   Fairy  50     100      150   \n",
       "796  719    DiancieMega Diancie     Rock   Fairy  50     160      110   \n",
       "797  720    HoopaHoopa Confined  Psychic   Ghost  80     110       60   \n",
       "798  720     HoopaHoopa Unbound  Psychic    Dark  80     160       60   \n",
       "799  721              Volcanion     Fire   Water  80     110      120   \n",
       "\n",
       "     Sp. Atk  Sp. Def  Speed  Generation  Legendary  \n",
       "0         65       65     45           1      False  \n",
       "1         80       80     60           1      False  \n",
       "2        100      100     80           1      False  \n",
       "3        122      120     80           1      False  \n",
       "4         60       50     65           1      False  \n",
       "..       ...      ...    ...         ...        ...  \n",
       "795      100      150     50           6       True  \n",
       "796      160      110    110           6       True  \n",
       "797      150      130     70           6       True  \n",
       "798      170      130     80           6       True  \n",
       "799      130       90     70           6       True  \n",
       "\n",
       "[800 rows x 12 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/KeithGalli/pandas/master/pokemon_data.csv\"\n",
    "# fail https://github.com/KeithGalli/pandas/blob/master/pokemon_data.csv\n",
    "pokeaman = pd.read_csv(url) \n",
    "pokeaman\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7178e9f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>HP</td>        <th>  R-squared:         </th> <td>   0.176</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.164</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   15.27</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Mon, 11 Nov 2024</td> <th>  Prob (F-statistic):</th> <td>3.50e-27</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>21:06:23</td>     <th>  Log-Likelihood:    </th> <td> -3649.4</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   800</td>      <th>  AIC:               </th> <td>   7323.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   788</td>      <th>  BIC:               </th> <td>   7379.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    11</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "                 <td></td>                    <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>                       <td>   26.8971</td> <td>    5.246</td> <td>    5.127</td> <td> 0.000</td> <td>   16.599</td> <td>   37.195</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Generation)[T.2]</th>              <td>   20.0449</td> <td>    7.821</td> <td>    2.563</td> <td> 0.011</td> <td>    4.692</td> <td>   35.398</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Generation)[T.3]</th>              <td>   21.3662</td> <td>    6.998</td> <td>    3.053</td> <td> 0.002</td> <td>    7.629</td> <td>   35.103</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Generation)[T.4]</th>              <td>   31.9575</td> <td>    8.235</td> <td>    3.881</td> <td> 0.000</td> <td>   15.793</td> <td>   48.122</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Generation)[T.5]</th>              <td>    9.4926</td> <td>    7.883</td> <td>    1.204</td> <td> 0.229</td> <td>   -5.982</td> <td>   24.968</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Generation)[T.6]</th>              <td>   22.2693</td> <td>    8.709</td> <td>    2.557</td> <td> 0.011</td> <td>    5.173</td> <td>   39.366</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q(\"Sp. Def\")</th>                    <td>    0.5634</td> <td>    0.071</td> <td>    7.906</td> <td> 0.000</td> <td>    0.423</td> <td>    0.703</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q(\"Sp. Def\"):C(Generation)[T.2]</th> <td>   -0.2350</td> <td>    0.101</td> <td>   -2.316</td> <td> 0.021</td> <td>   -0.434</td> <td>   -0.036</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q(\"Sp. Def\"):C(Generation)[T.3]</th> <td>   -0.3067</td> <td>    0.093</td> <td>   -3.300</td> <td> 0.001</td> <td>   -0.489</td> <td>   -0.124</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q(\"Sp. Def\"):C(Generation)[T.4]</th> <td>   -0.3790</td> <td>    0.105</td> <td>   -3.600</td> <td> 0.000</td> <td>   -0.586</td> <td>   -0.172</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q(\"Sp. Def\"):C(Generation)[T.5]</th> <td>   -0.0484</td> <td>    0.108</td> <td>   -0.447</td> <td> 0.655</td> <td>   -0.261</td> <td>    0.164</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q(\"Sp. Def\"):C(Generation)[T.6]</th> <td>   -0.3083</td> <td>    0.112</td> <td>   -2.756</td> <td> 0.006</td> <td>   -0.528</td> <td>   -0.089</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>337.229</td> <th>  Durbin-Watson:     </th> <td>   1.505</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>2871.522</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 1.684</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td>11.649</td>  <th>  Cond. No.          </th> <td>1.40e+03</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.4e+03. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}                  &        HP        & \\textbf{  R-squared:         } &     0.176   \\\\\n",
       "\\textbf{Model:}                          &       OLS        & \\textbf{  Adj. R-squared:    } &     0.164   \\\\\n",
       "\\textbf{Method:}                         &  Least Squares   & \\textbf{  F-statistic:       } &     15.27   \\\\\n",
       "\\textbf{Date:}                           & Mon, 11 Nov 2024 & \\textbf{  Prob (F-statistic):} &  3.50e-27   \\\\\n",
       "\\textbf{Time:}                           &     21:06:23     & \\textbf{  Log-Likelihood:    } &   -3649.4   \\\\\n",
       "\\textbf{No. Observations:}               &         800      & \\textbf{  AIC:               } &     7323.   \\\\\n",
       "\\textbf{Df Residuals:}                   &         788      & \\textbf{  BIC:               } &     7379.   \\\\\n",
       "\\textbf{Df Model:}                       &          11      & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}                &    nonrobust     & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                                         & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{Intercept}                       &      26.8971  &        5.246     &     5.127  &         0.000        &       16.599    &       37.195     \\\\\n",
       "\\textbf{C(Generation)[T.2]}              &      20.0449  &        7.821     &     2.563  &         0.011        &        4.692    &       35.398     \\\\\n",
       "\\textbf{C(Generation)[T.3]}              &      21.3662  &        6.998     &     3.053  &         0.002        &        7.629    &       35.103     \\\\\n",
       "\\textbf{C(Generation)[T.4]}              &      31.9575  &        8.235     &     3.881  &         0.000        &       15.793    &       48.122     \\\\\n",
       "\\textbf{C(Generation)[T.5]}              &       9.4926  &        7.883     &     1.204  &         0.229        &       -5.982    &       24.968     \\\\\n",
       "\\textbf{C(Generation)[T.6]}              &      22.2693  &        8.709     &     2.557  &         0.011        &        5.173    &       39.366     \\\\\n",
       "\\textbf{Q(\"Sp. Def\")}                    &       0.5634  &        0.071     &     7.906  &         0.000        &        0.423    &        0.703     \\\\\n",
       "\\textbf{Q(\"Sp. Def\"):C(Generation)[T.2]} &      -0.2350  &        0.101     &    -2.316  &         0.021        &       -0.434    &       -0.036     \\\\\n",
       "\\textbf{Q(\"Sp. Def\"):C(Generation)[T.3]} &      -0.3067  &        0.093     &    -3.300  &         0.001        &       -0.489    &       -0.124     \\\\\n",
       "\\textbf{Q(\"Sp. Def\"):C(Generation)[T.4]} &      -0.3790  &        0.105     &    -3.600  &         0.000        &       -0.586    &       -0.172     \\\\\n",
       "\\textbf{Q(\"Sp. Def\"):C(Generation)[T.5]} &      -0.0484  &        0.108     &    -0.447  &         0.655        &       -0.261    &        0.164     \\\\\n",
       "\\textbf{Q(\"Sp. Def\"):C(Generation)[T.6]} &      -0.3083  &        0.112     &    -2.756  &         0.006        &       -0.528    &       -0.089     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 337.229 & \\textbf{  Durbin-Watson:     } &    1.505  \\\\\n",
       "\\textbf{Prob(Omnibus):} &   0.000 & \\textbf{  Jarque-Bera (JB):  } & 2871.522  \\\\\n",
       "\\textbf{Skew:}          &   1.684 & \\textbf{  Prob(JB):          } &     0.00  \\\\\n",
       "\\textbf{Kurtosis:}      &  11.649 & \\textbf{  Cond. No.          } & 1.40e+03  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. \\newline\n",
       " [2] The condition number is large, 1.4e+03. This might indicate that there are \\newline\n",
       " strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                     HP   R-squared:                       0.176\n",
       "Model:                            OLS   Adj. R-squared:                  0.164\n",
       "Method:                 Least Squares   F-statistic:                     15.27\n",
       "Date:                Mon, 11 Nov 2024   Prob (F-statistic):           3.50e-27\n",
       "Time:                        21:06:23   Log-Likelihood:                -3649.4\n",
       "No. Observations:                 800   AIC:                             7323.\n",
       "Df Residuals:                     788   BIC:                             7379.\n",
       "Df Model:                          11                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "===================================================================================================\n",
       "                                      coef    std err          t      P>|t|      [0.025      0.975]\n",
       "---------------------------------------------------------------------------------------------------\n",
       "Intercept                          26.8971      5.246      5.127      0.000      16.599      37.195\n",
       "C(Generation)[T.2]                 20.0449      7.821      2.563      0.011       4.692      35.398\n",
       "C(Generation)[T.3]                 21.3662      6.998      3.053      0.002       7.629      35.103\n",
       "C(Generation)[T.4]                 31.9575      8.235      3.881      0.000      15.793      48.122\n",
       "C(Generation)[T.5]                  9.4926      7.883      1.204      0.229      -5.982      24.968\n",
       "C(Generation)[T.6]                 22.2693      8.709      2.557      0.011       5.173      39.366\n",
       "Q(\"Sp. Def\")                        0.5634      0.071      7.906      0.000       0.423       0.703\n",
       "Q(\"Sp. Def\"):C(Generation)[T.2]    -0.2350      0.101     -2.316      0.021      -0.434      -0.036\n",
       "Q(\"Sp. Def\"):C(Generation)[T.3]    -0.3067      0.093     -3.300      0.001      -0.489      -0.124\n",
       "Q(\"Sp. Def\"):C(Generation)[T.4]    -0.3790      0.105     -3.600      0.000      -0.586      -0.172\n",
       "Q(\"Sp. Def\"):C(Generation)[T.5]    -0.0484      0.108     -0.447      0.655      -0.261       0.164\n",
       "Q(\"Sp. Def\"):C(Generation)[T.6]    -0.3083      0.112     -2.756      0.006      -0.528      -0.089\n",
       "==============================================================================\n",
       "Omnibus:                      337.229   Durbin-Watson:                   1.505\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             2871.522\n",
       "Skew:                           1.684   Prob(JB):                         0.00\n",
       "Kurtosis:                      11.649   Cond. No.                     1.40e+03\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.4e+03. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "\n",
    "model1_spec = smf.ols(formula='HP ~ Q(\"Sp. Def\") + C(Generation)', data=pokeaman)\n",
    "model2_spec = smf.ols(formula='HP ~ Q(\"Sp. Def\") + C(Generation) + Q(\"Sp. Def\"):C(Generation)', data=pokeaman)\n",
    "model2_spec = smf.ols(formula='HP ~ Q(\"Sp. Def\") * C(Generation)', data=pokeaman)\n",
    "\n",
    "model2_fit = model2_spec.fit()\n",
    "model2_fit.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9f211b",
   "metadata": {},
   "source": [
    "- \"the model only explains 17.6% of the variability in the data\": means the R^2 which is the variance of the dependent variables. only small part of the R^2 means that this model cannot explain the variability well. This may possible because there may exsit other factor that the model didn't involve. \n",
    "\n",
    "- \"many of the coefficients are larger than 10 while having strong or very strong evidence against the null hypothesis of 'no effect'\": this predicted variables have strong effect to the dependent variables and p-value usually smaller than 0.05.\n",
    "\n",
    "- together: R^2 small doesn't means the model is useless. there manybe exist small part of variables that have strong effect on dependent variables and the coefficent my very obvious.\n",
    "\n",
    "Here’s a summary of the discussion for your homework:\n",
    "\n",
    "In the context of the model, R² of 17.6% means that the model explains only a small portion (17.6%) of the variability in the dependent variable, indicating that there may be other unaccounted factors influencing the outcome. However, despite the low R², some of the predictor coefficients are large and have strong statistical significance, meaning these predictors have a strong effect on the dependent variable. The small p-values (typically < 0.05) provide strong evidence to reject the null hypothesis, suggesting that the predictors are not due to random chance.\n",
    "\n",
    "In summary, while the model doesn’t explain much of the overall variability (low R²), the significant predictors included in the model show strong and meaningful effects on the outcome variable.\n",
    "\n",
    "Let me know if you need further adjustments!\n",
    "\n",
    "https://chatgpt.com/share/67317d4c-3530-8004-8c48-89cfb6b157a8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b40682",
   "metadata": {},
   "source": [
    "# 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0cfd06e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#</th>\n",
       "      <th>Name</th>\n",
       "      <th>Type 1</th>\n",
       "      <th>Type 2</th>\n",
       "      <th>HP</th>\n",
       "      <th>Attack</th>\n",
       "      <th>Defense</th>\n",
       "      <th>Sp. Atk</th>\n",
       "      <th>Sp. Def</th>\n",
       "      <th>Speed</th>\n",
       "      <th>Generation</th>\n",
       "      <th>Legendary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>370</th>\n",
       "      <td>338</td>\n",
       "      <td>Solrock</td>\n",
       "      <td>Rock</td>\n",
       "      <td>Psychic</td>\n",
       "      <td>70</td>\n",
       "      <td>95</td>\n",
       "      <td>85</td>\n",
       "      <td>55</td>\n",
       "      <td>65</td>\n",
       "      <td>70</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>Charizard</td>\n",
       "      <td>Fire</td>\n",
       "      <td>Flying</td>\n",
       "      <td>78</td>\n",
       "      <td>84</td>\n",
       "      <td>78</td>\n",
       "      <td>109</td>\n",
       "      <td>85</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>224</td>\n",
       "      <td>Octillery</td>\n",
       "      <td>Water</td>\n",
       "      <td>None</td>\n",
       "      <td>75</td>\n",
       "      <td>105</td>\n",
       "      <td>75</td>\n",
       "      <td>105</td>\n",
       "      <td>75</td>\n",
       "      <td>45</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>661</th>\n",
       "      <td>600</td>\n",
       "      <td>Klang</td>\n",
       "      <td>Steel</td>\n",
       "      <td>None</td>\n",
       "      <td>60</td>\n",
       "      <td>80</td>\n",
       "      <td>95</td>\n",
       "      <td>70</td>\n",
       "      <td>85</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>265</td>\n",
       "      <td>Wurmple</td>\n",
       "      <td>Bug</td>\n",
       "      <td>None</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>35</td>\n",
       "      <td>20</td>\n",
       "      <td>30</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>522</th>\n",
       "      <td>471</td>\n",
       "      <td>Glaceon</td>\n",
       "      <td>Ice</td>\n",
       "      <td>None</td>\n",
       "      <td>65</td>\n",
       "      <td>60</td>\n",
       "      <td>110</td>\n",
       "      <td>130</td>\n",
       "      <td>95</td>\n",
       "      <td>65</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>225</td>\n",
       "      <td>Delibird</td>\n",
       "      <td>Ice</td>\n",
       "      <td>Flying</td>\n",
       "      <td>45</td>\n",
       "      <td>55</td>\n",
       "      <td>45</td>\n",
       "      <td>65</td>\n",
       "      <td>45</td>\n",
       "      <td>75</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797</th>\n",
       "      <td>720</td>\n",
       "      <td>HoopaHoopa Confined</td>\n",
       "      <td>Psychic</td>\n",
       "      <td>Ghost</td>\n",
       "      <td>80</td>\n",
       "      <td>110</td>\n",
       "      <td>60</td>\n",
       "      <td>150</td>\n",
       "      <td>130</td>\n",
       "      <td>70</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>109</td>\n",
       "      <td>Koffing</td>\n",
       "      <td>Poison</td>\n",
       "      <td>None</td>\n",
       "      <td>40</td>\n",
       "      <td>65</td>\n",
       "      <td>95</td>\n",
       "      <td>60</td>\n",
       "      <td>45</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>373</td>\n",
       "      <td>SalamenceMega Salamence</td>\n",
       "      <td>Dragon</td>\n",
       "      <td>Flying</td>\n",
       "      <td>95</td>\n",
       "      <td>145</td>\n",
       "      <td>130</td>\n",
       "      <td>120</td>\n",
       "      <td>90</td>\n",
       "      <td>120</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       #                     Name   Type 1   Type 2  HP  Attack  Defense  \\\n",
       "370  338                  Solrock     Rock  Psychic  70      95       85   \n",
       "6      6                Charizard     Fire   Flying  78      84       78   \n",
       "242  224                Octillery    Water     None  75     105       75   \n",
       "661  600                    Klang    Steel     None  60      80       95   \n",
       "288  265                  Wurmple      Bug     None  45      45       35   \n",
       "..   ...                      ...      ...      ...  ..     ...      ...   \n",
       "522  471                  Glaceon      Ice     None  65      60      110   \n",
       "243  225                 Delibird      Ice   Flying  45      55       45   \n",
       "797  720      HoopaHoopa Confined  Psychic    Ghost  80     110       60   \n",
       "117  109                  Koffing   Poison     None  40      65       95   \n",
       "409  373  SalamenceMega Salamence   Dragon   Flying  95     145      130   \n",
       "\n",
       "     Sp. Atk  Sp. Def  Speed  Generation  Legendary  \n",
       "370       55       65     70           3      False  \n",
       "6        109       85    100           1      False  \n",
       "242      105       75     45           2      False  \n",
       "661       70       85     50           5      False  \n",
       "288       20       30     20           3      False  \n",
       "..       ...      ...    ...         ...        ...  \n",
       "522      130       95     65           4      False  \n",
       "243       65       45     75           2      False  \n",
       "797      150      130     70           6       True  \n",
       "117       60       45     35           1      False  \n",
       "409      120       90    120           3      False  \n",
       "\n",
       "[400 rows x 12 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "fifty_fifty_split_size = int(pokeaman.shape[0]*0.5)\n",
    "\n",
    "# Replace \"NaN\" (in the \"Type 2\" column with \"None\")\n",
    "pokeaman.fillna('None', inplace=True)\n",
    "\n",
    "np.random.seed(130)\n",
    "pokeaman_train,pokeaman_test = \\\n",
    "  train_test_split(pokeaman, train_size=fifty_fifty_split_size)\n",
    "pokeaman_train\n",
    "\n",
    "## this code replace the missing values in the dataframe\n",
    "## split half dataframe into train set and show it\n",
    "## the other half comes into test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f1b77600",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>HP</td>        <th>  R-squared:         </th> <td>   0.148</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.143</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   34.40</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Mon, 11 Nov 2024</td> <th>  Prob (F-statistic):</th> <td>1.66e-14</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>21:06:24</td>     <th>  Log-Likelihood:    </th> <td> -1832.6</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   400</td>      <th>  AIC:               </th> <td>   3671.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   397</td>      <th>  BIC:               </th> <td>   3683.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td>   42.5882</td> <td>    3.580</td> <td>   11.897</td> <td> 0.000</td> <td>   35.551</td> <td>   49.626</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack</th>    <td>    0.2472</td> <td>    0.041</td> <td>    6.051</td> <td> 0.000</td> <td>    0.167</td> <td>    0.327</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense</th>   <td>    0.1001</td> <td>    0.045</td> <td>    2.201</td> <td> 0.028</td> <td>    0.011</td> <td>    0.190</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>284.299</td> <th>  Durbin-Watson:     </th> <td>   2.006</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>5870.841</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 2.720</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td>20.963</td>  <th>  Cond. No.          </th> <td>    343.</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}    &        HP        & \\textbf{  R-squared:         } &     0.148   \\\\\n",
       "\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared:    } &     0.143   \\\\\n",
       "\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       } &     34.40   \\\\\n",
       "\\textbf{Date:}             & Mon, 11 Nov 2024 & \\textbf{  Prob (F-statistic):} &  1.66e-14   \\\\\n",
       "\\textbf{Time:}             &     21:06:24     & \\textbf{  Log-Likelihood:    } &   -1832.6   \\\\\n",
       "\\textbf{No. Observations:} &         400      & \\textbf{  AIC:               } &     3671.   \\\\\n",
       "\\textbf{Df Residuals:}     &         397      & \\textbf{  BIC:               } &     3683.   \\\\\n",
       "\\textbf{Df Model:}         &           2      & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                   & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{Intercept} &      42.5882  &        3.580     &    11.897  &         0.000        &       35.551    &       49.626     \\\\\n",
       "\\textbf{Attack}    &       0.2472  &        0.041     &     6.051  &         0.000        &        0.167    &        0.327     \\\\\n",
       "\\textbf{Defense}   &       0.1001  &        0.045     &     2.201  &         0.028        &        0.011    &        0.190     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 284.299 & \\textbf{  Durbin-Watson:     } &    2.006  \\\\\n",
       "\\textbf{Prob(Omnibus):} &   0.000 & \\textbf{  Jarque-Bera (JB):  } & 5870.841  \\\\\n",
       "\\textbf{Skew:}          &   2.720 & \\textbf{  Prob(JB):          } &     0.00  \\\\\n",
       "\\textbf{Kurtosis:}      &  20.963 & \\textbf{  Cond. No.          } &     343.  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                     HP   R-squared:                       0.148\n",
       "Model:                            OLS   Adj. R-squared:                  0.143\n",
       "Method:                 Least Squares   F-statistic:                     34.40\n",
       "Date:                Mon, 11 Nov 2024   Prob (F-statistic):           1.66e-14\n",
       "Time:                        21:06:24   Log-Likelihood:                -1832.6\n",
       "No. Observations:                 400   AIC:                             3671.\n",
       "Df Residuals:                     397   BIC:                             3683.\n",
       "Df Model:                           2                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept     42.5882      3.580     11.897      0.000      35.551      49.626\n",
       "Attack         0.2472      0.041      6.051      0.000       0.167       0.327\n",
       "Defense        0.1001      0.045      2.201      0.028       0.011       0.190\n",
       "==============================================================================\n",
       "Omnibus:                      284.299   Durbin-Watson:                   2.006\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             5870.841\n",
       "Skew:                           2.720   Prob(JB):                         0.00\n",
       "Kurtosis:                      20.963   Cond. No.                         343.\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_spec3 = smf.ols(formula='HP ~ Attack + Defense', \n",
    "                      data=pokeaman_train)\n",
    "model3_fit = model_spec3.fit()\n",
    "model3_fit.summary()\n",
    "\n",
    "## use OLS method to fit a linear regression model\n",
    "## use data in the trainning set\n",
    "## use attack values to predict defense values\n",
    "## show the result and the statistical measues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d8f69f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'In sample' R-squared:     0.14771558304519894\n",
      "'Out of sample' R-squared: 0.21208501873920738\n"
     ]
    }
   ],
   "source": [
    "yhat_model3 = model3_fit.predict(pokeaman_test)\n",
    "y = pokeaman_test.HP\n",
    "print(\"'In sample' R-squared:    \", model3_fit.rsquared)\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat_model3)[0,1]**2)\n",
    "\n",
    "## testing which dataset has the better effect of fitting linear regression model\n",
    "## Comparing training set and testing set\n",
    "## trainning set = in sample\n",
    "## testing set = out of sample\n",
    "## use R^2 to show the fitting effect good or not.\n",
    "## it shows whether the model is:\n",
    "    ## Overfitting (performing well on the training data but poorly on the test data).\n",
    "    ## Underfitting (performing poorly on both the training and test data).\n",
    "    ## Well-generalized (performing well on both training and test data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "408e7f77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>HP</td>        <th>  R-squared:         </th> <td>   0.467</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.369</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   4.764</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Mon, 11 Nov 2024</td> <th>  Prob (F-statistic):</th> <td>4.23e-21</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>21:06:24</td>     <th>  Log-Likelihood:    </th> <td> -1738.6</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   400</td>      <th>  AIC:               </th> <td>   3603.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   337</td>      <th>  BIC:               </th> <td>   3855.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    62</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "                                  <td></td>                                    <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>                                                        <td>  521.5715</td> <td>  130.273</td> <td>    4.004</td> <td> 0.000</td> <td>  265.322</td> <td>  777.821</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Legendary[T.True]</th>                                                <td>   -6.1179</td> <td>    2.846</td> <td>   -2.150</td> <td> 0.032</td> <td>  -11.716</td> <td>   -0.520</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack</th>                                                           <td>   -8.1938</td> <td>    2.329</td> <td>   -3.518</td> <td> 0.000</td> <td>  -12.775</td> <td>   -3.612</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Legendary[T.True]</th>                                         <td>-1224.9610</td> <td>  545.105</td> <td>   -2.247</td> <td> 0.025</td> <td>-2297.199</td> <td> -152.723</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense</th>                                                          <td>   -6.1989</td> <td>    2.174</td> <td>   -2.851</td> <td> 0.005</td> <td>  -10.475</td> <td>   -1.923</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:Legendary[T.True]</th>                                        <td> -102.4030</td> <td>   96.565</td> <td>   -1.060</td> <td> 0.290</td> <td> -292.350</td> <td>   87.544</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense</th>                                                   <td>    0.0985</td> <td>    0.033</td> <td>    2.982</td> <td> 0.003</td> <td>    0.034</td> <td>    0.164</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense:Legendary[T.True]</th>                                 <td>   14.6361</td> <td>    6.267</td> <td>    2.336</td> <td> 0.020</td> <td>    2.310</td> <td>   26.963</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Speed</th>                                                            <td>   -7.2261</td> <td>    2.178</td> <td>   -3.318</td> <td> 0.001</td> <td>  -11.511</td> <td>   -2.942</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Speed:Legendary[T.True]</th>                                          <td>  704.8798</td> <td>  337.855</td> <td>    2.086</td> <td> 0.038</td> <td>   40.309</td> <td> 1369.450</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Speed</th>                                                     <td>    0.1264</td> <td>    0.038</td> <td>    3.351</td> <td> 0.001</td> <td>    0.052</td> <td>    0.201</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Speed:Legendary[T.True]</th>                                   <td>    5.8648</td> <td>    2.692</td> <td>    2.179</td> <td> 0.030</td> <td>    0.570</td> <td>   11.160</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:Speed</th>                                                    <td>    0.1026</td> <td>    0.039</td> <td>    2.634</td> <td> 0.009</td> <td>    0.026</td> <td>    0.179</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:Speed:Legendary[T.True]</th>                                  <td>   -6.9266</td> <td>    3.465</td> <td>   -1.999</td> <td> 0.046</td> <td>  -13.742</td> <td>   -0.111</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense:Speed</th>                                             <td>   -0.0016</td> <td>    0.001</td> <td>   -2.837</td> <td> 0.005</td> <td>   -0.003</td> <td>   -0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense:Speed:Legendary[T.True]</th>                           <td>   -0.0743</td> <td>    0.030</td> <td>   -2.477</td> <td> 0.014</td> <td>   -0.133</td> <td>   -0.015</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q(\"Sp. Def\")</th>                                                     <td>   -5.3982</td> <td>    1.938</td> <td>   -2.785</td> <td> 0.006</td> <td>   -9.211</td> <td>   -1.586</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Legendary[T.True]:Q(\"Sp. Def\")</th>                                   <td> -282.2496</td> <td>  126.835</td> <td>   -2.225</td> <td> 0.027</td> <td> -531.738</td> <td>  -32.761</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Q(\"Sp. Def\")</th>                                              <td>    0.1094</td> <td>    0.034</td> <td>    3.233</td> <td> 0.001</td> <td>    0.043</td> <td>    0.176</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Legendary[T.True]:Q(\"Sp. Def\")</th>                            <td>   12.6503</td> <td>    5.851</td> <td>    2.162</td> <td> 0.031</td> <td>    1.141</td> <td>   24.160</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:Q(\"Sp. Def\")</th>                                             <td>    0.0628</td> <td>    0.028</td> <td>    2.247</td> <td> 0.025</td> <td>    0.008</td> <td>    0.118</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:Legendary[T.True]:Q(\"Sp. Def\")</th>                           <td>    3.3949</td> <td>    1.783</td> <td>    1.904</td> <td> 0.058</td> <td>   -0.112</td> <td>    6.902</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense:Q(\"Sp. Def\")</th>                                      <td>   -0.0012</td> <td>    0.000</td> <td>   -2.730</td> <td> 0.007</td> <td>   -0.002</td> <td>   -0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense:Legendary[T.True]:Q(\"Sp. Def\")</th>                    <td>   -0.1456</td> <td>    0.065</td> <td>   -2.253</td> <td> 0.025</td> <td>   -0.273</td> <td>   -0.018</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Speed:Q(\"Sp. Def\")</th>                                               <td>    0.0624</td> <td>    0.031</td> <td>    2.027</td> <td> 0.043</td> <td>    0.002</td> <td>    0.123</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Speed:Legendary[T.True]:Q(\"Sp. Def\")</th>                             <td>   -3.2219</td> <td>    1.983</td> <td>   -1.625</td> <td> 0.105</td> <td>   -7.122</td> <td>    0.678</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Speed:Q(\"Sp. Def\")</th>                                        <td>   -0.0014</td> <td>    0.001</td> <td>   -2.732</td> <td> 0.007</td> <td>   -0.002</td> <td>   -0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Speed:Legendary[T.True]:Q(\"Sp. Def\")</th>                      <td>   -0.0695</td> <td>    0.033</td> <td>   -2.100</td> <td> 0.036</td> <td>   -0.135</td> <td>   -0.004</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:Speed:Q(\"Sp. Def\")</th>                                       <td>   -0.0008</td> <td>    0.000</td> <td>   -1.743</td> <td> 0.082</td> <td>   -0.002</td> <td>    0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:Speed:Legendary[T.True]:Q(\"Sp. Def\")</th>                     <td>    0.0334</td> <td>    0.021</td> <td>    1.569</td> <td> 0.117</td> <td>   -0.008</td> <td>    0.075</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense:Speed:Q(\"Sp. Def\")</th>                                <td> 1.629e-05</td> <td> 6.92e-06</td> <td>    2.355</td> <td> 0.019</td> <td> 2.68e-06</td> <td> 2.99e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense:Speed:Legendary[T.True]:Q(\"Sp. Def\")</th>              <td>    0.0008</td> <td>    0.000</td> <td>    2.433</td> <td> 0.015</td> <td>    0.000</td> <td>    0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q(\"Sp. Atk\")</th>                                                     <td>   -8.3636</td> <td>    2.346</td> <td>   -3.565</td> <td> 0.000</td> <td>  -12.978</td> <td>   -3.749</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Legendary[T.True]:Q(\"Sp. Atk\")</th>                                   <td>  850.5436</td> <td>  385.064</td> <td>    2.209</td> <td> 0.028</td> <td>   93.112</td> <td> 1607.975</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Q(\"Sp. Atk\")</th>                                              <td>    0.1388</td> <td>    0.040</td> <td>    3.500</td> <td> 0.001</td> <td>    0.061</td> <td>    0.217</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Legendary[T.True]:Q(\"Sp. Atk\")</th>                            <td>    2.1809</td> <td>    1.136</td> <td>    1.920</td> <td> 0.056</td> <td>   -0.054</td> <td>    4.416</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:Q(\"Sp. Atk\")</th>                                             <td>    0.0831</td> <td>    0.038</td> <td>    2.162</td> <td> 0.031</td> <td>    0.007</td> <td>    0.159</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:Legendary[T.True]:Q(\"Sp. Atk\")</th>                           <td>   -7.3121</td> <td>    3.376</td> <td>   -2.166</td> <td> 0.031</td> <td>  -13.953</td> <td>   -0.671</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense:Q(\"Sp. Atk\")</th>                                      <td>   -0.0014</td> <td>    0.001</td> <td>   -2.480</td> <td> 0.014</td> <td>   -0.003</td> <td>   -0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense:Legendary[T.True]:Q(\"Sp. Atk\")</th>                    <td>   -0.0434</td> <td>    0.022</td> <td>   -2.010</td> <td> 0.045</td> <td>   -0.086</td> <td>   -0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Speed:Q(\"Sp. Atk\")</th>                                               <td>    0.1011</td> <td>    0.035</td> <td>    2.872</td> <td> 0.004</td> <td>    0.032</td> <td>    0.170</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Speed:Legendary[T.True]:Q(\"Sp. Atk\")</th>                             <td>  -12.6343</td> <td>    5.613</td> <td>   -2.251</td> <td> 0.025</td> <td>  -23.674</td> <td>   -1.594</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Speed:Q(\"Sp. Atk\")</th>                                        <td>   -0.0018</td> <td>    0.001</td> <td>   -3.102</td> <td> 0.002</td> <td>   -0.003</td> <td>   -0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Speed:Legendary[T.True]:Q(\"Sp. Atk\")</th>                      <td>    0.0151</td> <td>    0.009</td> <td>    1.609</td> <td> 0.109</td> <td>   -0.003</td> <td>    0.034</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:Speed:Q(\"Sp. Atk\")</th>                                       <td>   -0.0012</td> <td>    0.001</td> <td>   -1.860</td> <td> 0.064</td> <td>   -0.002</td> <td> 6.62e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:Speed:Legendary[T.True]:Q(\"Sp. Atk\")</th>                     <td>    0.1210</td> <td>    0.054</td> <td>    2.260</td> <td> 0.024</td> <td>    0.016</td> <td>    0.226</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense:Speed:Q(\"Sp. Atk\")</th>                                <td> 2.125e-05</td> <td>  9.1e-06</td> <td>    2.334</td> <td> 0.020</td> <td> 3.34e-06</td> <td> 3.92e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense:Speed:Legendary[T.True]:Q(\"Sp. Atk\")</th>              <td> 6.438e-06</td> <td> 7.69e-05</td> <td>    0.084</td> <td> 0.933</td> <td>   -0.000</td> <td>    0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>                                        <td>    0.1265</td> <td>    0.033</td> <td>    3.821</td> <td> 0.000</td> <td>    0.061</td> <td>    0.192</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>                      <td>   -5.0544</td> <td>    2.506</td> <td>   -2.017</td> <td> 0.044</td> <td>   -9.983</td> <td>   -0.126</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>                                 <td>   -0.0021</td> <td>    0.001</td> <td>   -3.606</td> <td> 0.000</td> <td>   -0.003</td> <td>   -0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>               <td>   -0.0346</td> <td>    0.017</td> <td>   -1.992</td> <td> 0.047</td> <td>   -0.069</td> <td>   -0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>                                <td>   -0.0012</td> <td>    0.000</td> <td>   -2.406</td> <td> 0.017</td> <td>   -0.002</td> <td>   -0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>              <td>    0.0446</td> <td>    0.025</td> <td>    1.794</td> <td> 0.074</td> <td>   -0.004</td> <td>    0.093</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>                         <td> 1.973e-05</td> <td> 7.28e-06</td> <td>    2.710</td> <td> 0.007</td> <td> 5.41e-06</td> <td>  3.4e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>       <td>    0.0005</td> <td>    0.000</td> <td>    1.957</td> <td> 0.051</td> <td>-2.56e-06</td> <td>    0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Speed:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>                                  <td>   -0.0013</td> <td>    0.000</td> <td>   -2.740</td> <td> 0.006</td> <td>   -0.002</td> <td>   -0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Speed:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>                <td>    0.0841</td> <td>    0.040</td> <td>    2.125</td> <td> 0.034</td> <td>    0.006</td> <td>    0.162</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Speed:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>                           <td> 2.379e-05</td> <td> 7.85e-06</td> <td>    3.030</td> <td> 0.003</td> <td> 8.34e-06</td> <td> 3.92e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Speed:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>         <td> 2.864e-05</td> <td> 7.73e-05</td> <td>    0.370</td> <td> 0.711</td> <td>   -0.000</td> <td>    0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:Speed:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>                          <td> 1.284e-05</td> <td> 7.46e-06</td> <td>    1.721</td> <td> 0.086</td> <td>-1.83e-06</td> <td> 2.75e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:Speed:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>        <td>   -0.0008</td> <td>    0.000</td> <td>   -2.085</td> <td> 0.038</td> <td>   -0.002</td> <td>-4.68e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense:Speed:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>                   <td> -2.53e-07</td> <td>  1.1e-07</td> <td>   -2.292</td> <td> 0.023</td> <td> -4.7e-07</td> <td>-3.59e-08</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense:Speed:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th> <td>-1.425e-06</td> <td> 1.14e-06</td> <td>   -1.249</td> <td> 0.212</td> <td>-3.67e-06</td> <td> 8.19e-07</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>214.307</td> <th>  Durbin-Watson:     </th> <td>   1.992</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>2354.664</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 2.026</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td>14.174</td>  <th>  Cond. No.          </th> <td>1.20e+16</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.2e+16. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}                                                   &        HP        & \\textbf{  R-squared:         } &     0.467   \\\\\n",
       "\\textbf{Model:}                                                           &       OLS        & \\textbf{  Adj. R-squared:    } &     0.369   \\\\\n",
       "\\textbf{Method:}                                                          &  Least Squares   & \\textbf{  F-statistic:       } &     4.764   \\\\\n",
       "\\textbf{Date:}                                                            & Mon, 11 Nov 2024 & \\textbf{  Prob (F-statistic):} &  4.23e-21   \\\\\n",
       "\\textbf{Time:}                                                            &     21:06:24     & \\textbf{  Log-Likelihood:    } &   -1738.6   \\\\\n",
       "\\textbf{No. Observations:}                                                &         400      & \\textbf{  AIC:               } &     3603.   \\\\\n",
       "\\textbf{Df Residuals:}                                                    &         337      & \\textbf{  BIC:               } &     3855.   \\\\\n",
       "\\textbf{Df Model:}                                                        &          62      & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}                                                 &    nonrobust     & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                                                                          & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{Intercept}                                                        &     521.5715  &      130.273     &     4.004  &         0.000        &      265.322    &      777.821     \\\\\n",
       "\\textbf{Legendary[T.True]}                                                &      -6.1179  &        2.846     &    -2.150  &         0.032        &      -11.716    &       -0.520     \\\\\n",
       "\\textbf{Attack}                                                           &      -8.1938  &        2.329     &    -3.518  &         0.000        &      -12.775    &       -3.612     \\\\\n",
       "\\textbf{Attack:Legendary[T.True]}                                         &   -1224.9610  &      545.105     &    -2.247  &         0.025        &    -2297.199    &     -152.723     \\\\\n",
       "\\textbf{Defense}                                                          &      -6.1989  &        2.174     &    -2.851  &         0.005        &      -10.475    &       -1.923     \\\\\n",
       "\\textbf{Defense:Legendary[T.True]}                                        &    -102.4030  &       96.565     &    -1.060  &         0.290        &     -292.350    &       87.544     \\\\\n",
       "\\textbf{Attack:Defense}                                                   &       0.0985  &        0.033     &     2.982  &         0.003        &        0.034    &        0.164     \\\\\n",
       "\\textbf{Attack:Defense:Legendary[T.True]}                                 &      14.6361  &        6.267     &     2.336  &         0.020        &        2.310    &       26.963     \\\\\n",
       "\\textbf{Speed}                                                            &      -7.2261  &        2.178     &    -3.318  &         0.001        &      -11.511    &       -2.942     \\\\\n",
       "\\textbf{Speed:Legendary[T.True]}                                          &     704.8798  &      337.855     &     2.086  &         0.038        &       40.309    &     1369.450     \\\\\n",
       "\\textbf{Attack:Speed}                                                     &       0.1264  &        0.038     &     3.351  &         0.001        &        0.052    &        0.201     \\\\\n",
       "\\textbf{Attack:Speed:Legendary[T.True]}                                   &       5.8648  &        2.692     &     2.179  &         0.030        &        0.570    &       11.160     \\\\\n",
       "\\textbf{Defense:Speed}                                                    &       0.1026  &        0.039     &     2.634  &         0.009        &        0.026    &        0.179     \\\\\n",
       "\\textbf{Defense:Speed:Legendary[T.True]}                                  &      -6.9266  &        3.465     &    -1.999  &         0.046        &      -13.742    &       -0.111     \\\\\n",
       "\\textbf{Attack:Defense:Speed}                                             &      -0.0016  &        0.001     &    -2.837  &         0.005        &       -0.003    &       -0.001     \\\\\n",
       "\\textbf{Attack:Defense:Speed:Legendary[T.True]}                           &      -0.0743  &        0.030     &    -2.477  &         0.014        &       -0.133    &       -0.015     \\\\\n",
       "\\textbf{Q(\"Sp. Def\")}                                                     &      -5.3982  &        1.938     &    -2.785  &         0.006        &       -9.211    &       -1.586     \\\\\n",
       "\\textbf{Legendary[T.True]:Q(\"Sp. Def\")}                                   &    -282.2496  &      126.835     &    -2.225  &         0.027        &     -531.738    &      -32.761     \\\\\n",
       "\\textbf{Attack:Q(\"Sp. Def\")}                                              &       0.1094  &        0.034     &     3.233  &         0.001        &        0.043    &        0.176     \\\\\n",
       "\\textbf{Attack:Legendary[T.True]:Q(\"Sp. Def\")}                            &      12.6503  &        5.851     &     2.162  &         0.031        &        1.141    &       24.160     \\\\\n",
       "\\textbf{Defense:Q(\"Sp. Def\")}                                             &       0.0628  &        0.028     &     2.247  &         0.025        &        0.008    &        0.118     \\\\\n",
       "\\textbf{Defense:Legendary[T.True]:Q(\"Sp. Def\")}                           &       3.3949  &        1.783     &     1.904  &         0.058        &       -0.112    &        6.902     \\\\\n",
       "\\textbf{Attack:Defense:Q(\"Sp. Def\")}                                      &      -0.0012  &        0.000     &    -2.730  &         0.007        &       -0.002    &       -0.000     \\\\\n",
       "\\textbf{Attack:Defense:Legendary[T.True]:Q(\"Sp. Def\")}                    &      -0.1456  &        0.065     &    -2.253  &         0.025        &       -0.273    &       -0.018     \\\\\n",
       "\\textbf{Speed:Q(\"Sp. Def\")}                                               &       0.0624  &        0.031     &     2.027  &         0.043        &        0.002    &        0.123     \\\\\n",
       "\\textbf{Speed:Legendary[T.True]:Q(\"Sp. Def\")}                             &      -3.2219  &        1.983     &    -1.625  &         0.105        &       -7.122    &        0.678     \\\\\n",
       "\\textbf{Attack:Speed:Q(\"Sp. Def\")}                                        &      -0.0014  &        0.001     &    -2.732  &         0.007        &       -0.002    &       -0.000     \\\\\n",
       "\\textbf{Attack:Speed:Legendary[T.True]:Q(\"Sp. Def\")}                      &      -0.0695  &        0.033     &    -2.100  &         0.036        &       -0.135    &       -0.004     \\\\\n",
       "\\textbf{Defense:Speed:Q(\"Sp. Def\")}                                       &      -0.0008  &        0.000     &    -1.743  &         0.082        &       -0.002    &        0.000     \\\\\n",
       "\\textbf{Defense:Speed:Legendary[T.True]:Q(\"Sp. Def\")}                     &       0.0334  &        0.021     &     1.569  &         0.117        &       -0.008    &        0.075     \\\\\n",
       "\\textbf{Attack:Defense:Speed:Q(\"Sp. Def\")}                                &    1.629e-05  &     6.92e-06     &     2.355  &         0.019        &     2.68e-06    &     2.99e-05     \\\\\n",
       "\\textbf{Attack:Defense:Speed:Legendary[T.True]:Q(\"Sp. Def\")}              &       0.0008  &        0.000     &     2.433  &         0.015        &        0.000    &        0.001     \\\\\n",
       "\\textbf{Q(\"Sp. Atk\")}                                                     &      -8.3636  &        2.346     &    -3.565  &         0.000        &      -12.978    &       -3.749     \\\\\n",
       "\\textbf{Legendary[T.True]:Q(\"Sp. Atk\")}                                   &     850.5436  &      385.064     &     2.209  &         0.028        &       93.112    &     1607.975     \\\\\n",
       "\\textbf{Attack:Q(\"Sp. Atk\")}                                              &       0.1388  &        0.040     &     3.500  &         0.001        &        0.061    &        0.217     \\\\\n",
       "\\textbf{Attack:Legendary[T.True]:Q(\"Sp. Atk\")}                            &       2.1809  &        1.136     &     1.920  &         0.056        &       -0.054    &        4.416     \\\\\n",
       "\\textbf{Defense:Q(\"Sp. Atk\")}                                             &       0.0831  &        0.038     &     2.162  &         0.031        &        0.007    &        0.159     \\\\\n",
       "\\textbf{Defense:Legendary[T.True]:Q(\"Sp. Atk\")}                           &      -7.3121  &        3.376     &    -2.166  &         0.031        &      -13.953    &       -0.671     \\\\\n",
       "\\textbf{Attack:Defense:Q(\"Sp. Atk\")}                                      &      -0.0014  &        0.001     &    -2.480  &         0.014        &       -0.003    &       -0.000     \\\\\n",
       "\\textbf{Attack:Defense:Legendary[T.True]:Q(\"Sp. Atk\")}                    &      -0.0434  &        0.022     &    -2.010  &         0.045        &       -0.086    &       -0.001     \\\\\n",
       "\\textbf{Speed:Q(\"Sp. Atk\")}                                               &       0.1011  &        0.035     &     2.872  &         0.004        &        0.032    &        0.170     \\\\\n",
       "\\textbf{Speed:Legendary[T.True]:Q(\"Sp. Atk\")}                             &     -12.6343  &        5.613     &    -2.251  &         0.025        &      -23.674    &       -1.594     \\\\\n",
       "\\textbf{Attack:Speed:Q(\"Sp. Atk\")}                                        &      -0.0018  &        0.001     &    -3.102  &         0.002        &       -0.003    &       -0.001     \\\\\n",
       "\\textbf{Attack:Speed:Legendary[T.True]:Q(\"Sp. Atk\")}                      &       0.0151  &        0.009     &     1.609  &         0.109        &       -0.003    &        0.034     \\\\\n",
       "\\textbf{Defense:Speed:Q(\"Sp. Atk\")}                                       &      -0.0012  &        0.001     &    -1.860  &         0.064        &       -0.002    &     6.62e-05     \\\\\n",
       "\\textbf{Defense:Speed:Legendary[T.True]:Q(\"Sp. Atk\")}                     &       0.1210  &        0.054     &     2.260  &         0.024        &        0.016    &        0.226     \\\\\n",
       "\\textbf{Attack:Defense:Speed:Q(\"Sp. Atk\")}                                &    2.125e-05  &      9.1e-06     &     2.334  &         0.020        &     3.34e-06    &     3.92e-05     \\\\\n",
       "\\textbf{Attack:Defense:Speed:Legendary[T.True]:Q(\"Sp. Atk\")}              &    6.438e-06  &     7.69e-05     &     0.084  &         0.933        &       -0.000    &        0.000     \\\\\n",
       "\\textbf{Q(\"Sp. Def\"):Q(\"Sp. Atk\")}                                        &       0.1265  &        0.033     &     3.821  &         0.000        &        0.061    &        0.192     \\\\\n",
       "\\textbf{Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")}                      &      -5.0544  &        2.506     &    -2.017  &         0.044        &       -9.983    &       -0.126     \\\\\n",
       "\\textbf{Attack:Q(\"Sp. Def\"):Q(\"Sp. Atk\")}                                 &      -0.0021  &        0.001     &    -3.606  &         0.000        &       -0.003    &       -0.001     \\\\\n",
       "\\textbf{Attack:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")}               &      -0.0346  &        0.017     &    -1.992  &         0.047        &       -0.069    &       -0.000     \\\\\n",
       "\\textbf{Defense:Q(\"Sp. Def\"):Q(\"Sp. Atk\")}                                &      -0.0012  &        0.000     &    -2.406  &         0.017        &       -0.002    &       -0.000     \\\\\n",
       "\\textbf{Defense:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")}              &       0.0446  &        0.025     &     1.794  &         0.074        &       -0.004    &        0.093     \\\\\n",
       "\\textbf{Attack:Defense:Q(\"Sp. Def\"):Q(\"Sp. Atk\")}                         &    1.973e-05  &     7.28e-06     &     2.710  &         0.007        &     5.41e-06    &      3.4e-05     \\\\\n",
       "\\textbf{Attack:Defense:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")}       &       0.0005  &        0.000     &     1.957  &         0.051        &    -2.56e-06    &        0.001     \\\\\n",
       "\\textbf{Speed:Q(\"Sp. Def\"):Q(\"Sp. Atk\")}                                  &      -0.0013  &        0.000     &    -2.740  &         0.006        &       -0.002    &       -0.000     \\\\\n",
       "\\textbf{Speed:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")}                &       0.0841  &        0.040     &     2.125  &         0.034        &        0.006    &        0.162     \\\\\n",
       "\\textbf{Attack:Speed:Q(\"Sp. Def\"):Q(\"Sp. Atk\")}                           &    2.379e-05  &     7.85e-06     &     3.030  &         0.003        &     8.34e-06    &     3.92e-05     \\\\\n",
       "\\textbf{Attack:Speed:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")}         &    2.864e-05  &     7.73e-05     &     0.370  &         0.711        &       -0.000    &        0.000     \\\\\n",
       "\\textbf{Defense:Speed:Q(\"Sp. Def\"):Q(\"Sp. Atk\")}                          &    1.284e-05  &     7.46e-06     &     1.721  &         0.086        &    -1.83e-06    &     2.75e-05     \\\\\n",
       "\\textbf{Defense:Speed:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")}        &      -0.0008  &        0.000     &    -2.085  &         0.038        &       -0.002    &    -4.68e-05     \\\\\n",
       "\\textbf{Attack:Defense:Speed:Q(\"Sp. Def\"):Q(\"Sp. Atk\")}                   &    -2.53e-07  &      1.1e-07     &    -2.292  &         0.023        &     -4.7e-07    &    -3.59e-08     \\\\\n",
       "\\textbf{Attack:Defense:Speed:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")} &   -1.425e-06  &     1.14e-06     &    -1.249  &         0.212        &    -3.67e-06    &     8.19e-07     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 214.307 & \\textbf{  Durbin-Watson:     } &    1.992  \\\\\n",
       "\\textbf{Prob(Omnibus):} &   0.000 & \\textbf{  Jarque-Bera (JB):  } & 2354.664  \\\\\n",
       "\\textbf{Skew:}          &   2.026 & \\textbf{  Prob(JB):          } &     0.00  \\\\\n",
       "\\textbf{Kurtosis:}      &  14.174 & \\textbf{  Cond. No.          } & 1.20e+16  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. \\newline\n",
       " [2] The condition number is large, 1.2e+16. This might indicate that there are \\newline\n",
       " strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                     HP   R-squared:                       0.467\n",
       "Model:                            OLS   Adj. R-squared:                  0.369\n",
       "Method:                 Least Squares   F-statistic:                     4.764\n",
       "Date:                Mon, 11 Nov 2024   Prob (F-statistic):           4.23e-21\n",
       "Time:                        21:06:24   Log-Likelihood:                -1738.6\n",
       "No. Observations:                 400   AIC:                             3603.\n",
       "Df Residuals:                     337   BIC:                             3855.\n",
       "Df Model:                          62                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "====================================================================================================================================\n",
       "                                                                       coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------------------------------------------------------------\n",
       "Intercept                                                          521.5715    130.273      4.004      0.000     265.322     777.821\n",
       "Legendary[T.True]                                                   -6.1179      2.846     -2.150      0.032     -11.716      -0.520\n",
       "Attack                                                              -8.1938      2.329     -3.518      0.000     -12.775      -3.612\n",
       "Attack:Legendary[T.True]                                         -1224.9610    545.105     -2.247      0.025   -2297.199    -152.723\n",
       "Defense                                                             -6.1989      2.174     -2.851      0.005     -10.475      -1.923\n",
       "Defense:Legendary[T.True]                                         -102.4030     96.565     -1.060      0.290    -292.350      87.544\n",
       "Attack:Defense                                                       0.0985      0.033      2.982      0.003       0.034       0.164\n",
       "Attack:Defense:Legendary[T.True]                                    14.6361      6.267      2.336      0.020       2.310      26.963\n",
       "Speed                                                               -7.2261      2.178     -3.318      0.001     -11.511      -2.942\n",
       "Speed:Legendary[T.True]                                            704.8798    337.855      2.086      0.038      40.309    1369.450\n",
       "Attack:Speed                                                         0.1264      0.038      3.351      0.001       0.052       0.201\n",
       "Attack:Speed:Legendary[T.True]                                       5.8648      2.692      2.179      0.030       0.570      11.160\n",
       "Defense:Speed                                                        0.1026      0.039      2.634      0.009       0.026       0.179\n",
       "Defense:Speed:Legendary[T.True]                                     -6.9266      3.465     -1.999      0.046     -13.742      -0.111\n",
       "Attack:Defense:Speed                                                -0.0016      0.001     -2.837      0.005      -0.003      -0.001\n",
       "Attack:Defense:Speed:Legendary[T.True]                              -0.0743      0.030     -2.477      0.014      -0.133      -0.015\n",
       "Q(\"Sp. Def\")                                                        -5.3982      1.938     -2.785      0.006      -9.211      -1.586\n",
       "Legendary[T.True]:Q(\"Sp. Def\")                                    -282.2496    126.835     -2.225      0.027    -531.738     -32.761\n",
       "Attack:Q(\"Sp. Def\")                                                  0.1094      0.034      3.233      0.001       0.043       0.176\n",
       "Attack:Legendary[T.True]:Q(\"Sp. Def\")                               12.6503      5.851      2.162      0.031       1.141      24.160\n",
       "Defense:Q(\"Sp. Def\")                                                 0.0628      0.028      2.247      0.025       0.008       0.118\n",
       "Defense:Legendary[T.True]:Q(\"Sp. Def\")                               3.3949      1.783      1.904      0.058      -0.112       6.902\n",
       "Attack:Defense:Q(\"Sp. Def\")                                         -0.0012      0.000     -2.730      0.007      -0.002      -0.000\n",
       "Attack:Defense:Legendary[T.True]:Q(\"Sp. Def\")                       -0.1456      0.065     -2.253      0.025      -0.273      -0.018\n",
       "Speed:Q(\"Sp. Def\")                                                   0.0624      0.031      2.027      0.043       0.002       0.123\n",
       "Speed:Legendary[T.True]:Q(\"Sp. Def\")                                -3.2219      1.983     -1.625      0.105      -7.122       0.678\n",
       "Attack:Speed:Q(\"Sp. Def\")                                           -0.0014      0.001     -2.732      0.007      -0.002      -0.000\n",
       "Attack:Speed:Legendary[T.True]:Q(\"Sp. Def\")                         -0.0695      0.033     -2.100      0.036      -0.135      -0.004\n",
       "Defense:Speed:Q(\"Sp. Def\")                                          -0.0008      0.000     -1.743      0.082      -0.002       0.000\n",
       "Defense:Speed:Legendary[T.True]:Q(\"Sp. Def\")                         0.0334      0.021      1.569      0.117      -0.008       0.075\n",
       "Attack:Defense:Speed:Q(\"Sp. Def\")                                 1.629e-05   6.92e-06      2.355      0.019    2.68e-06    2.99e-05\n",
       "Attack:Defense:Speed:Legendary[T.True]:Q(\"Sp. Def\")                  0.0008      0.000      2.433      0.015       0.000       0.001\n",
       "Q(\"Sp. Atk\")                                                        -8.3636      2.346     -3.565      0.000     -12.978      -3.749\n",
       "Legendary[T.True]:Q(\"Sp. Atk\")                                     850.5436    385.064      2.209      0.028      93.112    1607.975\n",
       "Attack:Q(\"Sp. Atk\")                                                  0.1388      0.040      3.500      0.001       0.061       0.217\n",
       "Attack:Legendary[T.True]:Q(\"Sp. Atk\")                                2.1809      1.136      1.920      0.056      -0.054       4.416\n",
       "Defense:Q(\"Sp. Atk\")                                                 0.0831      0.038      2.162      0.031       0.007       0.159\n",
       "Defense:Legendary[T.True]:Q(\"Sp. Atk\")                              -7.3121      3.376     -2.166      0.031     -13.953      -0.671\n",
       "Attack:Defense:Q(\"Sp. Atk\")                                         -0.0014      0.001     -2.480      0.014      -0.003      -0.000\n",
       "Attack:Defense:Legendary[T.True]:Q(\"Sp. Atk\")                       -0.0434      0.022     -2.010      0.045      -0.086      -0.001\n",
       "Speed:Q(\"Sp. Atk\")                                                   0.1011      0.035      2.872      0.004       0.032       0.170\n",
       "Speed:Legendary[T.True]:Q(\"Sp. Atk\")                               -12.6343      5.613     -2.251      0.025     -23.674      -1.594\n",
       "Attack:Speed:Q(\"Sp. Atk\")                                           -0.0018      0.001     -3.102      0.002      -0.003      -0.001\n",
       "Attack:Speed:Legendary[T.True]:Q(\"Sp. Atk\")                          0.0151      0.009      1.609      0.109      -0.003       0.034\n",
       "Defense:Speed:Q(\"Sp. Atk\")                                          -0.0012      0.001     -1.860      0.064      -0.002    6.62e-05\n",
       "Defense:Speed:Legendary[T.True]:Q(\"Sp. Atk\")                         0.1210      0.054      2.260      0.024       0.016       0.226\n",
       "Attack:Defense:Speed:Q(\"Sp. Atk\")                                 2.125e-05    9.1e-06      2.334      0.020    3.34e-06    3.92e-05\n",
       "Attack:Defense:Speed:Legendary[T.True]:Q(\"Sp. Atk\")               6.438e-06   7.69e-05      0.084      0.933      -0.000       0.000\n",
       "Q(\"Sp. Def\"):Q(\"Sp. Atk\")                                            0.1265      0.033      3.821      0.000       0.061       0.192\n",
       "Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")                         -5.0544      2.506     -2.017      0.044      -9.983      -0.126\n",
       "Attack:Q(\"Sp. Def\"):Q(\"Sp. Atk\")                                    -0.0021      0.001     -3.606      0.000      -0.003      -0.001\n",
       "Attack:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")                  -0.0346      0.017     -1.992      0.047      -0.069      -0.000\n",
       "Defense:Q(\"Sp. Def\"):Q(\"Sp. Atk\")                                   -0.0012      0.000     -2.406      0.017      -0.002      -0.000\n",
       "Defense:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")                  0.0446      0.025      1.794      0.074      -0.004       0.093\n",
       "Attack:Defense:Q(\"Sp. Def\"):Q(\"Sp. Atk\")                          1.973e-05   7.28e-06      2.710      0.007    5.41e-06     3.4e-05\n",
       "Attack:Defense:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")           0.0005      0.000      1.957      0.051   -2.56e-06       0.001\n",
       "Speed:Q(\"Sp. Def\"):Q(\"Sp. Atk\")                                     -0.0013      0.000     -2.740      0.006      -0.002      -0.000\n",
       "Speed:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")                    0.0841      0.040      2.125      0.034       0.006       0.162\n",
       "Attack:Speed:Q(\"Sp. Def\"):Q(\"Sp. Atk\")                            2.379e-05   7.85e-06      3.030      0.003    8.34e-06    3.92e-05\n",
       "Attack:Speed:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")          2.864e-05   7.73e-05      0.370      0.711      -0.000       0.000\n",
       "Defense:Speed:Q(\"Sp. Def\"):Q(\"Sp. Atk\")                           1.284e-05   7.46e-06      1.721      0.086   -1.83e-06    2.75e-05\n",
       "Defense:Speed:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")           -0.0008      0.000     -2.085      0.038      -0.002   -4.68e-05\n",
       "Attack:Defense:Speed:Q(\"Sp. Def\"):Q(\"Sp. Atk\")                    -2.53e-07    1.1e-07     -2.292      0.023    -4.7e-07   -3.59e-08\n",
       "Attack:Defense:Speed:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\") -1.425e-06   1.14e-06     -1.249      0.212   -3.67e-06    8.19e-07\n",
       "==============================================================================\n",
       "Omnibus:                      214.307   Durbin-Watson:                   1.992\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             2354.664\n",
       "Skew:                           2.026   Prob(JB):                         0.00\n",
       "Kurtosis:                      14.174   Cond. No.                     1.20e+16\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.2e+16. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model4_linear_form = 'HP ~ Attack * Defense * Speed * Legendary'\n",
    "model4_linear_form += ' * Q(\"Sp. Def\") * Q(\"Sp. Atk\")'\n",
    "# DO NOT try adding '* C(Generation) * C(Q(\"Type 1\")) * C(Q(\"Type 2\"))'\n",
    "# That's 6*18*19 = 6*18*19 possible interaction combinations...\n",
    "# ...a huge number that will blow up your computer\n",
    "\n",
    "model4_spec = smf.ols(formula=model4_linear_form, data=pokeaman_train)\n",
    "model4_fit = model4_spec.fit()\n",
    "model4_fit.summary()\n",
    "\n",
    "## use interaction effect and categorical variables to fit better linear regression model\n",
    "## it shows how different attribute affect another\n",
    "    ## e.g. how differnt combination of attributes affect HP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "565946b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'In sample' R-squared:     0.46709442115833855\n",
      "'Out of sample' R-squared: 0.002485342598992873\n"
     ]
    }
   ],
   "source": [
    "\n",
    "yhat_model4 = model4_fit.predict(pokeaman_test)\n",
    "y = pokeaman_test.HP\n",
    "print(\"'In sample' R-squared:    \", model4_fit.rsquared)\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat_model4)[0,1]**2)\n",
    "\n",
    "## doing same things to the third code with more complex regression model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f104f5",
   "metadata": {},
   "source": [
    "Certainly! Here’s a summary of the discussion for your homework:\n",
    "\n",
    "Summary of Model Evaluation with In-Sample and Out-of-Sample R-squared\n",
    "\n",
    "In this discussion, we explored how to evaluate the performance of linear regression models using R-squared values. Specifically, we compared two models: one simpler and one more complex.\n",
    "\t1.\tIn-Sample R-squared:\n",
    "\t•\tThis measures how well the model fits the training data. It is calculated using the R-squared value from the model’s fit on the training set. A higher R-squared indicates that the model explains a larger proportion of the variance in the dependent variable (HP in this case) for the training data.\n",
    "\t2.\tOut-of-Sample R-squared:\n",
    "\t•\tThis measures how well the model generalizes to unseen data (the test set). It is computed by comparing the model’s predictions on the test set with the actual values in the test set. The correlation between predicted and actual values is squared to obtain the out-of-sample R-squared value. A higher value indicates that the model performs well on new, unseen data.\n",
    "\t3.\tComparison Between Models:\n",
    "\t•\tThe same evaluation process is applied to both the simpler regression model and the more complex model, which includes additional predictors and interaction terms.\n",
    "\t•\tThe in-sample R-squared value tells us how well the model fits the training data, while the out-of-sample R-squared value tells us how well the model predicts new data.\n",
    "\t•\tIdeally, both R-squared values should be close to each other. A large difference might indicate overfitting (the model is too specific to the training data and doesn’t generalize well) or underfitting (the model doesn’t capture the underlying patterns well).\n",
    "\n",
    "Conclusion:\n",
    "\n",
    "By comparing the in-sample and out-of-sample R-squared values for both the simpler and more complex regression models, you can assess how well each model fits and generalizes. This helps in selecting the best model for making accurate predictions on new data.\n",
    "\n",
    "Feel free to adjust any parts if needed!\n",
    "\n",
    "https://chatgpt.com/share/67318255-41d4-8004-8c0c-2d687a557665"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5ae6ec",
   "metadata": {},
   "source": [
    "# 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a6e83fdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>HP</td>        <th>  R-squared:         </th> <td>   0.148</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.143</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   34.40</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Mon, 11 Nov 2024</td> <th>  Prob (F-statistic):</th> <td>1.66e-14</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>21:06:25</td>     <th>  Log-Likelihood:    </th> <td> -1832.6</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   400</td>      <th>  AIC:               </th> <td>   3671.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   397</td>      <th>  BIC:               </th> <td>   3683.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td>   42.5882</td> <td>    3.580</td> <td>   11.897</td> <td> 0.000</td> <td>   35.551</td> <td>   49.626</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack</th>    <td>    0.2472</td> <td>    0.041</td> <td>    6.051</td> <td> 0.000</td> <td>    0.167</td> <td>    0.327</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense</th>   <td>    0.1001</td> <td>    0.045</td> <td>    2.201</td> <td> 0.028</td> <td>    0.011</td> <td>    0.190</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>284.299</td> <th>  Durbin-Watson:     </th> <td>   2.006</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>5870.841</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 2.720</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td>20.963</td>  <th>  Cond. No.          </th> <td>    343.</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}    &        HP        & \\textbf{  R-squared:         } &     0.148   \\\\\n",
       "\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared:    } &     0.143   \\\\\n",
       "\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       } &     34.40   \\\\\n",
       "\\textbf{Date:}             & Mon, 11 Nov 2024 & \\textbf{  Prob (F-statistic):} &  1.66e-14   \\\\\n",
       "\\textbf{Time:}             &     21:06:25     & \\textbf{  Log-Likelihood:    } &   -1832.6   \\\\\n",
       "\\textbf{No. Observations:} &         400      & \\textbf{  AIC:               } &     3671.   \\\\\n",
       "\\textbf{Df Residuals:}     &         397      & \\textbf{  BIC:               } &     3683.   \\\\\n",
       "\\textbf{Df Model:}         &           2      & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                   & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{Intercept} &      42.5882  &        3.580     &    11.897  &         0.000        &       35.551    &       49.626     \\\\\n",
       "\\textbf{Attack}    &       0.2472  &        0.041     &     6.051  &         0.000        &        0.167    &        0.327     \\\\\n",
       "\\textbf{Defense}   &       0.1001  &        0.045     &     2.201  &         0.028        &        0.011    &        0.190     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 284.299 & \\textbf{  Durbin-Watson:     } &    2.006  \\\\\n",
       "\\textbf{Prob(Omnibus):} &   0.000 & \\textbf{  Jarque-Bera (JB):  } & 5870.841  \\\\\n",
       "\\textbf{Skew:}          &   2.720 & \\textbf{  Prob(JB):          } &     0.00  \\\\\n",
       "\\textbf{Kurtosis:}      &  20.963 & \\textbf{  Cond. No.          } &     343.  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                     HP   R-squared:                       0.148\n",
       "Model:                            OLS   Adj. R-squared:                  0.143\n",
       "Method:                 Least Squares   F-statistic:                     34.40\n",
       "Date:                Mon, 11 Nov 2024   Prob (F-statistic):           1.66e-14\n",
       "Time:                        21:06:25   Log-Likelihood:                -1832.6\n",
       "No. Observations:                 400   AIC:                             3671.\n",
       "Df Residuals:                     397   BIC:                             3683.\n",
       "Df Model:                           2                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept     42.5882      3.580     11.897      0.000      35.551      49.626\n",
       "Attack         0.2472      0.041      6.051      0.000       0.167       0.327\n",
       "Defense        0.1001      0.045      2.201      0.028       0.011       0.190\n",
       "==============================================================================\n",
       "Omnibus:                      284.299   Durbin-Watson:                   2.006\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             5870.841\n",
       "Skew:                           2.720   Prob(JB):                         0.00\n",
       "Kurtosis:                      20.963   Cond. No.                         343.\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \"Cond. No.\" WAS 343.0 WITHOUT to centering and scaling\n",
    "model3_fit.summary() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "28e4b82a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>HP</td>        <th>  R-squared:         </th> <td>   0.148</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.143</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   34.40</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Mon, 11 Nov 2024</td> <th>  Prob (F-statistic):</th> <td>1.66e-14</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>21:06:25</td>     <th>  Log-Likelihood:    </th> <td> -1832.6</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   400</td>      <th>  AIC:               </th> <td>   3671.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   397</td>      <th>  BIC:               </th> <td>   3683.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "             <td></td>               <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>              <td>   69.3025</td> <td>    1.186</td> <td>   58.439</td> <td> 0.000</td> <td>   66.971</td> <td>   71.634</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>scale(center(Attack))</th>  <td>    8.1099</td> <td>    1.340</td> <td>    6.051</td> <td> 0.000</td> <td>    5.475</td> <td>   10.745</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>scale(center(Defense))</th> <td>    2.9496</td> <td>    1.340</td> <td>    2.201</td> <td> 0.028</td> <td>    0.315</td> <td>    5.585</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>284.299</td> <th>  Durbin-Watson:     </th> <td>   2.006</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>5870.841</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 2.720</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td>20.963</td>  <th>  Cond. No.          </th> <td>    1.66</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}         &        HP        & \\textbf{  R-squared:         } &     0.148   \\\\\n",
       "\\textbf{Model:}                 &       OLS        & \\textbf{  Adj. R-squared:    } &     0.143   \\\\\n",
       "\\textbf{Method:}                &  Least Squares   & \\textbf{  F-statistic:       } &     34.40   \\\\\n",
       "\\textbf{Date:}                  & Mon, 11 Nov 2024 & \\textbf{  Prob (F-statistic):} &  1.66e-14   \\\\\n",
       "\\textbf{Time:}                  &     21:06:25     & \\textbf{  Log-Likelihood:    } &   -1832.6   \\\\\n",
       "\\textbf{No. Observations:}      &         400      & \\textbf{  AIC:               } &     3671.   \\\\\n",
       "\\textbf{Df Residuals:}          &         397      & \\textbf{  BIC:               } &     3683.   \\\\\n",
       "\\textbf{Df Model:}              &           2      & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}       &    nonrobust     & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                                & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{Intercept}              &      69.3025  &        1.186     &    58.439  &         0.000        &       66.971    &       71.634     \\\\\n",
       "\\textbf{scale(center(Attack))}  &       8.1099  &        1.340     &     6.051  &         0.000        &        5.475    &       10.745     \\\\\n",
       "\\textbf{scale(center(Defense))} &       2.9496  &        1.340     &     2.201  &         0.028        &        0.315    &        5.585     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 284.299 & \\textbf{  Durbin-Watson:     } &    2.006  \\\\\n",
       "\\textbf{Prob(Omnibus):} &   0.000 & \\textbf{  Jarque-Bera (JB):  } & 5870.841  \\\\\n",
       "\\textbf{Skew:}          &   2.720 & \\textbf{  Prob(JB):          } &     0.00  \\\\\n",
       "\\textbf{Kurtosis:}      &  20.963 & \\textbf{  Cond. No.          } &     1.66  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                     HP   R-squared:                       0.148\n",
       "Model:                            OLS   Adj. R-squared:                  0.143\n",
       "Method:                 Least Squares   F-statistic:                     34.40\n",
       "Date:                Mon, 11 Nov 2024   Prob (F-statistic):           1.66e-14\n",
       "Time:                        21:06:25   Log-Likelihood:                -1832.6\n",
       "No. Observations:                 400   AIC:                             3671.\n",
       "Df Residuals:                     397   BIC:                             3683.\n",
       "Df Model:                           2                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==========================================================================================\n",
       "                             coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------------------\n",
       "Intercept                 69.3025      1.186     58.439      0.000      66.971      71.634\n",
       "scale(center(Attack))      8.1099      1.340      6.051      0.000       5.475      10.745\n",
       "scale(center(Defense))     2.9496      1.340      2.201      0.028       0.315       5.585\n",
       "==============================================================================\n",
       "Omnibus:                      284.299   Durbin-Watson:                   2.006\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             5870.841\n",
       "Skew:                           2.720   Prob(JB):                         0.00\n",
       "Kurtosis:                      20.963   Cond. No.                         1.66\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from patsy import center, scale\n",
    "\n",
    "model3_linear_form_center_scale = \\\n",
    "  'HP ~ scale(center(Attack)) + scale(center(Defense))' \n",
    "model_spec3_center_scale = smf.ols(formula=model3_linear_form_center_scale,\n",
    "                                   data=pokeaman_train)\n",
    "model3_center_scale_fit = model_spec3_center_scale.fit()\n",
    "model3_center_scale_fit.summary()\n",
    "# \"Cond. No.\" is NOW 1.66 due to centering and scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "88671b4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>214.307</td> <th>  Durbin-Watson:     </th> <td>   1.992</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>2354.663</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 2.026</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td>14.174</td>  <th>  Cond. No.          </th> <td>1.54e+16</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Omnibus:}       & 214.307 & \\textbf{  Durbin-Watson:     } &    1.992  \\\\\n",
       "\\textbf{Prob(Omnibus):} &   0.000 & \\textbf{  Jarque-Bera (JB):  } & 2354.663  \\\\\n",
       "\\textbf{Skew:}          &   2.026 & \\textbf{  Prob(JB):          } &     0.00  \\\\\n",
       "\\textbf{Kurtosis:}      &  14.174 & \\textbf{  Cond. No.          } & 1.54e+16  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\end{center}"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.table.SimpleTable'>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model4_linear_form_CS = 'HP ~ scale(center(Attack)) * scale(center(Defense))'\n",
    "model4_linear_form_CS += ' * scale(center(Speed)) * Legendary' \n",
    "model4_linear_form_CS += ' * scale(center(Q(\"Sp. Def\"))) * scale(center(Q(\"Sp. Atk\")))'\n",
    "# Legendary is an indicator, so we don't center and scale that\n",
    "\n",
    "model4_CS_spec = smf.ols(formula=model4_linear_form_CS, data=pokeaman_train)\n",
    "model4_CS_fit = model4_CS_spec.fit()\n",
    "model4_CS_fit.summary().tables[-1]  # Cond. No. is 2,250,000,000,000,000\n",
    "\n",
    "# The condition number is still bad even after centering and scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "10c0bfef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>214.307</td> <th>  Durbin-Watson:     </th> <td>   1.992</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>2354.664</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 2.026</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td>14.174</td>  <th>  Cond. No.          </th> <td>1.20e+16</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Omnibus:}       & 214.307 & \\textbf{  Durbin-Watson:     } &    1.992  \\\\\n",
       "\\textbf{Prob(Omnibus):} &   0.000 & \\textbf{  Jarque-Bera (JB):  } & 2354.664  \\\\\n",
       "\\textbf{Skew:}          &   2.026 & \\textbf{  Prob(JB):          } &     0.00  \\\\\n",
       "\\textbf{Kurtosis:}      &  14.174 & \\textbf{  Cond. No.          } & 1.20e+16  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\end{center}"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.table.SimpleTable'>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Just as the condition number was very bad to start with\n",
    "model4_fit.summary().tables[-1]  # Cond. No. is 12,000,000,000,000,000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670bfb54",
   "metadata": {},
   "source": [
    "Every column in design matrix is a predictor variables, such as attack and defense. In order to convenient for comparison, these variables minus their mean and divide their standard error (Centering and Scaling).  However, multicollinearity may occurs when some predictor variables are in common, which means they have a big part that overlapped together. When these variables affect each other, the model hard to judge that which one has the greater effect, which makes the model unreliable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9043f271",
   "metadata": {},
   "source": [
    "Here’s a summary of the discussion for your homework:\n",
    "\n",
    "Design Matrix and Multicollinearity:\n",
    "\n",
    "\t1.\tDesign Matrix: The design matrix is a table where each column represents a predictor variable (e.g., Attack, Defense). To make comparison easier, these predictor variables are centered (subtracting their mean) and scaled (dividing by their standard deviation). This helps to ensure that all variables are on the same scale.\n",
    "\t2.\tMulticollinearity: Multicollinearity occurs when some predictor variables are highly correlated with each other, meaning they share a lot of similar information. For example, Attack and Defense may both be related to the outcome variable (e.g., HP) and to each other.\n",
    "\t3.\tImpact of Multicollinearity: When predictor variables overlap in this way, the model struggles to determine the unique contribution of each variable. This makes it difficult for the model to identify which variable is more important for predicting the outcome. As a result, the model becomes less reliable, and it may have trouble making accurate predictions on new, unseen data.\n",
    "\n",
    "This should cover the key points discussed!\n",
    "\n",
    "https://chatgpt.com/share/67318700-0f34-8004-9249-d2aebfa5004d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa27283",
   "metadata": {},
   "source": [
    "# 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7bda0670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'In sample' R-squared:     0.3920134083531893\n",
      "'Out of sample' R-squared: 0.30015614488652215\n",
      "'In sample' R-squared:     0.3326310334310908\n",
      "'Out of sample' R-squared: 0.29572460427079933\n",
      "'In sample' R-squared:     0.37818209127432456\n",
      "'Out of sample' R-squared: 0.35055389205977444\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>252.300</td> <th>  Durbin-Watson:     </th> <td>   1.953</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>3474.611</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 2.438</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td>16.590</td>  <th>  Cond. No.          </th> <td>    15.4</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Omnibus:}       & 252.300 & \\textbf{  Durbin-Watson:     } &    1.953  \\\\\n",
       "\\textbf{Prob(Omnibus):} &   0.000 & \\textbf{  Jarque-Bera (JB):  } & 3474.611  \\\\\n",
       "\\textbf{Skew:}          &   2.438 & \\textbf{  Prob(JB):          } &     0.00  \\\\\n",
       "\\textbf{Kurtosis:}      &  16.590 & \\textbf{  Cond. No.          } &     15.4  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\end{center}"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.table.SimpleTable'>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here's something a little more reasonable...\n",
    "model5_linear_form = 'HP ~ Attack + Defense + Speed + Legendary'\n",
    "model5_linear_form += ' + Q(\"Sp. Def\") + Q(\"Sp. Atk\")'\n",
    "model5_linear_form += ' + C(Generation) + C(Q(\"Type 1\")) + C(Q(\"Type 2\"))'\n",
    "\n",
    "model5_spec = smf.ols(formula=model5_linear_form, data=pokeaman_train)\n",
    "model5_fit = model5_spec.fit()\n",
    "model5_fit.summary()\n",
    "yhat_model5 = model5_fit.predict(pokeaman_test)\n",
    "y = pokeaman_test.HP\n",
    "print(\"'In sample' R-squared:    \", model5_fit.rsquared)\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat_model5)[0,1]**2)\n",
    "# Here's something a little more reasonable...\n",
    "model6_linear_form = 'HP ~ Attack + Speed + Q(\"Sp. Def\") + Q(\"Sp. Atk\")'\n",
    "# And here we'll add the significant indicators from the previous model\n",
    "# https://chatgpt.com/share/81ab88df-4f07-49f9-a44a-de0cfd89c67c\n",
    "model6_linear_form += ' + I(Q(\"Type 1\")==\"Normal\")'\n",
    "model6_linear_form += ' + I(Q(\"Type 1\")==\"Water\")'\n",
    "model6_linear_form += ' + I(Generation==2)'\n",
    "model6_linear_form += ' + I(Generation==5)'\n",
    "\n",
    "model6_spec = smf.ols(formula=model6_linear_form, data=pokeaman_train)\n",
    "model6_fit = model6_spec.fit()\n",
    "model6_fit.summary()\n",
    "yhat_model6 = model6_fit.predict(pokeaman_test)\n",
    "y = pokeaman_test.HP\n",
    "print(\"'In sample' R-squared:    \", model6_fit.rsquared)\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat_model6)[0,1]**2)\n",
    "# And here's a slight change that seems to perhaps improve prediction...\n",
    "model7_linear_form = 'HP ~ Attack * Speed * Q(\"Sp. Def\") * Q(\"Sp. Atk\")'\n",
    "model7_linear_form += ' + I(Q(\"Type 1\")==\"Normal\")'\n",
    "model7_linear_form += ' + I(Q(\"Type 1\")==\"Water\")'\n",
    "model7_linear_form += ' + I(Generation==2)'\n",
    "model7_linear_form += ' + I(Generation==5)'\n",
    "\n",
    "model7_spec = smf.ols(formula=model7_linear_form, data=pokeaman_train)\n",
    "model7_fit = model7_spec.fit()\n",
    "model7_fit.summary()\n",
    "yhat_model7 = model7_fit.predict(pokeaman_test)\n",
    "y = pokeaman_test.HP\n",
    "print(\"'In sample' R-squared:    \", model7_fit.rsquared)\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat_model7)[0,1]**2)\n",
    "# And here's a slight change that seems to perhas improve prediction...\n",
    "model7_linear_form_CS = 'HP ~ scale(center(Attack)) * scale(center(Speed))'\n",
    "model7_linear_form_CS += ' * scale(center(Q(\"Sp. Def\"))) * scale(center(Q(\"Sp. Atk\")))'\n",
    "# We DO NOT center and scale indicator variables\n",
    "model7_linear_form_CS += ' + I(Q(\"Type 1\")==\"Normal\")'\n",
    "model7_linear_form_CS += ' + I(Q(\"Type 1\")==\"Water\")'\n",
    "model7_linear_form_CS += ' + I(Generation==2)'\n",
    "model7_linear_form_CS += ' + I(Generation==5)'\n",
    "\n",
    "model7_CS_spec = smf.ols(formula=model7_linear_form_CS, data=pokeaman_train)\n",
    "model7_CS_fit = model7_CS_spec.fit()\n",
    "model7_CS_fit.summary().tables[-1] \n",
    "# \"Cond. No.\" is NOW 15.4 due to centering and scaling\n",
    "# \"Cond. No.\" WAS 2,340,000,000 WITHOUT to centering and scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b82c81",
   "metadata": {},
   "source": [
    "Model 5 is the fitting of model 3 and 4. It introduce some important predictor variables: attack, defense, speed, legendary, generation, type, Q (\"Sp. Def\") and Q(\"Sp. Atk\"). Q is for the variable name that contain “.”. These variables used for expand the model and interaction effect, so it can have better prediction on dependent variable HP.\n",
    "Model 6 simplify the predictor variables. Only focus on really important variables: attack, speed, Q (\"Sp. Def\") and Q(\"Sp. Atk\"), adding interaction terms, e.g. generations and type.\n",
    "Model 7 contain all interaction effect in model 6 and it is the further expanding for model 6, especially on attack, speed, Q (\"Sp. Def\") and Q(\"Sp. Atk\"), focus on if these interactions have combination effect to HP. It also do the Centering and Scaling to these variables, to make sure they are on the same scale and improve the model’s stability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef2e120",
   "metadata": {},
   "source": [
    "Here’s a summary of the discussion for your homework:\n",
    "\n",
    "Key Concepts:\n",
    "\n",
    "\t1.\tQ() and C() in Formula:\n",
    "\t•\tQ(): Used to handle column names with special characters (like periods). For example, Q(\"Sp. Def\") ensures the variable with the name “Sp. Def” is correctly referenced in the model formula.\n",
    "\t•\tC(): Used to treat a variable as categorical, allowing the model to create dummy variables for each category (e.g., C(Generation)).\n",
    "\t2.\tModel 5:\n",
    "\t•\tPurpose: Expands on models 3 and 4 by introducing more predictor variables, such as Attack, Defense, Speed, Legendary, Generation, and Type.\n",
    "\t•\tKey Features: Adds categorical variables (C()) for Generation, Type 1, and Type 2, and includes interaction terms for variables like Q(\"Sp. Def\") and Q(\"Sp. Atk\"). These additions help the model better predict HP by considering more features.\n",
    "\t3.\tModel 6:\n",
    "\t•\tPurpose: Simplifies the predictor variables by focusing on the most important ones: Attack, Speed, Q(\"Sp. Def\"), and Q(\"Sp. Atk\").\n",
    "\t•\tKey Features: Introduces interaction terms between categorical variables (e.g., Generation == 2 or Generation == 5, Type 1 == \"Normal\") to capture significant categorical effects on HP.\n",
    "\t4.\tModel 7:\n",
    "\t•\tPurpose: Further expands upon Model 6 by adding more interaction effects between the continuous predictors (Attack, Speed, Q(\"Sp. Def\"), and Q(\"Sp. Atk\")).\n",
    "\t•\tKey Features: Applies centering and scaling to continuous variables to standardize them and improve model stability and interpretation.\n",
    "\t5.\tSp. Def:\n",
    "\t•\tDefinition: “Sp. Def” refers to Special Defense, one of the six core stats in Pokémon that determines how well a Pokémon resists special attacks.\n",
    "\n",
    "Summary of Model Development:\n",
    "\n",
    "\t•\tModel 5 adds more predictors and considers categorical variables to improve prediction accuracy.\n",
    "\t•\tModel 6 simplifies the model, focusing on key variables and adding categorical interactions.\n",
    "\t•\tModel 7 expands further by incorporating interactions between continuous variables and applying centering/scaling for improved model performance and stability.\n",
    "    \n",
    "https://chatgpt.com/share/67318cfb-d684-8004-95dd-f48427b31578"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bceb81",
   "metadata": {},
   "source": [
    "# 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0983a213",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Parameters\n",
    "num_simulations = 100  # Number of model performance metric pairs to generate\n",
    "in_sample_results = []\n",
    "out_sample_results = []\n",
    "\n",
    "# Simulation loop\n",
    "for _ in range(num_simulations):\n",
    "    # Generate in-sample metric (e.g., mean squared error)\n",
    "    in_sample_metric = np.random.normal(loc=0.1, scale=0.02)  # Mean 0.1, small variance\n",
    "\n",
    "    # Generate out-of-sample metric, often slightly worse than in-sample\n",
    "    out_sample_metric = np.random.normal(loc=0.12, scale=0.03)  # Mean 0.12, slightly larger variance\n",
    "\n",
    "    # Collect results\n",
    "    in_sample_results.append(in_sample_metric)\n",
    "    out_sample_results.append(out_sample_metric)\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(in_sample_results, bins=20, alpha=0.6, label=\"In-Sample Performance\")\n",
    "plt.hist(out_sample_results, bins=20, alpha=0.6, label=\"Out-of-Sample Performance\")\n",
    "plt.xlabel(\"Model Performance Metric\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend()\n",
    "plt.title(\"In-Sample vs Out-of-Sample Model Performance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab28746",
   "metadata": {},
   "source": [
    "Purpose: this “for” loops can simulate multiple times represented different fitting scenario. In-sample performance means that training data. Out-of-sample performance means the new data which should close the result of In-sample performance in ideal situation.\n",
    "\n",
    "Result: the Out-of-sample performance is so different with In-sample performance, it may suggested that this model is overfitting or differences in data distribution. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b8ef5d",
   "metadata": {},
   "source": [
    "Certainly! Here’s a concise summary:\n",
    "\n",
    "Purpose:\n",
    "\n",
    "This code uses a for loop to simulate multiple model fitting scenarios, each time generating “in-sample” and “out-of-sample” performance metrics. “In-sample” performance represents the model’s accuracy on the training data, while “out-of-sample” performance indicates how well the model performs on new, unseen data. Ideally, the out-of-sample performance should be close to the in-sample performance, which suggests the model generalizes well.\n",
    "\n",
    "Result:\n",
    "\n",
    "A large difference between in-sample and out-of-sample performance may indicate overfitting, where the model performs well on training data but poorly on new data. This discrepancy could also suggest differences in data distribution between the training and test datasets, affecting the model’s generalizability. The histogram visualization helps compare the stability and variance of these metrics across multiple simulations.\n",
    "\n",
    "This summary captures the essence of the code’s purpose and what the results reveal about model performance. Let me know if you’d like to add any other details!\n",
    "\n",
    "https://chatgpt.com/share/67325ae0-a430-8004-95c9-2557a4f0661f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "98706e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'In sample' R-squared:     0.37818209127432456 (original)\n",
      "'Out of sample' R-squared: 0.35055389205977444 (original)\n",
      "'In sample' R-squared:     0.5726118179916575 (gen1_predict_future)\n",
      "'Out of sample' R-squared: 0.11151363354803218 (gen1_predict_future)\n"
     ]
    }
   ],
   "source": [
    "model7_gen1_predict_future = smf.ols(formula=model7_linear_form,\n",
    "                                   data=pokeaman[pokeaman.Generation==1])\n",
    "model7_gen1_predict_future_fit = model7_gen1_predict_future.fit()\n",
    "print(\"'In sample' R-squared:    \", model7_fit.rsquared, \"(original)\")\n",
    "y = pokeaman_test.HP\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat_model7)[0,1]**2, \"(original)\")\n",
    "print(\"'In sample' R-squared:    \", model7_gen1_predict_future_fit.rsquared, \"(gen1_predict_future)\")\n",
    "y = pokeaman[pokeaman.Generation!=1].HP\n",
    "yhat = model7_gen1_predict_future_fit.predict(pokeaman[pokeaman.Generation!=1])\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat)[0,1]**2, \"(gen1_predict_future)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3373e863",
   "metadata": {},
   "source": [
    "this part use smf.ols function to set up a OLS regression model, aimming to analyze the predictive effect of different dataset by comparing R^2 difference between in-sample set and out-of-sample set in both oringinal and fitting model. in sample data from the first generation and the out of sample from 2-5 generation. This shows if the model overfitting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0f314225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'In sample' R-squared:     0.37818209127432456 (original)\n",
      "'Out of sample' R-squared: 0.35055389205977444 (original)\n",
      "'In sample' R-squared:     0.3904756578094535 (gen1to5_predict_future)\n",
      "'Out of sample' R-squared: 0.23394915464343125 (gen1to5_predict_future)\n"
     ]
    }
   ],
   "source": [
    "model7_gen1to5_predict_future = smf.ols(formula=model7_linear_form,\n",
    "                                   data=pokeaman[pokeaman.Generation!=6])\n",
    "model7_gen1to5_predict_future_fit = model7_gen1to5_predict_future.fit()\n",
    "print(\"'In sample' R-squared:    \", model7_fit.rsquared, \"(original)\")\n",
    "y = pokeaman_test.HP\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat_model7)[0,1]**2, \"(original)\")\n",
    "print(\"'In sample' R-squared:    \", model7_gen1to5_predict_future_fit.rsquared, \"(gen1to5_predict_future)\")\n",
    "y = pokeaman[pokeaman.Generation==6].HP\n",
    "yhat = model7_gen1to5_predict_future_fit.predict(pokeaman[pokeaman.Generation==6])\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat)[0,1]**2, \"(gen1to5_predict_future)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbc45da",
   "metadata": {},
   "source": [
    "this codes based on wider dataset that from gerneration 1 to generation 5 when computing in sample R^2 and the out of sample form generation 6. so this code can evaluate if the model training by 1-5 generation worked on predicting generation 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "48c0adfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'In sample' R-squared:     0.3326310334310908 (original)\n",
      "'Out of sample' R-squared: 0.29572460427079933 (original)\n",
      "'In sample' R-squared:     0.4433880517727282 (gen1_predict_future)\n",
      "'Out of sample' R-squared: 0.1932858534276128 (gen1_predict_future)\n"
     ]
    }
   ],
   "source": [
    "model6_gen1_predict_future = smf.ols(formula=model6_linear_form,\n",
    "                                   data=pokeaman[pokeaman.Generation==1])\n",
    "model6_gen1_predict_future_fit = model6_gen1_predict_future.fit()\n",
    "print(\"'In sample' R-squared:    \", model6_fit.rsquared, \"(original)\")\n",
    "y = pokeaman_test.HP\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat_model6)[0,1]**2, \"(original)\")\n",
    "print(\"'In sample' R-squared:    \", model6_gen1_predict_future_fit.rsquared, \"(gen1_predict_future)\")\n",
    "y = pokeaman[pokeaman.Generation!=1].HP\n",
    "yhat = model6_gen1_predict_future_fit.predict(pokeaman[pokeaman.Generation!=1])\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat)[0,1]**2, \"(gen1_predict_future)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fd74e4",
   "metadata": {},
   "source": [
    "this code training the model based on first generation and test on data of generations 2-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a0579f8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'In sample' R-squared:     0.3326310334310908 (original)\n",
      "'Out of sample' R-squared: 0.29572460427079933 (original)\n",
      "'In sample' R-squared:     0.33517279824114776 (gen1to5_predict_future)\n",
      "'Out of sample' R-squared: 0.26262690178799936 (gen1to5_predict_future)\n"
     ]
    }
   ],
   "source": [
    "model6_gen1to5_predict_future = smf.ols(formula=model6_linear_form,\n",
    "                                   data=pokeaman[pokeaman.Generation!=6])\n",
    "model6_gen1to5_predict_future_fit = model6_gen1to5_predict_future.fit()\n",
    "print(\"'In sample' R-squared:    \", model6_fit.rsquared, \"(original)\")\n",
    "y = pokeaman_test.HP\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat_model6)[0,1]**2, \"(original)\")\n",
    "print(\"'In sample' R-squared:    \", model6_gen1to5_predict_future_fit.rsquared, \"(gen1to5_predict_future)\")\n",
    "y = pokeaman[pokeaman.Generation==6].HP\n",
    "yhat = model6_gen1to5_predict_future_fit.predict(pokeaman[pokeaman.Generation==6])\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat)[0,1]**2, \"(gen1to5_predict_future)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af38306b",
   "metadata": {},
   "source": [
    "basicly same to the second model, but use different variables to predict HP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a928a5",
   "metadata": {},
   "source": [
    "I apologize for focusing too much on Model 6 and Model 7. Let’s address all four code sections and questions you asked about, including the explanations and differences.\n",
    "\n",
    "Complete Summary of Our Discussion:\n",
    "\n",
    "In this discussion, we focused on four different code snippets, each using ordinary least squares (OLS) regression to compare the predictive accuracy of models trained on different Pokémon generation datasets. The goal was to assess in-sample and out-of-sample performance of the models based on different sets of training data, and to understand how the models predict HP (Health Points) for Pokémon in unseen generations.\n",
    "\n",
    "Code Explanations:\n",
    "\n",
    "1st Code: Comparing In-Sample and Out-of-Sample for Model 7\n",
    "\n",
    "model7_gen1_predict_future = smf.ols(formula=model7_linear_form,\n",
    "                                   data=pokeaman[pokeaman.Generation==1])\n",
    "model7_gen1_predict_future_fit = model7_gen1_predict_future.fit()\n",
    "print(\"'In sample' R-squared:    \", model7_fit.rsquared, \"(original)\")\n",
    "y = pokeaman_test.HP\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat_model7)[0,1]**2, \"(original)\")\n",
    "print(\"'In sample' R-squared:    \", model7_gen1_predict_future_fit.rsquared, \"(gen1_predict_future)\")\n",
    "y = pokeaman[pokeaman.Generation!=1].HP\n",
    "yhat = model7_gen1_predict_future_fit.predict(pokeaman[pokeaman.Generation!=1])\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat)[0,1]**2, \"(gen1_predict_future)\")\n",
    "\n",
    "\t•\tPurpose: This code trains Model 7 on Generation 1 data and tests its predictive ability on Generations 2-5 (out-of-sample).\n",
    "\t•\tIn-sample ￼ is calculated for the model trained on Generation 1.\n",
    "\t•\tOut-of-sample ￼ compares the model’s predictions on Generations 2-5 to actual values.\n",
    "\t•\tThe goal is to see how well a model trained on Generation 1 can predict HP for Pokémon in later generations (2-5).\n",
    "\n",
    "2nd Code: Comparing In-Sample and Out-of-Sample for Model 7 (with Generations 1-5 vs. Generation 6)\n",
    "\n",
    "model7_gen1to5_predict_future = smf.ols(formula=model7_linear_form,\n",
    "                                   data=pokeaman[pokeaman.Generation!=6])\n",
    "model7_gen1to5_predict_future_fit = model7_gen1to5_predict_future.fit()\n",
    "print(\"'In sample' R-squared:    \", model7_fit.rsquared, \"(original)\")\n",
    "y = pokeaman_test.HP\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat_model7)[0,1]**2, \"(original)\")\n",
    "print(\"'In sample' R-squared:    \", model7_gen1to5_predict_future_fit.rsquared, \"(gen1to5_predict_future)\")\n",
    "y = pokeaman[pokeaman.Generation==6].HP\n",
    "yhat = model7_gen1to5_predict_future_fit.predict(pokeaman[pokeaman.Generation==6])\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat)[0,1]**2, \"(gen1to5_predict_future)\")\n",
    "\n",
    "\t•\tPurpose: This code trains Model 7 on Generations 1-5 data and tests its ability to predict HP for Generation 6 (out-of-sample).\n",
    "\t•\tIn-sample ￼ is calculated for the model trained on Generations 1-5.\n",
    "\t•\tOut-of-sample ￼ evaluates how well the model generalizes to Generation 6, which was not part of the training data.\n",
    "\t•\tThis helps assess whether a model trained on earlier generations can predict Pokémon HP for a completely new generation (Generation 6).\n",
    "\n",
    "3rd Code: Comparing In-Sample and Out-of-Sample for Model 6\n",
    "\n",
    "model6_gen1_predict_future = smf.ols(formula=model6_linear_form,\n",
    "                                   data=pokeaman[pokeaman.Generation==1])\n",
    "model6_gen1_predict_future_fit = model6_gen1_predict_future.fit()\n",
    "print(\"'In sample' R-squared:    \", model6_fit.rsquared, \"(original)\")\n",
    "y = pokeaman_test.HP\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat_model6)[0,1]**2, \"(original)\")\n",
    "print(\"'In sample' R-squared:    \", model6_gen1_predict_future_fit.rsquared, \"(gen1_predict_future)\")\n",
    "y = pokeaman[pokeaman.Generation!=1].HP\n",
    "yhat = model6_gen1_predict_future_fit.predict(pokeaman[pokeaman.Generation!=1])\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat)[0,1]**2, \"(gen1_predict_future)\")\n",
    "\n",
    "\t•\tPurpose: This code trains Model 6 on Generation 1 data and tests its predictive power on Generations 2-5 (out-of-sample).\n",
    "\t•\tSimilar to the first code, in-sample ￼ is calculated for the model trained on Generation 1, and out-of-sample ￼ is calculated for Generations 2-5.\n",
    "\t•\tIt helps assess how well Model 6 trained on Generation 1 can generalize to other generations.\n",
    "\n",
    "4th Code: Comparing In-Sample and Out-of-Sample for Model 6 (with Generations 1-5 vs. Generation 6)\n",
    "\n",
    "model6_gen1to5_predict_future = smf.ols(formula=model6_linear_form,\n",
    "                                   data=pokeaman[pokeaman.Generation!=6])\n",
    "model6_gen1to5_predict_future_fit = model6_gen1to5_predict_future.fit()\n",
    "print(\"'In sample' R-squared:    \", model6_fit.rsquared, \"(original)\")\n",
    "y = pokeaman_test.HP\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat_model6)[0,1]**2, \"(original)\")\n",
    "print(\"'In sample' R-squared:    \", model6_gen1to5_predict_future_fit.rsquared, \"(gen1to5_predict_future)\")\n",
    "y = pokeaman[pokeaman.Generation==6].HP\n",
    "yhat = model6_gen1to5_predict_future_fit.predict(pokeaman[pokeaman.Generation==6])\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat)[0,1]**2, \"(gen1to5_predict_future)\")\n",
    "\n",
    "\t•\tPurpose: This code trains Model 6 on Generations 1-5 and tests the model on Generation 6 (out-of-sample).\n",
    "\t•\tIn-sample ￼ is calculated for the model trained on Generations 1-5, and out-of-sample ￼ is calculated for Generation 6.\n",
    "\t•\tIt assesses if a model trained on previous generations (1-5) can predict HP for Generation 6 effectively.\n",
    "\n",
    "Key Differences Between the Codes:\n",
    "\n",
    "\t1.\tTraining Dataset:\n",
    "\t•\tIn the first and third code segments, models are trained on Generation 1 data.\n",
    "\t•\tIn the second and fourth segments, models are trained on Generations 1-5 data.\n",
    "\t2.\tTesting Dataset:\n",
    "\t•\tIn the first and third code segments, models are tested on Generations 2-5 (out-of-sample).\n",
    "\t•\tIn the second and fourth code segments, models are tested on Generation 6 (out-of-sample).\n",
    "\t3.\tPurpose:\n",
    "\t•\tThe first and third codes focus on evaluating whether a model trained on Generation 1 can predict for later generations (2-5).\n",
    "\t•\tThe second and fourth codes evaluate if models trained on Generations 1-5 can generalize and predict for Generation 6.\n",
    "\n",
    "Conclusion:\n",
    "\n",
    "These four code snippets are designed to evaluate the generalizability of two regression models (Model 6 and Model 7) trained on Pokémon data from various generations. By comparing in-sample and out-of-sample ￼, we can assess how well the models trained on earlier generations can predict HP for unseen generations. This approach helps determine the most effective set of predictor variables and models for predicting Pokémon HP in future, unseen generations.\n",
    "\n",
    "https://chatgpt.com/share/67327462-1ae4-8004-af1f-f6b4b41d8b91"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd627ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
